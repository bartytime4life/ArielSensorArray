# configs/model/v50.yaml

# ==============================================================================

# ðŸ›°ï¸ SpectraMind V50 â€” Canonical Model Architecture

# NeurIPS 2025 Ariel Data Challenge

#

# Purpose:

# Defines the hybrid FGS1 (Mamba SSM) + AIRS (Graph Neural Network) architecture

# with multi-scale decoders, physics-aware uncertainty heads, and symbolic overlays.

#

# Hydra:

# This config is Hydra-composable (referenced by train configs via `model=v50`).

# Override components via CLI, e.g.:

# spectramind train model=v50 model.encoder.fgs1.depth=12 model.fusion.method=gated\_sum

#

# Notes:

# â€¢ Encoders: FGS1 uses a Mamba-style SSM for ultra-long sequences; AIRS uses a GNN

# (default: GAT) with edge features for wavelength proximity, molecule links,

# and detector region adjacency.

# â€¢ Fusion: cross-attention lets FGS1 latents inform AIRS spectral reasoning.

# â€¢ Decoders: mean (Î¼) head + uncertainty (Ïƒ) head with physics/symbolic overlays.

# â€¢ Constraints: non-negativity, spectral smoothness (time & wavelength), FFT prior,

# optional gravitational lensing overlay region weighting.

# â€¢ Guardrails: parameter cap and runtime hints to remain Kaggle-safe (â‰¤ \~9 hrs).

# ==============================================================================

# ------------------------------------------------------------------------------

# Model metadata

# ------------------------------------------------------------------------------

model:
name: "spectramind\_v50"
version: "1.1"
description: "Hybrid FGS1-Mamba + AIRS-GNN with cross-attention fusion and physics/symbolic-aware decoders"
seed: 42
dvc\_stage: "model/v50"
save\_dir: "artifacts/models/v50"
backend: "torch"                    # torch | torchscript (exported) | onnx (exported)
framework: "pyg"                    # primary GNN backend: pyg (PyTorch Geometric)
init: "xavier\_uniform"              # weight init for MLP heads/fusion
allow\_compile: true                 # enable torch.compile where supported (guarded in code)

# ------------------------------------------------------------------------------

# Encoders

# ------------------------------------------------------------------------------

encoder:

# ------------------------------ FGS1 Encoder -------------------------------

fgs1:
type: "mamba"                     # Structured State Space Model (Mamba/SSM)
input\_shape: \[135000, 32, 32]     # \[T, H, W] raw frame cube length & aperture (sim)
d\_model: 128
depth: 12
dropout: 0.10
residual: true
layer\_norm: true
positional\_encoding: "fourier"    # fourier | learned | none
fourier:
max\_period: 100000.0
n\_frequencies: 64
concat\_input: true
gating:                           # optional channel/time gating for stability
enabled: true
type: "silu"                    # silu | sigmoid | gelu
downsampling:
enabled: true
method: "avgpool"               # avgpool | strided\_conv | none
stride\_time: 4                  # reduces effective sequence length 4x
augmentation:
jitter\_injection: true          # tiny phase/jitter augmentation (diagnostics-aware)
noise\_std: 0.005
export\_latents: true              # expose latent features for fusion/diagnostics
attention\_trace: false            # SSMs donâ€™t use attention; kept for interface symmetry
notes: "Mamba SSM for ultra-long photometry; downsampled for runtime while preserving transit shape"

# ------------------------------ AIRS Encoder ------------------------------

airs:
type: "gnn"                       # Graph encoder over spectral bins
input\_shape: \[283]                # 283 wavelength bins (AIRS)
gnn\_type: "gat"                   # gat | rgcn | mpnn | nnconv
hidden\_dim: 128
num\_layers: 6
dropout: 0.10
residual: true
layer\_norm: true
activation: "gelu"
positional\_encoding: "fourier"    # per-bin PE across wavelength axis
fourier:
n\_frequencies: 32
normalize: true
graph\_builder:
kind: "hybrid"                  # knn | radius | hybrid
knn\_k: 8
radius: 3.0e-3                  # in normalized wavelength units (if used)
add\_self\_loops: true
undirected: true
edge\_features:
wavelength\_proximity: true
molecule\_links: true
detector\_region: true
edge\_feature\_dims:
wavelength\_proximity: 1
molecule\_links: 8
detector\_region: 4
weights:
wavelength\_proximity: 1.0
molecule\_links: 1.0
detector\_region: 0.5
attention:
heads: 4                        # for GAT-style layers
concat\_heads: true
dropout: 0.10
return\_coeffs: true             # export attention for explainability
export\_latents: true
validate\_graph\_shapes: true       # check edge\_index vs edge\_attr size consistency
notes: "Spectral bins as nodes; edges encode smoothness & molecular priors with optional region adjacency"

# ------------------------------------------------------------------------------

# Fusion

# ------------------------------------------------------------------------------

fusion:
method: "cross\_attention"           # concat | cross\_attention | gated\_sum
hidden\_dim: 256
num\_heads: 4
dropout: 0.10
normalize: true
residual: true
gating:
enabled: true                     # used by gated\_sum and to modulate cross-attn output
type: "sigmoid"
init: "xavier\_uniform"
export\_attention: true              # store fusion attention maps for diagnostics
notes: "FGS1 latent guides AIRS spectral reasoning via cross-attention; fall back to concat/gated\_sum on small GPUs"

# ------------------------------------------------------------------------------

# Decoders

# ------------------------------------------------------------------------------

decoder:

# Mean spectrum Î¼

mean\_head:
type: "mlp"                       # mlp | transformer | graph\_readout
hidden\_dim: 256
layers: 3
activation: "gelu"
dropout: 0.10
residual: true
layer\_norm: true
out\_activation: "identity"        # identity (Î¼ is unconstrained, later clamped by constraints)
weight\_norm: false

# Uncertainty Ïƒ

sigma\_head:
type: "flow\_uncertainty"          # learned flow/normalizing-flow-inspired uncertainty head
hidden\_dim: 128
layers: 2
activation: "softplus"            # enforce Ïƒ > 0
dropout: 0.10
residual: true
layer\_norm: false
min\_sigma: 1.0e-5                 # numerical floor
symbolic\_overlay: true            # reweight Ïƒ using symbolic/physics priors
overlay:
molecules\_weight: 0.25          # upweight Ïƒ near molecular bands
smoothness\_weight: 0.25
lensing\_weight: 0.15
calibration\_weight: 0.35        # integrates COREL / temperature scaling post-hoc if enabled
export\_maps: true                 # save per-bin Ïƒ and overlay contributions for diagnostics

optional:
quantile: false                   # enable quantile regression head(s)
diffusion: false                  # enable diffusion decoder (experimental)
quantile\_levels: \[0.05, 0.5, 0.95]

# ------------------------------------------------------------------------------

# Physics / Symbolic Constraints (loss-side, applied during training/validation)

# ------------------------------------------------------------------------------

constraints:
enforce\_nonnegativity: true         # clamp Î¼ â‰¥ 0 at output or via loss barrier
nonnegativity\_barrier: 1.0e-6
smoothness\_weight\_time: 0.0005      # temporal smoothness (FGS1 guidance)
smoothness\_weight\_wavelength: 0.010 # spectral smoothness (AIRS)
asymmetry\_weight: 0.001             # penalize unphysical antisymmetry in Î¼
fft\_smoothness\_weight: 0.002        # FFT energy penalty to encourage low-freq coherence
molecular\_fingerprint:
enabled: true
molecules: \["H2O", "CO2", "CH4"]
band\_expansion\_bins: 1            # widen bands by Â±bins when weighting
weight: 0.25
gravitational\_lensing\_overlay:
enabled: true
bins: \[120, 160]                  # AIRS region to weight if lensing is suspected
weight: 0.15

# Optional symbolic logic engine integration (rule routing/weights)

symbolic\_logic:
enabled: true
mode: "soft"                      # soft | hard
per\_rule\_weights: {}              # e.g., {"smoothness": 1.0, "nonneg": 1.0, "molecule\_coherence": 0.5}

# ------------------------------------------------------------------------------

# Training hooks (diagnostics / aux losses / exports)

# ------------------------------------------------------------------------------

training\_hooks:
spectral\_smoothness: true
shap\_overlay: true
symbolic\_violation\_loss: true
latent\_umap\_logging: true
tsne\_logging: false
diagnostics\_html: true
attention\_logging: true             # export fusion + GAT attention maps
fft\_profile\_logging: true
calibration\_checks: true            # run Ïƒ vs residual calibration evaluation
rule\_violation\_table: true
export\_every\_n\_epochs: 1            # diagnostics cadence (guarded for CI/Kaggle)

# ------------------------------------------------------------------------------

# Uncertainty calibration (post-hoc; called in pipeline, but kept here for cohesion)

# ------------------------------------------------------------------------------

calibration:
enabled: true
temperature\_scaling:
enabled: true
init\_temp: 1.0
bounds: \[0.05, 10.0]
corel:
enabled: true
gnn\_backend: "gat"                # reuse AIRS GNN infra for COREL if desired
hidden\_dim: 64
num\_layers: 3
dropout: 0.10
coverage\_target: 0.90             # nominal predictive interval coverage
per\_bin: true
molecule\_weighting: 0.25
export\_heatmaps: true

# ------------------------------------------------------------------------------

# Runtime & Export Guardrails

# ------------------------------------------------------------------------------

runtime:
jit: true
torchscript: true
allow\_half: true                    # enable AMP/bfloat16 in training code (guarded by CLI)
kaggle\_safe: true
param\_limit: 50000000               # â‰¤ 50M parameters
check\_param\_count: true
max\_hours\_hint: 9                   # informational; scheduling hints in CLI
deterministic\_ops: true             # torch deterministic flags where feasible
matmul\_precision: "high"            # high | medium | low (forward-compatible hint)

export:
onnx: true
opset: 17
traced\_model\_path: "artifacts/models/v50\_traced.pt"
onnx\_path: "artifacts/models/v50.onnx"
hf\_hub: false
save\_attention\_artifacts: true
save\_symbolic\_artifacts: true

# ------------------------------------------------------------------------------

# Metrics / Loss routing (model-local weights; global trainer may further scale)

# ------------------------------------------------------------------------------

loss:
gll\_weight: 1.0                     # Gaussian log-likelihood for (Î¼, Ïƒ)
smoothness\_time\_weight: "\${constraints.smoothness\_weight\_time}"
smoothness\_wavelength\_weight: "\${constraints.smoothness\_weight\_wavelength}"
asymmetry\_weight: "\${constraints.asymmetry\_weight}"
fft\_weight: "\${constraints.fft\_smoothness\_weight}"
symbolic\_weight: 0.25
calibration\_penalty\_weight: 0.10    # penalize mis-calibrated Ïƒ during training (aux metric â†’ loss)

# ------------------------------------------------------------------------------

# Explainability / Debug

# ------------------------------------------------------------------------------

explainability:
save\_latents: true
save\_umap\_html: true
save\_tsne\_html: false
save\_shap\_plots: true
save\_symbolic\_overlay: true
save\_fft\_plots: true
max\_examples: 64
html\_report\_path: "artifacts/diagnostics/v50\_diagnostics.html"

# ------------------------------------------------------------------------------

# Sanity / Self-test expectations (light checks used by selftest.py and CI)

# ------------------------------------------------------------------------------

selftest:
expect\_keys:
\- "encoder.fgs1.type"
\- "encoder.airs.gnn\_type"
\- "fusion.method"
\- "decoder.mean\_head.type"
\- "decoder.sigma\_head.type"
\- "constraints.enforce\_nonnegativity"
param\_ceiling: "\${runtime.param\_limit}"
allow\_export\_paths:
\- "\${export.traced\_model\_path}"
\- "\${export.onnx\_path}"
require\_attention\_when\_enabled: true
require\_graph\_shape\_checks: true

# ------------------------------------------------------------------------------

# Free-form notes (persisted with run manifest for provenance)

# ------------------------------------------------------------------------------

notes: |
SpectraMind V50 model config:
â€¢ FGS1 encoder: Mamba/SSM with Fourier PE, optional time downsampling for runtime.
â€¢ AIRS encoder: GAT with hybrid graph (k-NN + radius) and rich edge features (molecule/region/proximity).
â€¢ Fusion: cross-attention by default; export attention maps for diagnostics.
â€¢ Decoders: Î¼ via MLP; Ïƒ via flow-style head with symbolic/physics overlays and COREL-aware weighting.
â€¢ Constraints: nonnegativity barrier, multi-axis smoothness, FFT prior, molecular fingerprints, lensing region.
â€¢ Calibration: temperature scaling + COREL GNN; exports per-bin coverage heatmaps.
â€¢ Guardrails: param count limit, deterministic flags, TorchScript/ONNX export paths.
â€¢ Explainability: UMAP/SHAP/symbolic overlays and attention traces saved to HTML report.
â€¢ This file is Hydra-composable; override any leaf with `+key=value` in the CLI.
