# configs/optimizer/adam.yaml
# ==============================================================================
# ‚öôÔ∏è Optimizer ‚Äî Adam (baseline adaptive optimizer, upgraded)
#
# Purpose
#   Define Adam optimizer hyperparameters for SpectraMind V50 training.
#   Adam is the canonical baseline optimizer for noisy exoplanet spectroscopy
#   data (FGS1 + AIRS). This config is Hydra-safe, CLI-overridable, and logged
#   for full reproducibility.
#
# Usage
#   Default in train.yaml via:
#     defaults:
#       - optimizer: adam
#
#   Override examples:
#     spectramind train optimizer=adam optimizer.lr=1e-3 optimizer.weight_decay=0.0
#     spectramind train optimizer=adam optimizer.lookahead.enabled=true
#
# References
#   ‚Ä¢ SpectraMind V50 Technical Plan:contentReference[oaicite:3]{index=3}
#   ‚Ä¢ Kaggle runtime guardrails: ‚â§9h GPU safe:contentReference[oaicite:4]{index=4}
#   ‚Ä¢ NASA-grade reproducibility: Hydra + DVC + CI:contentReference[oaicite:5]{index=5}
#   ‚Ä¢ Hugging Face Accelerate + PyTorch AMP integration:contentReference[oaicite:6]{index=6}:contentReference[oaicite:7]{index=7}
# ==============================================================================

optimizer:
  name: "adam"                   # string identifier
  type: "Adam"                   # torch.optim.Adam
  lr: 3.0e-4                     # base learning rate
  betas: [0.9, 0.999]            # momentum terms
  eps: 1.0e-8                    # numerical stability
  weight_decay: 0.01             # L2 penalty (see adamw.yaml for decoupled)
  amsgrad: false                 # AMSGrad variant toggle

  # --------------------------------------------------------------------------
  # üöÄ Advanced Variants
  # --------------------------------------------------------------------------
  decoupled: false               # true ‚Üí AdamW semantics
  lookahead:                     # Lookahead meta-optimizer (optional)
    enabled: false
    alpha: 0.5                   # interpolation factor
    k: 6                         # synchronization steps
  gradient_centralization: false # normalize gradients before update
  adaptive_weight_decay: false   # scale decay per-parameter group

  # --------------------------------------------------------------------------
  # Scheduler / Runtime Hooks
  # --------------------------------------------------------------------------
  scheduler_hook: true           # allow override (cosine, onecycle, poly)
  warmup_steps: 0                # linear warmup steps before steady LR
  clip_grad_norm: 1.0            # gradient norm clipping (None = disable)
  accumulate_steps: 1            # gradient accumulation (amp safe)

  # --------------------------------------------------------------------------
  # Precision / Kernel Optimizations
  # --------------------------------------------------------------------------
  precision_safe: true           # AMP/mixed precision compatibility
  fused: false                   # use fused Adam kernel if available
  bf16_preferred: true           # prefer bfloat16 on A100 GPUs
  detect_anomaly: false          # enable anomaly detection in backward pass

  # --------------------------------------------------------------------------
  # Diagnostics & Logging
  # --------------------------------------------------------------------------
  log_config: true
  export_json: "outputs/diagnostics/optimizer_adam.json"
  rich_console: true             # show table in CLI with Rich
  hash_to_debuglog: true         # write config hash ‚Üí logs/v50_debug_log.md

  # --------------------------------------------------------------------------
  # Notes
  # --------------------------------------------------------------------------
  notes: |
    ‚Ä¢ Default lr = 3e-4, tuned for leaderboard-safe runs:contentReference[oaicite:8]{index=8}.
    ‚Ä¢ Set decoupled=true or use optimizer/adamw.yaml for AdamW.
    ‚Ä¢ Supports Lookahead, gradient centralization, adaptive decay.
    ‚Ä¢ Fully Hydra-overridable from CLI, logged to DVC/CI:contentReference[oaicite:9]{index=9}.
    ‚Ä¢ AMP/bf16 safe (recommended: bf16 on A100 GPUs):contentReference[oaicite:10]{index=10}.
    ‚Ä¢ Compatible with Hugging Face Accelerate + TorchScript export:contentReference[oaicite:11]{index=11}.
    ‚Ä¢ Kaggle-verified ‚â§9h runtime, tuned for ~1100 planets:contentReference[oaicite:12]{index=12}.
