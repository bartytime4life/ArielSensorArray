# configs/optimizer/adamw.yaml
# ==============================================================================
# ‚öôÔ∏è Optimizer ‚Äî AdamW (decoupled weight decay; recommended baseline)
#
# Purpose
#   Configure the AdamW optimizer for SpectraMind V50 training.
#   AdamW is preferred over Adam due to its decoupled weight decay formulation,
#   which improves generalization stability and is widely used in modern deep learning.
#
# Usage
#   Composed in train.yaml via:
#     defaults:
#       - optimizer: adamw
#
#   Override examples:
#     spectramind train optimizer=adamw optimizer.lr=5e-4 optimizer.weight_decay=0.02
# ==============================================================================

optimizer:
  name: "adamw"                  # identifier string; consumed by our trainer
  type: "AdamW"                  # torch.optim.AdamW
  lr: 3.0e-4                     # base learning rate (scheduler may override)
  betas: [0.9, 0.999]            # momentum terms
  eps: 1.0e-8                    # numerical stability constant
  weight_decay: 0.01             # decoupled weight decay (preferred vs L2)
  amsgrad: false                 # AMSGrad toggle (rarely needed for AdamW)

  # --------------------------------------------------------------------------
  # üî¨ Parameter groups (optional quality-of-life helpers)
  # --------------------------------------------------------------------------
  # When true, the trainer can auto-build param groups so that biases and norm/BN
  # parameters use weight_decay=0.0. (The model-to-optimizer bridge implements this.)
  auto_param_groups:
    enabled: true
    no_decay_patterns: ["bias", "LayerNorm.weight", "layer_norm.weight", "ln", "norm", "bn", "BatchNorm.weight"]
    override_no_decay_weight: 0.0

  # --------------------------------------------------------------------------
  # üöÄ Advanced variants
  # --------------------------------------------------------------------------
  lookahead:
    enabled: false               # Wrap AdamW in Lookahead for extra stability
    alpha: 0.5                   # slow weight step size
    k: 6                         # synchronization steps
  gradient_centralization: false # normalize gradients before update
  adaptive_weight_decay: false   # per-group adaptive decay (off by default)

  # --------------------------------------------------------------------------
  # üìà Scheduler / runtime hooks
  # --------------------------------------------------------------------------
  scheduler_hook: true           # allow cosine/onecycle/poly/etc. via config
  warmup_steps: 0                # linear warmup steps before steady LR
  clip_grad_norm: 1.0            # gradient norm clipping (None/0 to disable)
  accumulate_steps: 1            # gradient accumulation for large effective batch

  # --------------------------------------------------------------------------
  # ‚öôÔ∏è Precision / kernel optimizations
  # --------------------------------------------------------------------------
  precision_safe: true           # AMP/mixed precision compatibility
  bf16_preferred: true           # prefer bfloat16 on A100/modern GPUs
  fused: false                   # enable if torch has fused AdamW kernel
  detect_anomaly: false          # PyTorch autograd anomaly detection

  # --------------------------------------------------------------------------
  # üß™ Diagnostics & logging
  # --------------------------------------------------------------------------
  log_config: true
  export_json: "outputs/diagnostics/optimizer_adamw.json"
  rich_console: true             # pretty-print optimizer table in CLI
  hash_to_debuglog: true         # append config hash ‚Üí logs/v50_debug_log.md

  # --------------------------------------------------------------------------
  # üìù Notes
  # --------------------------------------------------------------------------
  notes: |
    ‚Ä¢ AdamW is the recommended optimizer for SpectraMind V50.
    ‚Ä¢ Default lr = 3e-4 is a safe baseline for Kaggle ‚â§9h GPU budgets.
    ‚Ä¢ Decoupled weight decay avoids the implicit coupling of L2 in Adam.
    ‚Ä¢ Auto param groups can zero decay for biases/normalization params.
    ‚Ä¢ Supports Lookahead, gradient centralization, adaptive decay toggles.
    ‚Ä¢ Fully Hydra-overridable and AMP/bf16-ready; logs JSON + CLI table.
