# configs/loss/gll.yaml
# ==============================================================================
# ðŸ“‰ Gaussian Log-Likelihood (GLL) Loss â€” Primary Challenge Metric (UPGRADED)
#
# Purpose
#   Implements the competition's Gaussian log-likelihood:
#     L = 0.5 * Î£_bins [ log(2Ï€ÏƒÂ²) + (y - Î¼)Â² / ÏƒÂ² ]
#   where predictions are (Î¼, Ïƒ) per wavelength bin. Penalizes both inaccurate Î¼
#   and poorly calibrated Ïƒ, and serves as the anchor objective for training.
#
# Highlights
#   â€¢ AMP-safe numerics with global eps, Ïƒ clamping, and dtype control.
#   â€¢ Flexible per-bin weighting (uniform/molecule/entropy/custom + masks).
#   â€¢ Optional partial-Ïƒ gradient flow (detach) for ablations.
#   â€¢ Scheduling/warmup to ramp GLL impact or couple with regularizers.
#   â€¢ Symbolic hooks (smoothness/FFT/nonneg/molecule) for physics priors.
#   â€¢ Calibration integration stubs (temperature scaling, COREL, per-bin scaling).
#   â€¢ Rich diagnostics for dashboards and CI (per-bin, heatmap, JSON).
#
# Usage
#   In train.yaml:
#     defaults:
#       - loss: gll
#
#   CLI examples:
#     spectramind train loss.gll.sigma_min=1e-3 loss.gll.weighting=molecule
#     spectramind train loss.gll.calibration.temperature.enabled=true \
#                      loss.gll.calibration.temperature.T=1.15
#     spectramind train loss.gll.weighting=custom \
#                      loss.gll.custom_weights_path=weights/custom_bins.npy
# ==============================================================================

gll:
  enabled: true

  # -----------------------------------------
  # Numerics / Stability
  # -----------------------------------------
  reduction: "mean"            # mean | sum | none
  sigma_min: 1.0e-4            # lower bound on Ïƒ for log/ratio stability
  clamp_sigma: true            # clamp Ïƒ to [sigma_min, +inf)
  detach_sigma_grad: false     # if true, stop âˆ‚L/âˆ‚Ïƒ (useful for ablations)
  eps: 1.0e-12                 # global small epsilon for AMP/log/ratio stability
  dtype: "auto"                # auto | float32 | float64 (hint to implementation)

  # -----------------------------------------
  # Scheduling / Curriculum
  # -----------------------------------------
  schedule:
    enabled: true
    # Effective loss multiplier m(step) = warmup * anneal (both clamped in [floor, cap]).
    warmup:
      steps: 0                 # 0 disables warmup (starts at full strength)
    anneal:
      enabled: false
      steps: 0
    floor: 0.0
    cap: 1.0

  # -----------------------------------------
  # Weighting Strategy
  # -----------------------------------------
  weighting: "uniform"         # uniform | molecule | entropy | custom
  custom_weights_path: null    # .npy/.csv, shape [BINS] or [B,BINS]
  custom_weights_inline: []    # e.g., [1.0, 0.5, 0.5, ...]
  # Optional region mask to modulate weights (post-mix), e.g., continuum/line masks
  region_masking:
    enabled: false
    mask_path: null            # .npy/.csv in [0..1], broadcastable to per-bin shape
    invert_mask: false
    scale: 1.0

  # Optional molecule/entropy sources used when weighting=molecule/entropy
  sources:
    molecule_regions_path: null   # mask where molecule lines are present
    entropy_scores_path: null     # per-bin entropy (higher => larger weight)
    # Normalization of external weights
    normalize_external: true

  # -----------------------------------------
  # Calibration Integration (optional)
  # -----------------------------------------
  calibration:
    # Global post-hoc temperature scaling on Ïƒ (Ïƒ' = T*Ïƒ); applied at loss time
    temperature:
      enabled: false
      T: 1.0
    # Per-bin scaling factors (e.g., from validation coverage analysis); multiplies Ïƒ
    per_bin_scale:
      enabled: false
      path: null              # .npy/.csv shape [BINS] or [B,BINS]
      normalize: false
    # COREL/graphical conformal coverage (placeholder: implementation reads these)
    corel:
      enabled: false
      # paths to learned coverage parameters/graph edges/etc.
      params_path: null
      graph_path: null

  # -----------------------------------------
  # Symbolic / Physics Hooks (optional)
  # -----------------------------------------
  symbolic:
    smoothness_penalty: 0.0     # L2 curvature weight on Î¼ (discrete Laplacian)
    nonnegativity: true         # if true, add soft non-negativity penalty on Î¼
    fft_suppression: 0.0        # weight on FFT-domain high-frequency penalty
    molecule_consistency: 0.0   # encourage coherence with molecular bands
    # Fine-grained knobs (consumed by the symbolic engine if present)
    details:
      smoothness_order: 2       # 1 | 2
      fft_cutoff: 0.25          # fraction of Nyquist
      nonneg_mode: "hinge"      # hinge | softplus | exp
      nonneg_p: 2               # 1 or 2

  # -----------------------------------------
  # Diagnostics / Logging
  # -----------------------------------------
  log_per_bin: true             # record per-bin GLL terms during diagnostics
  export_heatmap: true          # export heatmap of per-bin contributions
  export_weights: false         # export effective per-bin weights after mixing/masking
  export_sigma_stats: true      # export summary of Ïƒ distribution (min/median/max, % clamped)
  save_json: true               # persist the effective loss config used at runtime

  # -----------------------------------------
  # Developer / Ablation Toggles
  # -----------------------------------------
  dev:
    # When true, compute separate aggregates for data-fit vs. log-variance parts,
    # enabling fine-grained dashboards (e.g., compare components over training).
    split_terms: true
    # If true, exclude bins with NaN/Inf target y from loss (mask them out)
    ignore_invalid_targets: true
    # Optional hard cap for individual per-bin contributions (outlier safety)
    per_bin_clip: null         # e.g., 1.0e3 (null = disabled)

  notes: |
    â€¢ GLL anchors training: it is sensitive to both Î¼ error and Ïƒ calibration; Ïƒ too small yields
      large residual terms, Ïƒ too large increases the log-variance term.
    â€¢ Keep sigma_min > 0 with clamp_sigma=true to avoid numerical issues in AMP/mixed precision.
    â€¢ Use weighting/masks to emphasize scientifically important bins (e.g., molecules) or entropy-rich
      regions; enable export_weights to audit effective bin influence.
    â€¢ Calibration stubs (temperature/per-bin/COREL) allow post-hoc uncertainty alignment without
      changing model code: they multiply Ïƒ inside the loss flow so diagnostics reflect calibrated Ïƒ.
    â€¢ Symbolic hooks allow additive physics priors; keep these moderate to avoid eclipsing GLL.
    â€¢ Schedule can ramp GLL (or be used inversely to ramp regularizers in your trainer code).
