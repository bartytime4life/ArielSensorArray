# configs/trainer/multi_gpu.yaml
# ==============================================================================
# ðŸ§© Multi-GPU (Single Node) Trainer â€” SpectraMind V50
#
# Purpose
#   Efficient single-node distributed training with PyTorch Lightning DDP.
#   â€¢ Auto-detect GPUs (or override: trainer.devices=4)
#   â€¢ AMP/bf16 for throughput; safe gradient clipping; resumable checkpoints
#   â€¢ Clean Hydra interpolation to compose with local/* profiles
#
# Usage
#   spectramind train trainer=multi_gpu
#   # Examples:
#   spectramind train trainer=multi_gpu trainer.devices=4
#   spectramind train trainer=multi_gpu trainer.max_epochs=50 trainer.accumulate_grad_batches=2
# ==============================================================================

trainer:
  # ---------------------------------------------------------------------------
  # Hardware / Strategy
  # ---------------------------------------------------------------------------
  accelerator: "gpu"
  devices: ${local.devices, "auto"}   # int (e.g., 2,4,8) or "auto"
  num_nodes: 1
  strategy:
    class_path: lightning.pytorch.strategies.DDPStrategy
    init_args:
      find_unused_parameters: false
      static_graph: true               # set false if your graph changes per step
      gradient_as_bucket_view: true
      bucket_cap_mb: 50                # 25â€“50 MB typical; tune per model
      broadcast_buffers: false
      process_group_backend: "nccl"
      timeout: 1800                    # seconds; avoid hanging on init

  # ---------------------------------------------------------------------------
  # Precision & Performance
  # ---------------------------------------------------------------------------
  # Prefer bf16 on Ampere/Hopper; fallback to 16 if not set in local/*
  precision: ${local.precision, 16}
  deterministic: ${local.deterministic, true}
  benchmark: false                     # set true only if shapes are fully static
  sync_batchnorm: true                 # keep BN stats consistent across ranks
  replace_sampler_ddp: true            # ensure DistributedSampler is used

  # ---------------------------------------------------------------------------
  # Runtime
  # ---------------------------------------------------------------------------
  max_epochs: ${local.max_epochs, 40}
  max_time: null                       # e.g., "12:00:00" (H:MM:SS)
  accumulate_grad_batches: ${local.accumulate_grad_batches, 1}
  gradient_clip_val: ${local.grad_clip_norm, 1.0}
  log_every_n_steps: ${local.log_every_n_steps, 50}
  # ckpt_path is provided to Trainer.fit(), kept here to allow Hydra wiring
  ckpt_path: null

  # ---------------------------------------------------------------------------
  # Checkpointing (ModelCheckpoint callback)
  # ---------------------------------------------------------------------------
  checkpoint:
    enabled: true
    dirpath: "${local.output_dir, outputs}/checkpoints/"
    save_top_k: 2
    monitor: "val/gll"
    mode: "min"
    save_last: true
    every_n_epochs: 2
    filename: "epoch{epoch}-valgll{val/gll:.5f}"

  # ---------------------------------------------------------------------------
  # Early Stopping (EarlyStopping callback)
  # ---------------------------------------------------------------------------
  early_stopping:
    enabled: true
    monitor: "val/gll"
    patience: 6
    mode: "min"

  # ---------------------------------------------------------------------------
  # Logging
  # ---------------------------------------------------------------------------
  logger:
    type: "tensorboard"                # can be swapped via logger group
    save_dir: "${local.log_dir, logs}/"
    name: "multi_gpu"
    version: "run_${now:%Y-%m-%d_%H-%M-%S}"

  # ---------------------------------------------------------------------------
  # DataLoader guidance for single-node multi-GPU
  # ---------------------------------------------------------------------------
  dataloader:
    num_workers: ${local.num_workers, 8}
    prefetch_factor: 2
    pin_memory: ${local.pin_memory, true}
    persistent_workers: ${local.persistent_workers, true}

  # ---------------------------------------------------------------------------
  # Seeds
  # ---------------------------------------------------------------------------
  seed: ${local.seed, 42}

  # ---------------------------------------------------------------------------
  # Notes
  # ---------------------------------------------------------------------------
  notes: |
    Single-node DDP optimized for throughput with stable defaults:
    â€¢ AMP/bf16 + gradient clipping for numerical safety.
    â€¢ DDP(find_unused_parameters=false) to reduce overhead.
    â€¢ static_graph=true reduces DDP sync overhead for stable graphs.
    â€¢ sync_batchnorm keeps BN statistics consistent across ranks.
    â€¢ Increase accumulate_grad_batches if batch size is memory-bound.
    â€¢ Recommended env for stability in launch scripts:
        NCCL_ASYNC_ERROR_HANDLING=1
        NCCL_DEBUG=WARN
        TORCH_DISTRIBUTED_DEBUG=OFF
        CUDA_LAUNCH_BLOCKING=0
