# configs/trainer/kaggle_safe.yaml
# ==============================================================================
# ðŸš¦ Kaggle-Safe Trainer Config â€” SpectraMind V50
#
# Purpose
#   Provide Kaggle runtimeâ€“compatible training defaults:
#   â€¢ Runtime budget: â‰¤ 9 hours
#   â€¢ GPU safe (T4/P100 class devices, mixed precision)
#   â€¢ Checkpoint frequency throttled to avoid storage issues
#   â€¢ Log-friendly, DVC/CI compatible
#
# Usage
#   In train.yaml (or CLI):
#     defaults:
#       - trainer: kaggle_safe
#
#   Example override:
#     spectramind train trainer=kaggle_safe trainer.max_epochs=40
#
# Notes
#   â€¢ Designed for leaderboard submissions and CI smoke runs.
#   â€¢ For deep research or ablations, prefer trainer=defaults.
# ==============================================================================

trainer:
  # ---------------------------------------------------------------------------
  # Backend / Devices
  # ---------------------------------------------------------------------------
  accelerator: "auto"            # let Lightning detect GPU
  devices: 1                     # Kaggle typically exposes a single GPU per session
  strategy: "ddp_find_unused_parameters_false"
  precision: 16                  # AMP mixed precision (saves memory, speeds up)

  # ---------------------------------------------------------------------------
  # Runtime Guardrails
  # ---------------------------------------------------------------------------
  max_epochs: 30                 # default epoch budget for â‰¤9h
  max_time: "8:30:00"            # safety wall (< 9h)
  accumulate_grad_batches: 2     # batch safety on limited VRAM
  gradient_clip_val: 1.0         # prevent exploding gradients

  # ---------------------------------------------------------------------------
  # Checkpointing (throttled I/O)
  # ---------------------------------------------------------------------------
  enable_checkpointing: true
  checkpoint:
    enabled: true
    dirpath: "checkpoints/"
    save_top_k: 1
    monitor: "val/gll"
    mode: "min"
    save_last: true
    every_n_epochs: 5            # reduce storage/time overhead

  # ---------------------------------------------------------------------------
  # Early Stopping (runtime safety)
  # ---------------------------------------------------------------------------
  early_stopping:
    enabled: true
    monitor: "val/gll"
    patience: 5
    mode: "min"

  # ---------------------------------------------------------------------------
  # Logging (Kaggle-friendly)
  # ---------------------------------------------------------------------------
  logger:
    type: "tensorboard"          # lightweight and familiar in Kaggle
    save_dir: "logs/"
    name: "kaggle_safe"

  # ---------------------------------------------------------------------------
  # Determinism & Reproducibility
  # ---------------------------------------------------------------------------
  deterministic: true
  benchmark: false
  seed: 42

  # ---------------------------------------------------------------------------
  # Dataloader Settings (Kaggle-safe defaults)
  # ---------------------------------------------------------------------------
  dataloader:
    num_workers: 2               # conservative on P100/T4 class sessions
    prefetch_factor: 2
    pin_memory: true
    persistent_workers: false

  # ---------------------------------------------------------------------------
  # Profiling
  # ---------------------------------------------------------------------------
  profiler: null                 # disabled by default to save time

  # ---------------------------------------------------------------------------
  # Notes
  # ---------------------------------------------------------------------------
  notes: |
    Kaggle-safe trainer ensures jobs complete under 9h with P100/T4 GPUs.
    â€¢ AMP precision 16 saves memory and accelerates training.
    â€¢ Gradient accumulation balances batch size with GPU memory constraints.
    â€¢ Checkpoints throttled to reduce storage overhead.
    â€¢ Early stopping prevents wasted epochs on plateaus.
    â€¢ Determinism enabled for full reproducibility.

# Optional dataset limiting knobs (honored by your DataModule, if implemented)
limits:
  train_frac: 1.0               # e.g., 0.5 to halve training set for quick iteration
  val_frac: 1.0
  max_train_batches: null       # set int to cap batches (e.g., 300)
  max_val_batches: null

# Hydra output dirs (helpful in notebook environments)
hydra:
  run:
    dir: outputs/${now:%Y-%m-%d}/${now:%H-%M-%S}_kaggle_safe
  sweep:
    dir: multiruns/${now:%Y-%m-%d_%H-%M-%S}_kaggle_safe
    subdir: ${hydra.job.num}
  job_logging:
    root:
      level: INFO
