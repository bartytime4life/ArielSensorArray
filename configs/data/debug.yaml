# ======================================================================
# configs/data/debug.yaml — SpectraMind V50 Debug / CI Smoke Dataset
# ======================================================================
# Purpose
#   Tiny, deterministic slice for CI/self-test and local smoke runs.
#   Validates end-to-end pipeline (calibration → dataload → 1 training step)
#   in seconds without heavy I/O or GPU demand.
#
# Usage
#   spectramind train    --config-name train data=debug training.epochs=1
#   spectramind diagnose --config-name train data=debug diagnostics.fft=false
#
# Notes
#   • Keep this config hermetic: no internet, no remote pulls, tiny files only.
#   • Shapes & dtypes align with nominal to exercise the same codepaths.
#   • Meant to pair with toy/debug artifacts (tracked via DVC or generated).
#   • Compatible with new toy/debug generator (npz/pkl formats, manifest, splits).
# ======================================================================

# ----------------------------------------------------------------------
# Hydra defaults (quiet logs for CI)
# ----------------------------------------------------------------------
defaults:
  - override hydra/job_logging: default
  - override hydra/hydra_logging: default

# ----------------------------------------------------------------------
# Dataset metadata & debug paths
# ----------------------------------------------------------------------
dataset:
  name: "Ariel_Debug_Slice"
  provider: "local"                          # tiny local/debug subset
  input_dir: "data/debug"                    # e.g., DVC-tracked tiny files
  output_dir: "outputs/debug"                # writable artifacts for smoke runs
  cache_dir: "temp/debug_cache"
  version: "1.1-debug"
  source: "debug-slice"
  # Tiny slice for speed (e.g., 5 planets × full 283 bins)
  num_planets: 5
  num_bins: 283
  t_fgs1: 64
  t_airs: 32
  channels_fgs1: 32
  file_priority:                             # loader will use the first existing path per key
    arrays_first: true                       # prefer array containers for speed if present (npz>npy>pkl)

paths:
  # Packed debug tensors (PKL bundle; same semantics as arrays)
  train_file_pkl: "${dataset.input_dir}/train_debug.pkl"
  val_file_pkl:   "${dataset.input_dir}/val_debug.pkl"
  test_file_pkl:  "${dataset.input_dir}/test_debug.pkl"

  # Array exports (NEW generator defaults to .npz; fall back to .npy if present)
  # Train arrays
  fgs1_train_npz: "${dataset.input_dir}/fgs1_train.npz"   # contains key 'fgs1' → (N, T_fgs1, 32)
  airs_train_npz: "${dataset.input_dir}/airs_train.npz"   # contains key 'airs' → (N, T_airs, 283 or 356)
  fgs1_train_npy: "${dataset.input_dir}/fgs1_train_debug.npy"
  airs_train_npy: "${dataset.input_dir}/airs_train_debug.npy"

  # Test arrays
  fgs1_test_npz: "${dataset.input_dir}/fgs1_test.npz"
  airs_test_npz: "${dataset.input_dir}/airs_test.npz"
  fgs1_test_npy: "${dataset.input_dir}/fgs1_test_debug.npy"
  airs_test_npy: "${dataset.input_dir}/airs_test_debug.npy"

  # Labels & splits (optional but preferred for debug determinism)
  labels_csv:        "${dataset.input_dir}/labels_debug.csv"
  toy_manifest_json: "${dataset.input_dir}/toy_manifest.json"
  # Generated by toy/debug generator:
  split_indices_dir_src: "${dataset.input_dir}/splits"
  split_indices_dir:     "${dataset.output_dir}/splits"    # copied/cache for run

  # Binmap for 356→283 (if AIRS arrays are in 356)
  binmap_356_to_283: "${dataset.input_dir}/binmap_356_to_283_debug.npy"

  artifacts_dir: "${dataset.output_dir}/diagnostics"
  tensor_cache_dir: "${dataset.cache_dir}/tensors"
  calibrated_dir: "${dataset.output_dir}/calibrated"

io:
  allow_pickle: true
  allow_npz: true
  memmap: false                # keep off for tiny debug files
  strict_endianness: false
  fail_on_missing: true
  internet_access: false
  external_urls: []
  prefer_npz: true             # try .npz, then .npy, then .pkl (if file_priority.arrays_first)

# ----------------------------------------------------------------------
# Interface contract (kept identical to nominal to hit same codepaths)
# ----------------------------------------------------------------------
interface:
  num_bins: ${dataset.num_bins}
  t_fgs1: ${dataset.t_fgs1}
  t_airs: ${dataset.t_airs}
  channels_fgs1: ${dataset.channels_fgs1}
  output_dir: "${hydra.run.dir}"

# ----------------------------------------------------------------------
# Schema (light but fail-fast)
# ----------------------------------------------------------------------
schema:
  fgs1:
    expected_rank: 3
    # Accept either (N, T_fgs1, 32) or historical (N, 32, 32) and transpose if needed
    accepted_shapes:
      - [-1, ${dataset.t_fgs1}, ${dataset.channels_fgs1}]
      - [-1, ${dataset.channels_fgs1}, ${dataset.channels_fgs1}]  # legacy smoke export
    dtype: "float32"
    allow_nan: false
    allow_inf: false
  airs:
    expected_rank: 3
    # Accept 283 (preferred) or 356 (remap to 283 if bin_remap.enabled)
    accepted_shapes:
      - [-1, ${dataset.t_airs}, ${dataset.num_bins}]
      - [-1, ${dataset.t_airs}, 356]
    dtype: "float32"
    allow_nan: false
    allow_inf: false
  labels:
    required_columns:
      - "planet_id"
      - "split"
      # Either explicit columns target_mu_0..target_mu_282, target_sigma_0.._282
      # or wide columns recognized by loader's range expansion
      - "target_mu[0:283]"
      - "target_sigma[0:283]"
    allow_missing_targets: true         # allow test-only smoke

instrument:
  bin_remap:
    enabled: true
    map_path: "${paths.binmap_356_to_283}"
    fail_if_missing: false              # if absent and input already 283, continue
  time_grid:
    enforce_len: ${dataset.t_airs}
    resample_missing: "linear"
    max_gap_frac: 0.25

# ----------------------------------------------------------------------
# Calibration (lean; OK to toggle steps off for raw-less smoke)
# ----------------------------------------------------------------------
calibration:
  enabled: true
  steps:
    - "adc_correction"
    - "nonlinearity_correction"
    - "dark_subtraction"
    - "flat_fielding"
    - "correlated_double_sampling"
    - "photometric_extraction"
    - "trace_normalization"
    - "phase_alignment"
  save_intermediate: false
  save_preview_png: true
  preview_stride: 32
  output_dir: "${paths.calibrated_dir}"
  method:
    adc_correction:           { enabled: true, bit_depth: 16 }
    nonlinearity_correction:  { enabled: true, saturation_dn: 65535 }
    dark_subtraction:         { enabled: true }
    flat_fielding:            { enabled: true, epsilon: 1.0e-6 }
    correlated_double_sampling:
      enabled: true
      strategy: "nearest"
      noise_threshold_dn: 50
    photometric_extraction:
      enabled: true
      aperture: "circular"
      radius_px: 6
      bkg_annulus_px: [8, 12]
      method: "sum"
    trace_normalization:
      enabled: true
      reference_window: [0.15, 0.35]
      epsilon: 1.0e-6
    phase_alignment:
      enabled: true
      method: "xcorr"
      max_shift: 3

# ----------------------------------------------------------------------
# Preprocessing (minimal & deterministic)
# ----------------------------------------------------------------------
preprocessing:
  normalize_flux: true
  standardize_time: true
  clip_outliers: true
  outlier_sigma: 6.0
  detrend_method: "polynomial"          # "polynomial" | "savgol" | "none"
  savgol:
    window_length: 7
    polyorder: 2
    mode: "interp"
  spectral_smoothing: false
  resample_time:
    enabled: false
    target_len: ${dataset.t_airs}
    method: "linear"

# ----------------------------------------------------------------------
# Symbolic hooks (keep tiny & deterministic)
# ----------------------------------------------------------------------
symbolic:
  enforce_nonnegativity: true
  fft_prior:
    enabled: false
  molecular_masking:
    enabled: true
    molecules: ["H2O", "CO2", "CH4"]
    apply_in_preprocessing: true

# ----------------------------------------------------------------------
# Augmentation (OFF for CI stability)
# ----------------------------------------------------------------------
augmentation:
  jitter_injection: false
  jitter:           { enabled: false }
  noise_injection:  { enabled: false }
  dropout:          { enabled: false }
  mask:
    enabled: false
    scheme: "block"
    fraction: 0.0

# ----------------------------------------------------------------------
# Splits (deterministic small holdout or rely on *_debug.pkl / splits/*.npy)
# ----------------------------------------------------------------------
splits:
  strategy: "planet_holdout"
  fractions: { train: 0.80, val: 0.20, test: 0.00 }
  seed: 42
  # Source split indices (preferred) and export mirror
  source:
    dir: "${paths.split_indices_dir_src}"
    filenames: { train: "train_idx.npy", val: "val_idx.npy", test: "test_idx.npy" }
    required: false
  export:
    enabled: true
    dir: "${paths.split_indices_dir}"
    filenames: { train: "train_idx.npy", val: "val_idx.npy", test: "test_idx.npy" }
  epsilon: 1.0e-6

# ----------------------------------------------------------------------
# Loader (deterministic CI)
# ----------------------------------------------------------------------
loader:
  batch_size: 2
  num_workers: 0                       # no multiprocessing → deterministic
  pin_memory: false
  persistent_workers: false
  prefetch_factor: 2
  drop_last: false
  shuffle_train: false                 # keep steady order for snapshots
  shuffle_val: false
  shuffle_test: false

# Optional “fast-dev” caps if your trainer honors them via interpolation
dev_limits:
  limit_train_batches: 4
  limit_val_batches:   2
  limit_test_batches:  2

# ----------------------------------------------------------------------
# Validation gates (quick but strict)
# ----------------------------------------------------------------------
validate:
  enabled: true
  fail_fast: true
  sample_size_preview: 2
  checks:
    - "paths_exist"
    - "schema_match"
    - "no_nan_inf"
    - "bins_match_interface"
    - "bin_remap_ok"
    - "split_indices_ok"
    - "manifest_consistency"   # verify toy_manifest metrics if present

# ----------------------------------------------------------------------
# Diagnostics (very light)
# ----------------------------------------------------------------------
diagnostics:
  enabled: true
  output_dir: "${paths.artifacts_dir}"
  save_plots: true
  fft_analysis: false
  zscore_analysis: true
  symbolic_overlay: true
  shap_overlay: false
  per_step_previews: true
  artifacts:
    stats_json: "${diagnostics.output_dir}/stats.json"
    preview_png: "${diagnostics.output_dir}/preview.png"
    z_png: "${diagnostics.output_dir}/zscore_preview.png"
    sym_png: "${diagnostics.output_dir}/symbolic_overlay.png"

# ----------------------------------------------------------------------
# Runtime guardrails (debug/CI)
# ----------------------------------------------------------------------
runtime:
  ci_safe: true
  time_budget_sec: 60           # expected walltime budget for smoke
  write_intermediates: false
  reduce_heavy_ops: true

# ----------------------------------------------------------------------
# Consistency & safety
# ----------------------------------------------------------------------
consistency_guard:
  check_paths_writable: true
  check_gpu_memory: false       # CPU-mode friendly for CI
  min_gpu_mem_gb: 0
  warn_only: false

safety:
  max_abs_mu: 10.0
  min_sigma: 1.0e-8
  max_sigma: 1.0
  action_on_violation: "fail"

# ----------------------------------------------------------------------
# Logging / manifests
# ----------------------------------------------------------------------
logging:
  enable: true
  save_dir: "${dataset.output_dir}/logs"
  format: "jsonl"
  include_config_hash: true
  capture_manifest:
    enabled: true
    path: "${paths.toy_manifest_json}"
    fields:
      - "version"
      - "N"
      - "T_fgs1"
      - "T_airs"
      - "bins"
      - "hashes"

# ----------------------------------------------------------------------
# Hydra-managed outputs (isolated runs)
# ----------------------------------------------------------------------
hydra:
  run:
    dir: ${dataset.output_dir}/runs/${now:%Y-%m-%d_%H-%M-%S}
  sweep:
    dir: ${dataset.output_dir}/multirun/${now:%Y-%m-%d_%H-%M-%S}
    subdir: ${hydra.job.num}