# configs/data/nominal.yaml
# ==============================================================================
# 🛰️ SpectraMind V50 — Nominal Data Configuration (ULTRA–UPGRADED)
# NeurIPS 2025 Ariel Data Challenge (ESA Ariel telescope simulation)
#
# Role
# ----
# • Canonical, Hydra-composable DATA config for SpectraMind V50
# • Drives ingestion → calibration kill-chain → preprocessing → splits → loaders
# • Mission-grade: DVC/lakeFS traceability, CI/Kaggle guardrails, sanity gates
#
# Quick usage
# -----------
#   spectramind train data=nominal
#   spectramind train data=nominal preprocessing.detrend.method=savgol
#   spectramind train data=nominal augmentation.jitter.amplitude_ppm=50
#   spectramind calibrate data=nominal calibration.save_intermediate=true
# ==============================================================================

# ----------------------------------------------------------------------
# Hydra defaults
# ----------------------------------------------------------------------
defaults:
  - override hydra/job_logging: default
  - override hydra/hydra_logging: default

# ----------------------------------------------------------------------
# Dataset metadata & provenance (recorded into run manifests)
# ----------------------------------------------------------------------
dataset:
  name: "nominal_v50"
  description: "Default ESA Ariel dataset (FGS1 + AIRS) with full calibration and diagnostics."
  version: "1.4"                       # bump when schema/guards change
  source: "neurips-2025-ariel"         # Kaggle competition slug / internal source
  dvc_stage: "data/nominal"            # DVC pipeline stage name
  dvc_remote: "storage"                # DVC/lakeFS remote name
  cache: "data/cache"                  # local cache for prepared tensors
  # Lightweight lineage token for run-to-run linkage (log-only)
  hash: "${now:%Y%m%d}_${now:%H%M%S}"

  # Loader preference order (faster array paths first)
  file_priority:
    arrays_first: true                 # prefer arrays (.npz > .npy) over PKL bundles

provenance:
  git_commit: null
  dvc_rev: null
  created_at: "${now:%Y-%m-%d %H:%M:%S}"
  kaggle_notebook: false
  kaggle_run_id: null
  tags: ["v50", "nominal", "ariel", "fgs1+airs"]

# ----------------------------------------------------------------------
# Paths (DVC-tracked unless explicitly excluded)
# ----------------------------------------------------------------------
paths:
  raw_dir: "data/raw"
  processed_dir: "data/processed/nominal"
  artifacts_dir: "outputs/diagnostics/data_nominal"

  # ---- PKL bundles (single-file per split) ----
  train_file_pkl: "${paths.raw_dir}/train.pkl"
  val_file_pkl:   "${paths.raw_dir}/val.pkl"
  test_file_pkl:  "${paths.raw_dir}/test.pkl"

  # ---- Array exports (.npz preferred; fall back to .npy) ----
  # Train arrays
  fgs1_train_npz: "${paths.raw_dir}/fgs1_train.npz"   # key 'fgs1' → (N, T_fgs1, 32)
  airs_train_npz: "${paths.raw_dir}/airs_train.npz"   # key 'airs' → (N, T_airs, 283|356)
  fgs1_train_npy: "${paths.raw_dir}/fgs1_train.npy"
  airs_train_npy: "${paths.raw_dir}/airs_train.npy"
  # Val arrays
  fgs1_val_npz: "${paths.raw_dir}/fgs1_val.npz"
  airs_val_npz: "${paths.raw_dir}/airs_val.npz"
  fgs1_val_npy: "${paths.raw_dir}/fgs1_val.npy"
  airs_val_npy: "${paths.raw_dir}/airs_val.npy"
  # Test arrays
  test_fgs1_npz: "${paths.raw_dir}/fgs1_test.npz"
  test_airs_npz: "${paths.raw_dir}/airs_test.npz"
  test_fgs1_npy: "${paths.raw_dir}/fgs1_test.npy"
  test_airs_npy: "${paths.raw_dir}/airs_test.npy"

  # Labels & submission template
  labels_file: "${paths.raw_dir}/labels.csv"          # metadata + targets
  submission_template: "data/sample_submission.csv"

  # Split indices export (and optional source)
  split_indices_dir:      "${paths.processed_dir}/splits"
  split_indices_dir_src:  "${paths.raw_dir}/splits"   # if provided by prep stage

  # Calibrated outputs (intermediates + final)
  calibrated_dir: "${paths.processed_dir}/calibrated"

  # Caches
  tensor_cache_dir: "${dataset.cache}/tensors"
  preview_dir: "${paths.artifacts_dir}/previews"

  # Optional spectral bin map (356→283)
  binmap_356_to_283: "calib_refs/airs/binmap_356_to_283.npy"

io:
  memmap: true                      # try np.memmap for large arrays
  allow_compressed_npy: true        # transparently load .npy/.npz if present
  allow_npz: true
  prefer_npz: true                  # try .npz → .npy → .pkl
  strict_endianness: false          # normalize dtype endianness on load
  fail_on_missing: true             # hard-fail if declared input missing
  internet_access: false            # hermetic offline runs

# ----------------------------------------------------------------------
# Interface contract
# ----------------------------------------------------------------------
interface:
  num_bins: 283                     # AIRS spectral bins used throughout V50
  t_fgs1: 128
  t_airs: 32
  channels_fgs1: 32
  output_dir: "${hydra.run.dir}"

# ----------------------------------------------------------------------
# Schema expectations (fail-fast validation, accept legacy shapes)
# ----------------------------------------------------------------------
schema:
  fgs1:
    expected_rank: 3
    accepted_shapes:
      - [-1, ${interface.t_fgs1}, ${interface.channels_fgs1}]  # preferred
      - [-1, 32, 32]                                           # legacy debug export
    dtype: "float32"
    allow_nan: false
    allow_inf: false
  airs:
    expected_rank: 3
    accepted_shapes:
      - [-1, ${interface.t_airs}, ${interface.num_bins}]       # preferred 283
      - [-1, ${interface.t_airs}, 356]                         # remap to 283 if enabled
    dtype: "float32"
    allow_nan: false
    allow_inf: false
  labels:
    required_columns:
      - "planet_id"
      - "split"
      - "target_mu[0:283]"
      - "target_sigma[0:283]"
    allow_missing_targets: false

instrument:
  # Optional bin map to collapse 356→283 (or any provided spectral mapping)
  bin_remap:
    enabled: true
    map_path: "${paths.binmap_356_to_283}"   # shape (283,) int indices
    fail_if_missing: false
  time_grid:
    enforce_len: ${interface.t_airs}
    resample_missing: "linear"  # linear|nearest|cubic
    max_gap_frac: 0.25          # fail if >25% time steps missing

# ----------------------------------------------------------------------
# Calibration kill chain — order matters
# ----------------------------------------------------------------------
calibration:
  enabled: true
  steps:
    - "adc_correction"
    - "nonlinearity_correction"
    - "dark_subtraction"
    - "flat_fielding"
    - "correlated_double_sampling"
    - "photometric_extraction"
    - "trace_normalization"
    - "phase_alignment"
  save_intermediate: true
  save_preview_png: true
  preview_stride: 5
  output_dir: "${paths.calibrated_dir}"
  method:
    adc_correction:
      enabled: true
      bit_depth: 16
      gain_map: "calib_refs/adc/gain_map.npy"
      offset_map: "calib_refs/adc/offset_map.npy"
    nonlinearity_correction:
      enabled: true
      lut_path: "calib_refs/nlin/lut.npy"
      saturation_dn: 65535
    dark_subtraction:
      enabled: true
      master_dark_path: "calib_refs/dark/master_dark.npy"
      scaling:
        enabled: true
        exposure_time_seconds: 10.0
        temperature_compensation:
          enabled: false
          T0_C: -40.0
          A: 1.0
          B: 0.07
          sensor_temperature_key: "SENSOR_TEMP_C"
    flat_fielding:
      enabled: true
      master_flat_path: "calib_refs/flat/master_flat.npy"
      epsilon: 1.0e-6
    correlated_double_sampling:
      enabled: true
      reset_frame_path: "calib_refs/cds/reset_frame.npy"
      strategy: "nearest"         # nearest|exact|interpolate
      noise_threshold_dn: 50
    photometric_extraction:
      enabled: true
      aperture: "circular"
      radius_px: 6
      bkg_annulus_px: [8, 12]
      method: "sum"               # sum|optimal
    trace_normalization:
      enabled: true
      reference_window: [0.15, 0.35]
      epsilon: 1.0e-6
    phase_alignment:
      enabled: true
      method: "xcorr"             # xcorr|template
      max_shift: 3

# ----------------------------------------------------------------------
# Preprocessing (post-calibration)
# ----------------------------------------------------------------------
preprocessing:
  normalize_flux: true
  standardize_time: true
  clip_outliers: true
  outlier_sigma: 6.0
  detrend:
    enabled: true
    method: "polyfit"             # polyfit|savgol|none
    degree: 2
    savgol:
      window_length: 7
      polyorder: 2
      mode: "interp"
  smoothing:
    enabled: true
    method: "median"              # median|gaussian
    window: 5
    gaussian_sigma: 1.0
  resample_time:
    enabled: false
    target_len: ${interface.t_airs}
    method: "linear"

# ----------------------------------------------------------------------
# Symbolic & physics constraints (data-stage hooks)
# ----------------------------------------------------------------------
symbolic:
  enforce_nonnegativity: true
  smoothness_penalty: 0.01
  asymmetry_penalty: 0.001
  fft_prior:
    enabled: true
    freq_cutoff: 0.25
    weight: 0.002
    log_fft_power: true
  molecular_masking:
    enabled: true
    molecules: ["H2O", "CO2", "CH4"]
    apply_in_preprocessing: true
    windows:
      H2O: [[25, 55], [180, 210]]
      CO2: [[95, 125], [220, 245]]
      CH4: [[135, 165], [250, 275]]
  temporal_dropout:
    enabled: true
    fraction: 0.10
    mode: "random"                # random|block
    block:
      max_len: 4
  region_masking:
    enabled: true
    regions:
      - { name: "AIRS_short_IR", start: 0,   end: 100 }
      - { name: "AIRS_long_IR",  start: 200, end: 356 }
      - { name: "Grav_lens_corr", start: 120, end: 160 }
  corel_uncertainty:
    enabled: true
    weight: 0.005
    preload_sigma_path: null

# ----------------------------------------------------------------------
# Augmentation (on-the-fly)
# ----------------------------------------------------------------------
augmentation:
  jitter:
    enabled: true
    amplitude_ppm: 100
    mode: "sinusoid"              # sinusoid|random|mixed
    max_hz: 0.35
  dropout:
    enabled: true
    prob: 0.01
  mask:
    enabled: true
    scheme: "block"               # block|random
    fraction: 0.15
    block:
      max_bins: 12
      max_steps: 6
  snr_dropout:
    enabled: true
    snr_threshold: 5.0
    invert: false
  temporal_shift:
    enabled: false
    max_phase_offset: 0.02
  noise_injection:
    enabled: true
    sigma_ppm: 15.0
    mode: "gaussian"              # gaussian|laplace

# ----------------------------------------------------------------------
# Splits (deterministic & exportable)
# ----------------------------------------------------------------------
splits:
  strategy: "stratified"          # stratified|random|group
  stratify_on: "planet_id"
  group_on: null
  fractions: { train: 0.8, val: 0.1, test: 0.1 }
  seed: 42
  source:
    dir: "${paths.split_indices_dir_src}"
    filenames: { train: "train_idx.npy", val: "val_idx.npy", test: "test_idx.npy" }
    required: false
  export:
    enabled: true
    dir: "${paths.split_indices_dir}"
    filenames: { train: "train_idx.npy", val: "val_idx.npy", test: "test_idx.npy" }
  epsilon: 1.0e-6

# ----------------------------------------------------------------------
# Loader (PyTorch DataLoader knobs)
# ----------------------------------------------------------------------
loader:
  batch_size: 32
  num_workers: 4
  pin_memory: true
  prefetch_factor: 2
  persistent_workers: true
  drop_last: false
  shuffle_train: true
  shuffle_val: false
  shuffle_test: false

# ----------------------------------------------------------------------
# Validation gates (fail-fast)
# ----------------------------------------------------------------------
validate:
  enabled: true
  fail_fast: true
  sample_size_preview: 8
  checks:
    - "paths_exist"
    - "schema_match"
    - "no_nan_inf"
    - "split_sums_to_one"
    - "stratify_ok"
    - "calibration_order_ok"
    - "bins_match_interface"
    - "nonnegativity_after_preproc"
    - "bin_remap_ok"              # 356→283 remap integrity
    - "split_indices_ok"
    - "manifest_consistency"      # if a prep manifest is available

# ----------------------------------------------------------------------
# Diagnostics (for dashboards & CI artifacts)
# ----------------------------------------------------------------------
diagnostics:
  enabled: true
  output_dir: "${paths.artifacts_dir}"
  save_plots: true
  fft_analysis: true
  zscore_analysis: true
  symbolic_overlay: true
  shap_overlay: true
  per_step_previews: true
  artifacts:
    stats_json: "${diagnostics.output_dir}/stats.json"
    preview_png: "${diagnostics.output_dir}/preview.png"
    fft_png: "${diagnostics.output_dir}/fft_preview.png"
    z_png: "${diagnostics.output_dir}/zscore_preview.png"
    sym_png: "${diagnostics.output_dir}/symbolic_overlay.png"
    shap_png: "${diagnostics.output_dir}/shap_overlay.png"

# ----------------------------------------------------------------------
# Kaggle/runtime guardrails
# ----------------------------------------------------------------------
runtime:
  kaggle_safe: true
  max_hours: 9
  write_intermediates: true
  fast_preview_limit: 64
  reduce_heavy_ops: false

# ----------------------------------------------------------------------
# Consistency guard (preflight)
# ----------------------------------------------------------------------
consistency_guard:
  check_dvc_remote: true
  check_paths_writable: true
  check_gpu_memory: true
  min_gpu_mem_gb: 12
  warn_only: false

# ----------------------------------------------------------------------
# Safety bounds (numeric sanity)
# ----------------------------------------------------------------------
safety:
  max_abs_mu: 10.0
  min_sigma: 1.0e-8
  max_sigma: 1.0
  max_outlier_frac: 0.05      # warn/fail if >5% values clipped as outliers
  action_on_violation: "fail" # "fail" | "warn"

# ----------------------------------------------------------------------
# Notes (logged into manifest & v50_debug_log.md)
# ----------------------------------------------------------------------
notes: |
  • Nominal V50 data config with extended integrity checks and remap support.
  • FFT/molecular windows provide immediate physical sanity checks.
  • Split indices are exported for reproducible apples-to-apples comparisons.
  • Kaggle guardrails bound I/O and previews for ≤9 hr runs.
  • Interoperates with selftest.py, generate_html_report.py, and COREL evaluators.

# ----------------------------------------------------------------------
# Logging / manifests
# ----------------------------------------------------------------------
logging:
  enable: true
  save_dir: "outputs/data_nominal/logs"
  format: "jsonl"
  include_config_hash: true
  capture_manifest:
    enabled: true
    path: "${paths.raw_dir}/toy_manifest.json"     # if a prep step produced it
    fields:
      - "version"
      - "N"
      - "T_fgs1"
      - "T_airs"
      - "bins"
      - "hashes"

# ----------------------------------------------------------------------
# Hydra output dirs
# ----------------------------------------------------------------------
hydra:
  run:
    dir: outputs/data_nominal/${now:%Y%m%d_%H%M%S}
  sweep:
    dir: outputs/data_nominal/multirun
    subdir: ${hydra.job.num}