```yaml
# configs/data/nominal.yaml
# ==============================================================================
# üõ∞Ô∏è SpectraMind V50 ‚Äî Nominal Data Configuration (UPGRADED)
# NeurIPS 2025 Ariel Data Challenge (ESA Ariel telescope simulation)
#
# Role of this file
# -----------------
# ‚Ä¢ Single source of truth for the DATA SIDE of the V50 pipeline
# ‚Ä¢ Drives ingestion, calibration kill-chain, preprocessing, splits, and loaders
# ‚Ä¢ Hydra-composable; zero-code overrides from the CLI
# ‚Ä¢ DVC/lakeFS traceability baked in; Kaggle-safe guardrails included
#
# Usage (examples)
# ----------------
#   spectramind train data=nominal
#   spectramind train data=nominal preprocessing.detrend.method=savgol
#   spectramind train data=nominal augmentation.jitter.amplitude_ppm=50
#   spectramind calibrate data=nominal calibration.save_intermediate=true
#
# Notes
# -----
# ‚Ä¢ Keys and structure align with V50 readers in: ariel_datasets_v50.py,
#   calibration_pipeline.py, and split_util.py
# ‚Ä¢ Every path under `paths.*` is DVC-tracked; `dataset.hash` helps link runs,
#   artifacts, and diagnostics to precise data versions
# ==============================================================================

# ----------------------------------------------------------------------
# Hydra defaults (ensures clean composition, reproducible logs)
# ----------------------------------------------------------------------
defaults:
  - override hydra/job_logging: default
  - override hydra/hydra_logging: default

# Optional namespacing if your repo uses Hydra packages per group
# (Uncomment if your loader expects `data.*` under a package)
# _target_: spectramind.data.builders.make_dataset   # ‚Üê for structured configs
# _convert_: all

# ----------------------------------------------------------------------
# Dataset metadata & provenance (recorded into run manifests)
# ----------------------------------------------------------------------
dataset:
  name: "nominal_v50"
  description: "Default ESA Ariel dataset (FGS1 + AIRS) with full calibration and diagnostics."
  version: "1.2"                       # ‚Üê bumped for upgraded schema/guardrails
  source: "neurips-2025-ariel"         # Kaggle competition slug / internal source ID
  dvc_stage: "data/nominal"            # DVC pipeline stage name
  dvc_remote: "storage"                # DVC/lakeFS remote name
  cache: "data/cache"                  # local cache for prepared tensors
  # A lightweight run hash for data lineage (log-only; not a cryptographic hash)
  hash: "${now:%Y%m%d}_${now:%H%M%S}"

# Populated at runtime by the CLI; recorded in run manifests and HTML dashboards
provenance:
  git_commit: null                     # filled by CLI
  dvc_rev: null                        # filled by CLI
  created_at: "${now:%Y-%m-%d %H:%M:%S}"
  kaggle_notebook: false               # set true when running on Kaggle
  kaggle_run_id: null                  # optional external run identifier

# ----------------------------------------------------------------------
# Paths (all DVC-tracked unless explicitly excluded)
# ----------------------------------------------------------------------
paths:
  raw_dir: "data/raw"
  processed_dir: "data/processed/nominal"
  artifacts_dir: "outputs/diagnostics/data_nominal"
  # Training inputs
  fgs1_file: "${paths.raw_dir}/fgs1_train.npy"     # shape (N_fgs1, 32, 32)
  airs_file: "${paths.raw_dir}/airs_train.npy"     # shape (N_airs, 32, 356)
  labels_file: "${paths.raw_dir}/labels.csv"       # metadata + targets
  # Test inputs
  test_fgs1: "${paths.raw_dir}/fgs1_test.npy"
  test_airs: "${paths.raw_dir}/airs_test.npy"
  submission_template: "data/sample_submission.csv"
  # Split indices export (reproducible)
  split_indices_dir: "${paths.processed_dir}/splits"
  # Calibrated output root (intermediates + final)
  calibrated_dir: "${paths.processed_dir}/calibrated"
  # Caches for speed
  tensor_cache_dir: "${dataset.cache}/tensors"
  preview_dir: "${paths.artifacts_dir}/previews"

# ----------------------------------------------------------------------
# Interface contract (shared across configs/modules)
# ----------------------------------------------------------------------
interface:
  num_bins: 283                        # AIRS spectral bins used throughout V50
  # Default hydra run dir fallback; modules can override their own outputs
  output_dir: "${hydra.run.dir}"

# ----------------------------------------------------------------------
# Instrument & schema expectations (validated before training)
# ----------------------------------------------------------------------
schema:
  fgs1:
    expected_rank: 3
    expected_shape: [-1, 32, 32]      # -1 means any number of frames
    dtype: "float32"
    allow_nan: false
    allow_inf: false
  airs:
    expected_rank: 3
    expected_shape: [-1, 32, 356]
    dtype: "float32"
    allow_nan: false
    allow_inf: false
  labels:
    required_columns:
      - "planet_id"
      - "split"
      # Targets can be column-encoded or wide CSV; the loader supports both modes.
      - "target_mu[0:283]"
      - "target_sigma[0:283]"
    allow_missing_targets: false

# ----------------------------------------------------------------------
# Calibration pipeline (kill chain) ‚Äî order matters
# ----------------------------------------------------------------------
calibration:
  enabled: true
  # The specific method settings live in configs/calib/method/*.yaml too; this
  # block mirrors the composite state so `data=nominal` works standalone.
  steps:
    - "adc_correction"
    - "nonlinearity_correction"
    - "dark_subtraction"
    - "flat_fielding"
    - "correlated_double_sampling"
    - "photometric_extraction"
    - "trace_normalization"
    - "phase_alignment"
  # Save per-step intermediates for diagnostics & HTML dashboard embedding
  save_intermediate: true
  save_preview_png: true
  preview_stride: 5                     # save every k-th preview to limit I/O
  output_dir: "${paths.calibrated_dir}"
  # Per-method knobs (kept concise; detailed knobs live in method YAMLs)
  method:
    adc_correction:
      enabled: true
      bit_depth: 16
      gain_map: "calib_refs/adc/gain_map.npy"
      offset_map: "calib_refs/adc/offset_map.npy"
    nonlinearity_correction:
      enabled: true
      lut_path: "calib_refs/nlin/lut.npy"
      saturation_dn: 65535
    dark_subtraction:
      enabled: true
      master_dark_path: "calib_refs/dark/master_dark.npy"
      scaling:
        enabled: true
        exposure_time_seconds: 10.0
        temperature_compensation:
          enabled: false
          T0_C: -40.0
          A: 1.0
          B: 0.07
          sensor_temperature_key: "SENSOR_TEMP_C"
    flat_fielding:
      enabled: true
      master_flat_path: "calib_refs/flat/master_flat.npy"
      epsilon: 1.0e-6
    correlated_double_sampling:
      enabled: true
      reset_frame_path: "calib_refs/cds/reset_frame.npy"
      strategy: "nearest"               # nearest|exact|interpolate
      noise_threshold_dn: 50
    photometric_extraction:
      enabled: true
      aperture: "circular"
      radius_px: 6
      bkg_annulus_px: [8, 12]
      method: "sum"                     # sum|optimal
    trace_normalization:
      enabled: true
      reference_window: [0.15, 0.35]    # phase window for normalization
      epsilon: 1.0e-6
    phase_alignment:
      enabled: true
      method: "xcorr"                   # xcorr|template
      max_shift: 3

# ----------------------------------------------------------------------
# Preprocessing & normalization (post-calibration)
# ----------------------------------------------------------------------
preprocessing:
  normalize_flux: true
  standardize_time: true
  clip_outliers: true
  outlier_sigma: 6.0
  detrend:
    enabled: true
    method: "polyfit"                  # polyfit|savgol|none
    degree: 2
    savgol:
      window_length: 7
      polyorder: 2
      mode: "interp"
  smoothing:
    enabled: true
    method: "median"                   # median|gaussian
    window: 5
    gaussian_sigma: 1.0
  resample_time:
    enabled: false                     # preserve original cadence by default
    target_len: 32                     # if enabled, resample to this many steps
    method: "linear"                   # linear|cubic

# ----------------------------------------------------------------------
# Symbolic & physics-aware constraints (data-stage hooks)
# ----------------------------------------------------------------------
symbolic:
  enforce_nonnegativity: true          # clamp <0 to 0 after calibration/preproc
  smoothness_penalty: 0.01             # logged for diagnostics; model loss reads its own
  asymmetry_penalty: 0.001             # likewise (asymmetry on Œº profile when available)
  fft_prior:
    enabled: true
    freq_cutoff: 0.25                  # normalized frequency cutoff (0..0.5)
    weight: 0.002
    log_fft_power: true
  molecular_masking:
    enabled: true
    # Names align with molecule windows used elsewhere for overlays
    molecules: ["H2O", "CO2", "CH4"]
    apply_in_preprocessing: true
    # Optional spectral windows for masking/overlay (bin indices)
    windows:
      H2O: [[25, 55], [180, 210]]
      CO2: [[95, 125], [220, 245]]
      CH4: [[135, 165], [250, 275]]
  temporal_dropout:
    enabled: true
    fraction: 0.10
    mode: "random"                     # random|block
    block:
      max_len: 4
  region_masking:
    enabled: true
    regions:
      - { name: "AIRS_short_IR", start: 0,   end: 100 }
      - { name: "AIRS_long_IR",  start: 200, end: 356 }
      - { name: "Grav_lens_corr", start: 120, end: 160 }  # gravitational lensing overlay
  corel_uncertainty:
    enabled: true
    weight: 0.005
    # When available, a calibration map can be injected here for œÉ pre-cal hints
    preload_sigma_path: null

# ----------------------------------------------------------------------
# Augmentation (training robustness; applied on-the-fly by Dataset)
# ----------------------------------------------------------------------
augmentation:
  jitter:
    enabled: true
    amplitude_ppm: 100                 # amplitude of phase/focus-like jitter
    mode: "sinusoid"                   # sinusoid|random|mixed
    max_hz: 0.35
  dropout:
    enabled: true
    prob: 0.01                         # element-wise channel dropout
  mask:
    enabled: true
    scheme: "block"                    # block|random
    fraction: 0.15                     # fraction of bins/time masked
    block:
      max_bins: 12
      max_steps: 6
  snr_dropout:
    enabled: true
    snr_threshold: 5.0
    invert: false                      # drop below threshold when false
  temporal_shift:
    enabled: false
    max_phase_offset: 0.02
  noise_injection:
    enabled: true
    sigma_ppm: 15.0
    mode: "gaussian"                   # gaussian|laplace

# ----------------------------------------------------------------------
# Splits (deterministic & exportable)
# ----------------------------------------------------------------------
splits:
  strategy: "stratified"               # stratified|random|group
  stratify_on: "planet_id"             # column used when stratified
  group_on: null                       # optional grouping column
  fractions:
    train: 0.8
    val:   0.1
    test:  0.1
  seed: 42
  export:
    enabled: true
    dir: "${paths.split_indices_dir}"
    filenames:
      train: "train_idx.npy"
      val:   "val_idx.npy"
      test:  "test_idx.npy"
  # Integrity guard: allow a tiny float error in sums due to rounding
  epsilon: 1.0e-6

# ----------------------------------------------------------------------
# Loader parameters (PyTorch DataLoader knobs)
# ----------------------------------------------------------------------
loader:
  batch_size: 32
  num_workers: 4
  pin_memory: true
  prefetch_factor: 2
  persistent_workers: true
  drop_last: false
  shuffle_train: true
  shuffle_val: false
  shuffle_test: false

# ----------------------------------------------------------------------
# Validation & sanity checks (fail-fast prevents wasted GPU time)
# ----------------------------------------------------------------------
validate:
  enabled: true
  fail_fast: true
  sample_size_preview: 8
  checks:
    - "paths_exist"                    # raw_dir, labels_file present
    - "schema_match"                   # shapes/dtypes align with schema.*
    - "no_nan_inf"                     # numeric hygiene
    - "split_sums_to_one"              # train+val+test ‚âà 1.0 within epsilon
    - "stratify_ok"                    # balanced enough if stratified
    - "calibration_order_ok"           # steps are in allowed order
    - "bins_match_interface"           # num_bins consistent (283)
    - "nonnegativity_after_preproc"    # clamp verified if enabled

# ----------------------------------------------------------------------
# Diagnostics (saved for dashboards & CI artifacts)
# ----------------------------------------------------------------------
diagnostics:
  enabled: true
  output_dir: "${paths.artifacts_dir}"
  save_plots: true
  fft_analysis: true
  zscore_analysis: true
  symbolic_overlay: true
  shap_overlay: true
  per_step_previews: true
  artifacts:
    stats_json: "${diagnostics.output_dir}/stats.json"
    preview_png: "${diagnostics.output_dir}/preview.png"
    fft_png: "${diagnostics.output_dir}/fft_preview.png"
    z_png: "${diagnostics.output_dir}/zscore_preview.png"
    sym_png: "${diagnostics.output_dir}/symbolic_overlay.png"
    shap_png: "${diagnostics.output_dir}/shap_overlay.png"

# ----------------------------------------------------------------------
# Kaggle/runtime guardrails (lightweight I/O, bounded runtime)
# ----------------------------------------------------------------------
runtime:
  kaggle_safe: true                    # keep plots/IO modest, avoid huge caches
  max_hours: 9
  write_intermediates: true
  fast_preview_limit: 64               # cap number of preview tiles/frames saved
  # When true, skips heavy optional diagnostics to save walltime
  reduce_heavy_ops: false

# ----------------------------------------------------------------------
# Internal consistency guard (cheap checks run by the CLI before jobs)
# ----------------------------------------------------------------------
consistency_guard:
  check_dvc_remote: true
  check_paths_writable: true
  check_gpu_memory: true
  min_gpu_mem_gb: 12
  warn_only: false                     # set true to warn instead of fail

# ----------------------------------------------------------------------
# Runtime notes (logged into run manifest + v50_debug_log.md)
# ----------------------------------------------------------------------
notes: |
  ‚Ä¢ Nominal V50 data config with mission-grade calibration and diagnostics.
  ‚Ä¢ Symbolic clips/overlays executed at data stage to catch issues early.
  ‚Ä¢ FFT + molecular windows provide immediate physical sanity checks.
  ‚Ä¢ Split indices are exported and reused to ensure apples-to-apples comparisons.
  ‚Ä¢ Kaggle guardrails keep I/O bounded; previews are strided to reduce cost.
  ‚Ä¢ Designed to interoperate with: selftest.py, generate_html_report.py,
    spectramind diagnose dashboard, and COREL calibration evaluators.
```
