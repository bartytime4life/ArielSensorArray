# ======================================================================
# configs/data/kaggle.yaml — SpectraMind V50 Kaggle Runtime Data Config
# ======================================================================
# Purpose
#   Specialized dataset + runtime configuration for Kaggle submissions.
#   Enforces Kaggle kernel constraints:
#     • ≤ 9 hr total runtime on a single GPU
#     • No internet, no hidden state, deterministic behavior
#     • Local-only layout (no remote pulls), reproducible paths
#
# Usage
#   spectramind train   --config-name train.yaml   data=kaggle
#   spectramind predict --config-name predict.yaml data=kaggle
#
# Notes
#   • Uses Kaggle’s mounted volumes:
#       /kaggle/input     (read-only)
#       /kaggle/working   (read/write)
#       /kaggle/temp      (ephemeral cache)
#   • Keep diagnostics light; prefer HTML/PNG in /kaggle/working only.
#   • Do not attempt network calls or remote DVC pulls in Kaggle runs.
#   • Compatible with both .pkl bundles and .npz/.npy array exports.
# ======================================================================

# ----------------------------------------------------------------------
# Hydra defaults (quiet, CI/Kaggle friendly)
# ----------------------------------------------------------------------
defaults:
  - override hydra/job_logging: default
  - override hydra/hydra_logging: default

# ----------------------------------------------------------------------
# Dataset metadata & Kaggle paths
# ----------------------------------------------------------------------
dataset:
  name: "Ariel_Kaggle_2025"
  provider: "kaggle"
  input_dir: "/kaggle/input/neurips-2025-ariel"    # competition dataset mount
  output_dir: "/kaggle/working"                    # writable workspace
  cache_dir: "/kaggle/temp"                        # ephemeral cache
  version: "1.1-kaggle"
  source: "neurips-2025-ariel"
  # Fixed by competition spec (update if competition differs)
  num_planets: 1100
  num_bins: 283
  t_fgs1: 128            # if only AIRS time exists, loader will standardize
  t_airs: 32
  channels_fgs1: 32

  # Loader preference order (faster array paths first)
  file_priority:
    arrays_first: true   # prefer arrays (.npz > .npy) over PKL bundles

# ----------------------------------------------------------------------
# Structured paths used by loaders/tools (local-only; no remote DVC)
# ----------------------------------------------------------------------
paths:
  # --- PKL bundles (single-file per split) ---
  train_file_pkl: "${dataset.input_dir}/train.pkl"
  val_file_pkl:   "${dataset.input_dir}/val.pkl"
  test_file_pkl:  "${dataset.input_dir}/test.pkl"

  # --- Array exports (.npz preferred; fall back to .npy) ---
  # Train arrays
  fgs1_train_npz: "${dataset.input_dir}/fgs1_train.npz"   # key 'fgs1' → (N, T_fgs1, 32)
  airs_train_npz: "${dataset.input_dir}/airs_train.npz"   # key 'airs' → (N, T_airs, 283|356)
  fgs1_train_npy: "${dataset.input_dir}/fgs1_train.npy"
  airs_train_npy: "${dataset.input_dir}/airs_train.npy"
  # Val arrays
  fgs1_val_npz: "${dataset.input_dir}/fgs1_val.npz"
  airs_val_npz: "${dataset.input_dir}/airs_val.npz"
  fgs1_val_npy: "${dataset.input_dir}/fgs1_val.npy"
  airs_val_npy: "${dataset.input_dir}/airs_val.npy"
  # Test arrays
  fgs1_test_npz: "${dataset.input_dir}/fgs1_test.npz"
  airs_test_npz: "${dataset.input_dir}/airs_test.npz"
  fgs1_test_npy: "${dataset.input_dir}/fgs1_test.npy"
  airs_test_npy: "${dataset.input_dir}/airs_test.npy"

  # Labels & templates (optional; some runs are test-only)
  labels_csv:           "${dataset.input_dir}/labels.csv"
  submission_template:  "${dataset.input_dir}/sample_submission.csv"
  toy_manifest_json:    "${dataset.input_dir}/toy_manifest.json"   # if preparation notebook provided one

  # Outputs & artifacts
  artifacts_dir:     "${dataset.output_dir}/diagnostics"
  split_indices_dir: "${dataset.output_dir}/splits"
  tensor_cache_dir:  "${dataset.cache_dir}/tensors"
  calibrated_dir:    "${dataset.output_dir}/calibrated"

  # Optional spectral bin map (356→283) if competition ships 356-wide AIRS
  binmap_356_to_283: "${dataset.input_dir}/binmap_356_to_283.npy"

io:
  allow_pickle: true               # enable PKL fast paths
  allow_npz: true                  # transparently read .npz if present
  memmap: true                     # np.memmap for large arrays
  strict_endianness: false
  fail_on_missing: true
  prefer_npz: true                 # try .npz → .npy → .pkl
  internet_access: false           # never pull from remote
  external_urls: []

# ----------------------------------------------------------------------
# Interface contract (keep in sync with model/loader)
# ----------------------------------------------------------------------
interface:
  num_bins: ${dataset.num_bins}
  t_fgs1: ${dataset.t_fgs1}
  t_airs: ${dataset.t_airs}
  channels_fgs1: ${dataset.channels_fgs1}
  output_dir: "${hydra.run.dir}"

# ----------------------------------------------------------------------
# Schema (light checks; fail fast, accept legacy shapes)
# ----------------------------------------------------------------------
schema:
  fgs1:
    expected_rank: 3
    accepted_shapes:
      - [-1, ${dataset.t_fgs1}, ${dataset.channels_fgs1}]    # preferred
      - [-1, 32, 32]                                         # historical smoke export
    dtype: "float32"
    allow_nan: false
    allow_inf: false
  airs:
    expected_rank: 3
    accepted_shapes:
      - [-1, ${dataset.t_airs}, ${dataset.num_bins}]         # preferred 283
      - [-1, ${dataset.t_airs}, 356]                         # remap to 283 if bin_remap.enabled
    dtype: "float32"
    allow_nan: false
    allow_inf: false
  labels:
    required_columns:
      - "planet_id"
      - "split"
      - "target_mu[0:283]"
      - "target_sigma[0:283]"
    allow_missing_targets: true     # Kaggle test-only runs are OK

instrument:
  # Optional spectral bin remap 356→283 if competition data requires it
  bin_remap:
    enabled: true
    map_path: "${paths.binmap_356_to_283}"
    fail_if_missing: false
  time_grid:
    enforce_len: ${dataset.t_airs}
    resample_missing: "linear"
    max_gap_frac: 0.25

# ----------------------------------------------------------------------
# Kaggle-calibration (lean; heavy ops disabled)
# ----------------------------------------------------------------------
calibration:
  enabled: true
  steps:
    - "adc_correction"
    - "nonlinearity_correction"
    - "dark_subtraction"
    - "flat_fielding"
    - "correlated_double_sampling"
    - "photometric_extraction"
    - "trace_normalization"
    - "phase_alignment"
  save_intermediate: false          # keep storage usage low
  save_preview_png: true
  preview_stride: 16
  output_dir: "${paths.calibrated_dir}"
  method:
    adc_correction: { enabled: true, bit_depth: 16 }
    nonlinearity_correction: { enabled: true, saturation_dn: 65535 }
    dark_subtraction: { enabled: true }
    flat_fielding: { enabled: true, epsilon: 1.0e-6 }
    correlated_double_sampling:
      enabled: true
      strategy: "nearest"
      noise_threshold_dn: 50
    photometric_extraction:
      enabled: true
      aperture: "circular"
      radius_px: 6
      bkg_annulus_px: [8, 12]
      method: "sum"
    trace_normalization:
      enabled: true
      reference_window: [0.15, 0.35]
      epsilon: 1.0e-6
    phase_alignment:
      enabled: true
      method: "xcorr"
      max_shift: 3

# ----------------------------------------------------------------------
# Preprocessing (lean)
# ----------------------------------------------------------------------
preprocessing:
  normalize_flux: true
  standardize_time: true
  clip_outliers: true
  outlier_sigma: 6.0
  detrend_method: "polynomial"      # "polynomial" | "savgol" | "none"
  savgol:
    window_length: 7
    polyorder: 2
    mode: "interp"
  spectral_smoothing: false         # disable FFT-heavy ops
  resample_time:
    enabled: false
    target_len: ${dataset.t_airs}
    method: "linear"

# ----------------------------------------------------------------------
# Symbolic hooks (minimal; diagnostics on)
# ----------------------------------------------------------------------
symbolic:
  enforce_nonnegativity: true
  fft_prior:
    enabled: false
  molecular_masking:
    enabled: true
    molecules: ["H2O", "CO2", "CH4"]
    apply_in_preprocessing: true

# ----------------------------------------------------------------------
# Augmentation (training only; conservative, turn off for predict-only)
# ----------------------------------------------------------------------
augmentation:
  jitter_injection: true
  jitter:
    enabled: true
    amplitude_ppm: 60
    mode: "sinusoid"
    max_hz: 0.30
  noise_injection:
    enabled: true
    sigma_ppm: 10.0
    mode: "gaussian"
  dropout:
    enabled: true
    prob: 0.005
  mask:
    enabled: true
    scheme: "block"
    fraction: 0.10
    block: { max_bins: 8, max_steps: 4 }

# ----------------------------------------------------------------------
# Splits (deterministic CV inside Kaggle; or rely on provided val.pkl)
# ----------------------------------------------------------------------
splits:
  strategy: "planet_holdout"         # "planet_holdout" | "stratified" | "random"
  fractions: { train: 0.90, val: 0.10, test: 0.00 }
  seed: 42
  # If preparation notebook exported splits to /kaggle/input/..., you may point here.
  source:
    dir: null
    filenames: { train: "train_idx.npy", val: "val_idx.npy", test: "test_idx.npy" }
    required: false
  export:
    enabled: true
    dir: "${paths.split_indices_dir}"
    filenames: { train: "train_idx.npy", val: "val_idx.npy", test: "test_idx.npy" }
  epsilon: 1.0e-6

# ----------------------------------------------------------------------
# Loader (Kaggle-safe)
# ----------------------------------------------------------------------
loader:
  batch_size: 64                     # safe for T4/L4 16 GB with V50
  num_workers: 2                     # conservative to avoid kernel crashes
  pin_memory: true
  persistent_workers: false
  prefetch_factor: 2
  drop_last: false
  shuffle_train: true
  shuffle_val: false
  shuffle_test: false

# ----------------------------------------------------------------------
# Validation gates
# ----------------------------------------------------------------------
validate:
  enabled: true
  fail_fast: true
  sample_size_preview: 4
  checks:
    - "paths_exist"
    - "schema_match"
    - "no_nan_inf"
    - "bins_match_interface"
    - "bin_remap_ok"
    - "split_indices_ok"
    - "manifest_consistency"   # if toy_manifest.json is present

# ----------------------------------------------------------------------
# Diagnostics (lightweight only)
# ----------------------------------------------------------------------
diagnostics:
  enabled: true
  output_dir: "${paths.artifacts_dir}"
  save_plots: true
  fft_analysis: false
  zscore_analysis: true
  symbolic_overlay: true
  shap_overlay: false
  per_step_previews: true
  artifacts:
    stats_json: "${diagnostics.output_dir}/stats.json"
    preview_png: "${diagnostics.output_dir}/preview.png"
    z_png: "${diagnostics.output_dir}/zscore_preview.png"
    sym_png: "${diagnostics.output_dir}/symbolic_overlay.png"

# ----------------------------------------------------------------------
# Runtime guardrails (Kaggle)
# ----------------------------------------------------------------------
runtime:
  kaggle_safe: true
  kaggle_time_limit_hr: 9
  enforce_no_internet: true
  write_intermediates: true        # useful for debugging submission runs
  fast_preview_limit: 64
  reduce_heavy_ops: true

# ----------------------------------------------------------------------
# Consistency & safety
# ----------------------------------------------------------------------
consistency_guard:
  check_paths_writable: true
  check_gpu_memory: true
  min_gpu_mem_gb: 12
  warn_only: false

safety:
  max_abs_mu: 10.0
  min_sigma: 1.0e-8
  max_sigma: 1.0
  action_on_violation: "fail"

# ----------------------------------------------------------------------
# Logging / manifests
# ----------------------------------------------------------------------
logging:
  enable: true
  save_dir: "${dataset.output_dir}/logs"
  format: "jsonl"
  include_config_hash: true
  capture_manifest:
    enabled: true
    path: "${paths.toy_manifest_json}"
    fields:
      - "version"
      - "N"
      - "T_fgs1"
      - "T_airs"
      - "bins"
      - "hashes"

# ----------------------------------------------------------------------
# Hydra output dirs (under /kaggle/working)
# ----------------------------------------------------------------------
hydra:
  run:
    dir: ${dataset.output_dir}/outputs/${now:%Y-%m-%d_%H-%M-%S}
  sweep:
    dir: ${dataset.output_dir}/multirun/${now:%Y-%m-%d_%H-%M-%S}
    subdir: ${hydra.job.num}