# ==============================================================================
# ðŸš€ Precision â€” BF16 Automatic Mixed Precision (AMP)
# ------------------------------------------------------------------------------
# Purpose:
#   â€¢ Preferred on A100/H100-class GPUs (Ampere/Hopper architectures).
#   â€¢ Wider exponent range than FP16 â†’ fewer overflows, more stable training.
#   â€¢ Balances throughput and robustness; ideal for cloud/HPC clusters.
# ==============================================================================

precision:
  amp: "bf16-mixed"      # Lightning/Trainer-friendly string for AMP
  amp_backend: "native"  # use PyTorch native AMP backend
  amp_level: "O1"        # safe mixed precision level (ignored if not applicable)
  grad_clip: 1.0         # gradient clipping (essential in mixed precision)
  loss_scaler: null      # BF16 does not need dynamic scaling (unlike FP16)

deterministic: true       # cudnn_deterministic=True â†’ repeatable runs
cudnn_benchmark: true     # allow autotuned kernels (safe if input shapes fixed)
enable_tf32: false        # disable TF32 to preserve reproducibility

nan_monitor: true         # stop run on NaN/Inf detection in loss/gradients
seed_everything: 42       # enforce seeds across torch/numpy/random for auditability

notes: |
  BF16 AMP:
  â€¢ Best for A100/H100 GPUs (and future Hopper/Grace clusters).
  â€¢ Much wider exponent range vs FP16 â†’ minimizes overflows/underflows.
  â€¢ More stable than FP16 while maintaining mixed-precision throughput.
  â€¢ Retains ~FP32 dynamic range, making it robust for physics/symbolic tasks.
  â€¢ Recommended for cloud/HPC training jobs where determinism + stability matter.
  â€¢ Kaggle GPUs (T4/V100/L4) may not support BF16; fallback to FP16/FP32 if needed.
  â€¢ For numerical audits or symbolic rule validation, use float64 (see precision=float64).
