# ==============================================================================
# ðŸ§® Precision â€” FP32 (Strict, Debug-Safe)
# ------------------------------------------------------------------------------
# Purpose:
#   â€¢ Maximum numerical stability and scientific debuggability.
#   â€¢ Reference mode for CI, CPU-only runs, and symbolic/diagnostic validation.
#   â€¢ Ensures bitwise-deterministic ops where feasible (reproducibility priority).
# ==============================================================================

# Precision / numerical mode
precision: 32          # full single precision (FP32)
deterministic: true    # enforce deterministic kernels (cuDNN, CUDA ops)
cudnn_benchmark: false # disable autotuner to avoid nondeterministic kernels

# ------------------------------------------------------------------------------
# Notes & Usage
# ------------------------------------------------------------------------------
# â€¢ This profile sacrifices throughput for strict stability â€” ideal for:
#     - CI pipelines (CPU/GPU validation, â‰¤ 2h runtime)
#     - Symbolic diagnostics (âˆ‚L/âˆ‚Î¼ tracing, rule influence maps)
#     - Calibration verification & reproducibility audits
#
# â€¢ FP32 prevents NaN/Inf issues seen in mixed precision (AMP) on edge cases.
# â€¢ Expect ~2â€“3Ã— slower runtime vs. 16-mixed precision.
# â€¢ Use 16-mixed (`configs/local/precision/16.yaml`) for leaderboard training.
# â€¢ Keep gradient clipping enabled in trainer configs to avoid exploding grads.
# ==============================================================================

notes: |
  Full FP32 with strict determinism.  
  Best for debugging, scientific validation, or CI smoke runs.  
  Not recommended for leaderboard-scale training due to throughput cost.  
  Prefer AMP (`16-mixed`) unless investigating instability or symbolic drift.
