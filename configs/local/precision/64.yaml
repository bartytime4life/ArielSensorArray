# ==============================================================================
# ðŸ”¬ Precision â€” Float64 (Double)
# ------------------------------------------------------------------------------
# Purpose:
#   â€¢ Full 64-bit floating point precision for high-accuracy scientific diagnostics.
#   â€¢ Useful for debugging numerical stability, symbolic/physics checks, or
#     validating approximations against ground-truth math.
#   â€¢ Not recommended for Kaggle leaderboard runs (2â€“4Ã— slower, >2Ã— memory).
#   â€¢ Use locally or in CI diagnostics jobs when reproducibility > speed.
# ==============================================================================

precision:
  float: 64             # enforce float64 dtype across tensors
  amp_backend: "native" # AMP disabled effectively in double precision
  amp_level: "O0"       # "O0" = full precision, no mixed precision ops
  grad_clip: 1.0        # still clip gradients for stability
  loss_scaler: null     # no dynamic scaling in double precision

deterministic: true      # ensures bitwise-repeatable results
cudnn_benchmark: false   # disable heuristic autotuning (favor exact reproducibility)
enable_tf32: false       # ensure no TF32 kernels are used on Ampere+

nan_monitor: true        # fail fast on NaN/inf in loss or grads
seed_everything: 42      # enforce reproducible seeds (torch/numpy/random)

notes: |
  Float64 (double precision):
  â€¢ Use for numerical experiments, scientific validation, or symbolic debugging.
  â€¢ Guarantees maximum numeric fidelity but increases runtime & memory.
  â€¢ Disable on Kaggle GPUs (runtime >9h); reserve for local/CI audit jobs.
  â€¢ Logs should record dtype, hardware, and CUDA/cuDNN versions for traceability.
  â€¢ If training stability is the goal, first try AMP (16-mixed) + NaN monitor.
