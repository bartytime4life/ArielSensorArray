# ==============================================================================
# ðŸ›°ï¸ SpectraMind V50 â€” DataLoader Persistent Workers (TRAIN)
# ------------------------------------------------------------------------------
# Purpose:
#   â€¢ Configure whether DataLoader worker processes persist across training epochs
#     (workers stay alive instead of being respawned every epoch).
#   â€¢ Reduces per-epoch startup overhead, improving throughput on long jobs.
#
# When to Use
#   â€¢ âœ… Default: true â€” multi-epoch GPU training benefits significantly.
#   â€¢ ðŸš€ Recommended for Kaggle/HPC runs with large datasets and many epochs.
#   â€¢ âš ï¸ Set to false if:
#       â€“ Running CI/smoke tests (short jobs, reproducibility > speed).
#       â€“ Debugging worker code (ensures clean respawn each epoch).
#       â€“ Memory leaks or dangling worker state suspected.
#
# Hydra Usage:
#   spectramind train local/persistent_workers=train
#
# Implementation Note:
#   Persistent workers only take effect if `num_workers > 0`. With num_workers=0,
#   data loading happens in the main process, so this flag is ignored.
# ==============================================================================

persistent_workers: true

notes: |
  Training loaders:
  â€¢ Enabled by default for Kaggle/HPC multi-epoch training â†’ avoids worker churn.
  â€¢ Saves seconds to minutes per epoch on large jobs.
  â€¢ Disable in:
      â€“ Debug/CI mode (safer, reproducible, avoids hidden worker state).
      â€“ CPU-only jobs where DataLoader overhead is minimal anyway.
  â€¢ Pair with:
      â€“ pin_memory: true on GPU for max hostâ†’device transfer speed.
      â€“ num_workers tuned per environment (â‰¥2 on GPU, â‰¤1 in CI).
  â€¢ Environment-aware defaults:
      â€“ Kaggle leaderboard â†’ true
      â€“ HPC long runs â†’ true
      â€“ CI quick checks â†’ false
