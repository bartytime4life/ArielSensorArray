# ==============================================================================
# ðŸ›°ï¸ SpectraMind V50 â€” Gradient Accumulation (TRAINING)
# ------------------------------------------------------------------------------
# Purpose:
#   â€¢ Control how many forward/backward passes to accumulate before optimizer step.
#   â€¢ Allows training with an effectively larger batch size than fits in GPU memory.
#   â€¢ Essential knob for scaling up batch size under Kaggle/HPC GPU memory limits.
# ------------------------------------------------------------------------------
# When to Use
#   â€¢ âœ… Increase (>1) if:
#       â€“ Model or batch does not fit in GPU memory directly.
#       â€“ Want to simulate larger global batch size for stable updates.
#   â€¢ ðŸš« Keep =1 if GPU memory is sufficient for desired batch size.
#   â€¢ âš ï¸ Note: Effective batch size = batch_size Ã— accumulate_grad_batches.
#     Ensure LR scaling (linear rule) if you change this factor.
# ==============================================================================

accumulate_grad_batches: 1

notes: |
  Training loaders:
  â€¢ Default =1 (no accumulation; optimizer step every batch).
  â€¢ Use >1 to simulate large batches on limited-memory GPUs:
      â€“ Example: batch_size=32, accumulate_grad_batches=4 â†’ effective batch=128.
  â€¢ LR scaling rule of thumb: multiply learning rate by accumulate_grad_batches.
  â€¢ Critical for Kaggle environments:
      â€“ T4/A10 GPUs may require accumulation to reach target effective batch size.
      â€“ Balance runtime (â‰¤9h) vs. stability.
  â€¢ Always log chosen accumulation factor in v50_debug_log.md with config hash.
