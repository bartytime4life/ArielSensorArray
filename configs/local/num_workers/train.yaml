# ==============================================================================
# âš™ï¸ DataLoader Workers â€” Training (v1.2)
# ------------------------------------------------------------------------------
# Purpose
#   â€¢ Configure number of worker threads for training dataloaders.
#   â€¢ Balance throughput vs. determinism across Kaggle, HPC, and CI environments.
#   â€¢ Hydra-safe override, isolated from validation/test worker configs.
# ------------------------------------------------------------------------------
# Trade-offs
#   â€¢ âš¡ Throughput: More workers overlap CPU preprocessing with GPU compute.
#   â€¢ âŒ Risk: Too many workers â†’ high memory use, watchdog resets (esp. on Kaggle).
#   â€¢ ğŸ”’ Reproducibility: num_workers=0 ensures bitwise reproducibility in CI/audits.
# ==============================================================================

num_workers: 4   # safe default; tune for Kaggle/HPC runtime

# ------------------------------------------------------------------------------
# Extended Controls (optional, if trainer supports)
# ------------------------------------------------------------------------------
# prefetch_factor: 2        # batches prefetched per worker (default=2)
# persistent_workers: true  # keep workers alive across epochs (improves stability)
# pin_memory: true          # pin host memory for faster GPU transfer

# ------------------------------------------------------------------------------
# ğŸ““ Notes
# ------------------------------------------------------------------------------
notes: |
  Training dataloader workers.

  â€¢ Use 0 â†’ strict reproducibility/debug mode (CI, audits, symbolic tracing).
  â€¢ Use 2â€“4 â†’ Kaggle kernels (safe under 9h runtime, avoids watchdog OOM).
  â€¢ Use 8â€“16 â†’ HPC clusters with abundant CPU cores for max throughput.
  â€¢ Monitor CPU/RAM utilization: excessive workers can degrade stability.
  â€¢ For reproducibility-sensitive experiments, always log worker count in run manifest.
