# configs/train.yaml
# ==============================================================================
# 🛰️ SpectraMind V50 — Training Configuration (Hydra-safe, Kaggle/CI-ready)
# ------------------------------------------------------------------------------
# Purpose
#   Canonical training config for the V50 pipeline. Composes modular groups
#   (data/model/optimizer/trainer/logger/loss) and sets Kaggle/CI-safe defaults
#   with full reproducibility (hashes, logs, artifacts).
#
# Quickstart
#   spectramind train --config-name train
#   spectramind train --config-name train optimizer.lr=3e-4 training.epochs=30
#   spectramind train --config-name train model=v50 data=kaggle loss.smoothness.weight=0.1
#
# Principles
#   • CLI-first + Hydra composition for experiment control
#   • Reproducible logs/artifacts; small deltas via overrides (not code edits)
#   • Kaggle runtime guardrails (≤9h), no internet, deterministic when needed
# ==============================================================================

# ----------------------------------------------------------------------
# Compose defaults from config groups (override any on the CLI)
# ----------------------------------------------------------------------
defaults:
  # Environment/hardware/precision/seed
  - local: default                      # environment (paths, workers, etc.)
  - local/devices: kaggle_t4            # {cpu|single_gpu|multi_gpu|kaggle_t4|kaggle_a10|kaggle_l4|hpc_a100}
  - local/precision: 16-mixed           # {32|16-mixed|bf16-mixed|64}
  - local/seed: default                 # {default|fast|random|kaggle}
  - local/prefetch_factor: default      # {default|fast|debug|heavy}

  # Per-stage DataLoader policies (persistent_workers)
  - local/persistent_workers: train
  - local/persistent_workers@val_loader: val
  - local/persistent_workers@test_loader: test

  # Per-stage gradient accumulation policies
  - local/accumulate_grad_batches: train
  - local/accumulate_grad_batches@val_loader: val         # (no-op; symmetry)
  - local/accumulate_grad_batches@test_loader: test       # (no-op; symmetry)

  # Core pipeline groups
  - data: nominal                     # data/{nominal|kaggle|debug}.yaml
  - model: v50                        # model architecture & heads (FGS1+AIRS fusion)
  - model/decoder: decoder            # unified μ/σ decoder config
  - optimizer: adamw                  # optimizer/{adam|adamw|sgd}.yaml
  - loss: gll                         # loss stack; add smoothness/symbolic below
  - loss: smoothness
  - loss: symbolic
  - trainer: kaggle_safe              # device/loop knobs (Kaggle-safe baseline)

  # Logging
  - logger: tensorboard               # lightweight logging by default
  # - logger: mlflow
  # - logger: wandb

  # Hydra logging
  - override hydra/job_logging: default
  - override hydra/hydra_logging: default
  - _self_

# ----------------------------------------------------------------------
# Experiment metadata
# ----------------------------------------------------------------------
experiment:
  name: "v50_train"
  tag: "baseline"
  description: "V50 baseline: FGS1-Mamba + AIRS-GNN, GLL loss + optional priors"
  seed: ${seed}                        # from local/seed profile
  deterministic: ${deterministic}      # from local/seed/precision profiles
  cudnn_benchmark: ${cudnn_benchmark}  # from local/precision profile

# ----------------------------------------------------------------------
# Hydra run/sweep directories and behavior
# ----------------------------------------------------------------------
hydra:
  run:
    dir: "outputs/${experiment.name}/${now:%Y-%m-%d_%H-%M-%S}"
  sweep:
    dir: "outputs/${experiment.name}/multirun/${now:%Y-%m-%d_%H-%M-%S}"
    subdir: "${hydra.job.num}"
  job:
    chdir: true
  verbose: false
  output_subdir: ".hydra"

# ----------------------------------------------------------------------
# High-level training controls
# (Precision/AMP are controlled via `local/precision=*` defaults above)
# ----------------------------------------------------------------------
training:
  epochs: 20                             # bump for leaderboard training
  resume: false
  resume_ckpt: null                      # path to checkpoint to resume

  # Lightning-style: we surface both for clarity; CLI honors this value.
  gradient_accumulation_steps: ${accumulate_grad_batches}
  grad_clip_norm: 1.0
  grad_checkpointing: false              # model.enable_gradient_checkpointing() if supported

  # fast-dev toggles (for CI/smoke)
  limit_train_batches: null              # e.g., 0.2 for 20% of the data
  limit_val_batches: null
  limit_test_batches: null
  num_sanity_val_steps: 0

  # AMP/autocast guardrails (mirrors local/precision; override only if needed)
  autocast:
    enabled: true
    dtype: "fp16"                        # "fp16"|"bf16" (must match precision profile)
    cache_enabled: true

# ----------------------------------------------------------------------
# Data loader knobs (can be overridden by data/*.yaml)
# ----------------------------------------------------------------------
data:
  batch_size: 64
  eval_batch_size: 64
  num_workers: 4
  pin_memory: true
  prefetch_factor: ${prefetch_factor}            # from local/prefetch_factor group
  persistent_workers: ${persistent_workers}      # from local/persistent_workers=train

# Notes:
# • `persistent_workers` only has effect when `num_workers > 0`.
# • For GPU, pair with `pin_memory: true` for optimal H2D transfer throughput.
# • Effective batch size = batch_size * gradient_accumulation_steps.

# ----------------------------------------------------------------------
# Optimizer/Scheduler quick knobs (base values in optimizer/*.yaml)
# ----------------------------------------------------------------------
optimizer:
  lr: 3.0e-4
  weight_decay: 0.01
  betas: [0.9, 0.999]
  eps: 1.0e-8

scheduler:
  name: "cosine"                         # "none"|"cosine"|"onecycle"
  warmup_steps: 500
  min_lr: 1.0e-6
  # Epoch-aware fallback for short runs (set one of the two)
  warmup_epochs: null

# ----------------------------------------------------------------------
# Loss weights (physics/symbolic regularization)
# ----------------------------------------------------------------------
loss:
  gll:
    weight: 1.0                          # primary task loss (μ,σ via Gaussian LL)
  smoothness:
    weight: 0.0                          # set >0 to encourage spectral smoothness
    l2_lambda: 1.0
  symbolic:
    weight: 0.0                          # set >0 to enforce rule-based priors
    mode: "soft"                         # "soft"|"hard"

# ----------------------------------------------------------------------
# Uncertainty (train-time hooks; post-hoc calibration may run after train)
# ----------------------------------------------------------------------
uncertainty:
  enable_sigma_head: true                # predict per-bin σ (aleatoric)
  epistemic:
    mc_dropout:
      enabled: false
      passes: 8
    ensemble:
      enabled: false
      members: 3
  calibration:
    temperature_scaling:
      enabled: false
      T: 1.0
    corel:
      enabled: false                     # joint, graph-aware calibration post-train
      coverage: 0.9
      weight: 0.005

# ----------------------------------------------------------------------
# Checkpointing, early stopping, and artifacts
# ----------------------------------------------------------------------
checkpoint:
  dir: "artifacts/checkpoints/${experiment.name}"
  filename: "epoch={epoch:03d}-step={step}-val_gll={val/gll:.6f}"
  save_top_k: 2
  monitor: "val/gll"
  mode: "min"
  every_n_epochs: 1
  save_last: true

early_stopping:
  enabled: true
  monitor: "val/gll"
  mode: "min"
  patience: 5
  min_delta: 0.0

# ----------------------------------------------------------------------
# Evaluation & metrics
# ----------------------------------------------------------------------
eval:
  frequency: 1                           # evaluate every N epochs
  metrics:
    - "gll"                              # Gaussian Log-Likelihood
    - "rmse"
    - "mae"
    - "calibration/error_vs_sigma"       # σ-vs-residual diagnostic
  num_workers: ${data.num_workers}
  persistent_workers: ${val_loader.persistent_workers}      # from local/persistent_workers@val_loader
  gradient_accumulation_steps: ${val_loader.accumulate_grad_batches}  # informational (no-op)

# ----------------------------------------------------------------------
# (Optional) test/inference loader policy wiring
# ----------------------------------------------------------------------
test:
  num_workers: ${data.num_workers}
  persistent_workers: ${test_loader.persistent_workers}     # from local/persistent_workers@test_loader
  gradient_accumulation_steps: ${test_loader.accumulate_grad_batches} # informational (no-op)

# ----------------------------------------------------------------------
# Diagnostics hooks (lightweight by default; heavy viz off for Kaggle)
# ----------------------------------------------------------------------
diagnostics:
  output_dir: "outputs/diagnostics/train"
  html_report: false                     # keep false on Kaggle; enable locally
  plots:
    loss_curves: true
    spectra_examples: true
    fft_power: false
    zscore_hist: true
  explainability:
    shap_overlay: false                  # heavy on Kaggle; toggle as needed
    symbolic_overlay: false
    umap: false
    tsne: false
  save_artifacts: true

# ----------------------------------------------------------------------
# Logging backends
# ----------------------------------------------------------------------
logger:
  tensorboard:
    enabled: true
    log_dir: "logs/${experiment.name}"
    flush_secs: 30
  mlflow:
    enabled: false
    tracking_uri: null
    experiment_name: "${experiment.name}"
  csv:
    enabled: true
    dir: "logs/${experiment.name}/csv"

# ----------------------------------------------------------------------
# Runtime guardrails (Kaggle/CI)
# ----------------------------------------------------------------------
runtime:
  kaggle_safe: true                # disables heavy viz and I/O patterns automatically
  max_hours: 9
  device: "auto"                   # "cuda"|"cpu"|"auto"
  find_unused_parameters: false
  cudnn_allow_tf32: true
  torch_compile:
    enabled: false                 # enable if stable with current model/PyTorch version
    mode: "default"
  fail_fast: false                 # set true in CI to abort early on failure
  profiler:
    enabled: false                 # set true locally to profile; auto-disables on Kaggle/CI
    schedule: "simple"             # "simple"|"advanced"
    output_dir: "outputs/profiling/${experiment.name}"

# ----------------------------------------------------------------------
# Inter-config hooks (shared values or convenience)
# ----------------------------------------------------------------------
data_interface:
  num_classes: 283                 # number of spectral bins (AIRS output)
  output_dir: "${hydra.run.dir}"

# ----------------------------------------------------------------------
# Self-test / pre-flight checks
# ----------------------------------------------------------------------
selftest:
  enabled: true
  checks:
    - "paths_exist"                # verify critical paths exist (raw/cals/templates)
    - "dvc_tracked"                # verify DVC pointers for large artifacts
    - "config_hash_log"            # write config hash to v50_debug_log.md
    - "dry_run_ok"                 # pipeline can initialize in dry-run

# ----------------------------------------------------------------------
# CLI-side quality-of-life (read by spectramind CLI)
# ----------------------------------------------------------------------
cli:
  dry_run: false
  confirm: true
  log_to_markdown: "logs/v50_debug_log.md"
  export_manifest: true
  manifest_path: "outputs/${experiment.name}_manifest.json"

# ----------------------------------------------------------------------
# Notes:
#  • Switch groups on the CLI, e.g.:
#      local/devices=cpu | single_gpu | kaggle_t4 | kaggle_a10 | hpc_a100
#      local/precision=32 | 16-mixed | bf16-mixed | 64
#      local/prefetch_factor=default | fast | debug | heavy
#      local/seed=default | fast | random | kaggle
#      local/persistent_workers=train | val | test (per-stage via @val_loader/@test_loader)
#      local/accumulate_grad_batches=train | val | test (per-stage via @val_loader/@test_loader)
#  • Increase epochs + enable uncertainty.calibration.corel for leaderboard pushes.
#  • For smoke/CI runs, override:
#      training.epochs=1 data.num_workers=0 training.limit_train_batches=0.1
#      diagnostics.html_report=false local/accumulate_grad_batches=train@accumulate_grad_batches=1
#  • `persistent_workers` only applies when `num_workers > 0`.
#  • Effective LR tip: effective_lr = optimizer.lr * training.gradient_accumulation_steps.
#  • Filename template in `checkpoint.filename` includes epoch/step/val_gll for quick triage.
# ----------------------------------------------------------------------
