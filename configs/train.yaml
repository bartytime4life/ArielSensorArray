```yaml
# configs/train.yaml
# ==============================================================================
# ðŸ›°ï¸ SpectraMind V50 â€” Training Configuration (Hydra-safe, Kaggle/CI-ready)
# ------------------------------------------------------------------------------
# Purpose
#   Canonical training config for the V50 pipeline. Composes modular groups
#   (data/model/optimizer/trainer/logger/loss) and sets Kaggle/CI-safe defaults
#   with full reproducibility (hashes, logs, artifacts).
#
# Quickstart
#   spectramind train --config-name train
#   spectramind train --config-name train optimizer.lr=3e-4 training.epochs=30
#   spectramind train --config-name train model=v50 data=kaggle loss.smoothness.weight=0.1
#
# Principles
#   â€¢ CLI-first + Hydra composition for experiment control
#   â€¢ Reproducible logs/artifacts; small deltas via overrides (not code edits)
#   â€¢ Kaggle runtime guardrails (â‰¤9h), no internet, deterministic when needed
# ==============================================================================

# ----------------------------------------------------------------------
# Compose defaults from config groups (override any on the CLI)
# ----------------------------------------------------------------------
defaults:
  # Environment/hardware/precision/seed
  - local: default                 # environment (paths, workers, etc.)
  - local/devices: kaggle_t4       # {cpu|single_gpu|multi_gpu|kaggle_t4|kaggle_a10|kaggle_l4|hpc_a100}
  - local/precision: 16-mixed      # {32|16-mixed|bf16-mixed|64}
  - local/seed: default            # {default|fast|random|kaggle}
  - local/prefetch_factor: default # {default|fast|debug|heavy}

  # Per-stage DataLoader policies (persistent_workers)
  - local/persistent_workers: train                 # applies to training loader
  - local/persistent_workers@val_loader: val        # applies to validation loader
  - local/persistent_workers@test_loader: test      # applies to test/inference loader

  # Core pipeline groups
  - data: nominal                  # data/{nominal|kaggle|debug}.yaml
  - model: v50                     # model architecture & heads (FGS1+AIRS fusion)
  - model/decoder: decoder         # unified Î¼/Ïƒ decoder config
  - optimizer: adamw               # optimizer/{adam|adamw|sgd}.yaml
  - loss: gll                      # loss stack; add smoothness/symbolic below
  - loss: smoothness
  - loss: symbolic
  - trainer: kaggle_safe           # device/loop knobs (Kaggle-safe baseline)
  - logger: tensorboard            # lightweight logging by default
  # - logger: mlflow               # enable if desired
  # - logger: wandb                # enable if desired
  - override hydra/job_logging: default
  - override hydra/hydra_logging: default
  - _self_

# ----------------------------------------------------------------------
# Experiment metadata
# ----------------------------------------------------------------------
experiment:
  name: "v50_train"
  tag: "baseline"
  description: "V50 baseline: FGS1-Mamba + AIRS-GNN, GLL loss + optional priors"
  seed: ${seed}                    # reflect selected seed profile
  deterministic: ${deterministic}  # sourced from seed/precision profiles
  cudnn_benchmark: ${cudnn_benchmark}

# ----------------------------------------------------------------------
# Hydra run/sweep directories and behavior
# ----------------------------------------------------------------------
hydra:
  run:
    dir: "outputs/${experiment.name}/${now:%Y-%m-%d_%H-%M-%S}"
  sweep:
    dir: "outputs/${experiment.name}/multirun/${now:%Y-%m-%d_%H-%M-%S}"
    subdir: "${hydra.job.num}"
  job:
    chdir: true
  verbose: false
  output_subdir: ".hydra"

# ----------------------------------------------------------------------
# High-level training controls
# (Precision/AMP are controlled via `local/precision=*` defaults above)
# ----------------------------------------------------------------------
training:
  epochs: 20                         # bump for leaderboard training
  resume: false
  resume_ckpt: null                  # path to checkpoint to resume
  gradient_accumulation_steps: 1
  grad_clip_norm: 1.0

  # fast-dev toggles (for CI/smoke)
  limit_train_batches: null          # e.g., 0.2 for 20% of the data
  limit_val_batches: null
  limit_test_batches: null
  num_sanity_val_steps: 0

# ----------------------------------------------------------------------
# Data loader knobs (can be overridden by data/*.yaml)
# ----------------------------------------------------------------------
data:
  batch_size: 64
  eval_batch_size: 64
  num_workers: 4
  pin_memory: true
  prefetch_factor: ${prefetch_factor}      # wired to local/prefetch_factor group
  persistent_workers: ${persistent_workers}  # from local/persistent_workers=train

# Note:
# â€¢ `persistent_workers` only has effect when `num_workers > 0`.
# â€¢ For GPU, pair with `pin_memory: true` for optimal H2D transfer throughput.

# ----------------------------------------------------------------------
# Optimizer/Scheduler quick knobs (base values in optimizer/*.yaml)
# ----------------------------------------------------------------------
optimizer:
  lr: 3.0e-4
  weight_decay: 0.01
  betas: [0.9, 0.999]
  eps: 1.0e-8

scheduler:
  name: "cosine"                 # "none"|"cosine"|"onecycle"
  warmup_steps: 500
  min_lr: 1.0e-6

# ----------------------------------------------------------------------
# Loss weights (physics/symbolic regularization)
# ----------------------------------------------------------------------
loss:
  gll:
    weight: 1.0                  # primary task loss (Î¼,Ïƒ via Gaussian LL)
  smoothness:
    weight: 0.0                  # set >0 to encourage spectral smoothness
    l2_lambda: 1.0
  symbolic:
    weight: 0.0                  # set >0 to enforce rule-based priors
    mode: "soft"                 # "soft"|"hard"

# ----------------------------------------------------------------------
# Uncertainty (train-time hooks; post-hoc calibration may run after train)
# ----------------------------------------------------------------------
uncertainty:
  enable_sigma_head: true        # predict per-bin Ïƒ (aleatoric)
  epistemic:
    mc_dropout:
      enabled: false
      passes: 8
    ensemble:
      enabled: false
      members: 3
  calibration:
    temperature_scaling:
      enabled: false
      T: 1.0
    corel:
      enabled: false             # joint, graph-aware calibration post-train
      coverage: 0.9
      weight: 0.005

# ----------------------------------------------------------------------
# Checkpointing, early stopping, and artifacts
# ----------------------------------------------------------------------
checkpoint:
  dir: "artifacts/checkpoints/${experiment.name}"
  save_top_k: 2
  monitor: "val/gll"
  mode: "min"
  every_n_epochs: 1
  save_last: true

early_stopping:
  enabled: true
  monitor: "val/gll"
  mode: "min"
  patience: 5
  min_delta: 0.0

# ----------------------------------------------------------------------
# Evaluation & metrics
# ----------------------------------------------------------------------
eval:
  frequency: 1                     # evaluate every N epochs
  metrics:
    - "gll"                        # Gaussian Log-Likelihood
    - "rmse"
    - "mae"
    - "calibration/error_vs_sigma" # Ïƒ-vs-residual diagnostic
  num_workers: ${data.num_workers}
  persistent_workers: ${val_loader.persistent_workers}  # from local/persistent_workers@val_loader

# ----------------------------------------------------------------------
# (Optional) test/inference loader policy wiring
# ----------------------------------------------------------------------
test:
  num_workers: ${data.num_workers}
  persistent_workers: ${test_loader.persistent_workers} # from local/persistent_workers@test_loader

# ----------------------------------------------------------------------
# Diagnostics hooks (lightweight by default; heavy viz off for Kaggle)
# ----------------------------------------------------------------------
diagnostics:
  output_dir: "outputs/diagnostics/train"
  html_report: false               # keep false on Kaggle; enable locally
  plots:
    loss_curves: true
    spectra_examples: true
    fft_power: false
    zscore_hist: true
  explainability:
    shap_overlay: false            # heavy on Kaggle; toggle as needed
    symbolic_overlay: false
    umap: false
    tsne: false
  save_artifacts: true

# ----------------------------------------------------------------------
# Logging backends
# ----------------------------------------------------------------------
logger:
  tensorboard:
    enabled: true
    log_dir: "logs/${experiment.name}"
  mlflow:
    enabled: false
    tracking_uri: null
    experiment_name: "${experiment.name}"
  csv:
    enabled: true
    dir: "logs/${experiment.name}/csv"

# ----------------------------------------------------------------------
# Runtime guardrails (Kaggle/CI)
# ----------------------------------------------------------------------
runtime:
  kaggle_safe: true            # disables heavy viz and I/O patterns automatically
  max_hours: 9
  device: "auto"               # "cuda"|"cpu"|"auto"
  find_unused_parameters: false
  cudnn_allow_tf32: true
  torch_compile:
    enabled: false             # enable if stable with current model/PyTorch version
    mode: "default"
  fail_fast: false             # set true in CI to abort early on failure

# ----------------------------------------------------------------------
# Inter-config hooks (shared values or convenience)
# ----------------------------------------------------------------------
data_interface:
  num_classes: 283             # number of spectral bins (AIRS output)
  output_dir: "${hydra.run.dir}"

# ----------------------------------------------------------------------
# Self-test / pre-flight checks
# ----------------------------------------------------------------------
selftest:
  enabled: true
  checks:
    - "paths_exist"            # verify critical paths exist (raw/cals/templates)
    - "dvc_tracked"            # verify DVC pointers for large artifacts
    - "config_hash_log"        # write config hash to v50_debug_log.md
    - "dry_run_ok"             # pipeline can initialize in dry-run

# ----------------------------------------------------------------------
# CLI-side quality-of-life (read by spectramind CLI)
# ----------------------------------------------------------------------
cli:
  dry_run: false
  confirm: true
  log_to_markdown: "logs/v50_debug_log.md"
  export_manifest: true
  manifest_path: "outputs/${experiment.name}_manifest.json"

# ----------------------------------------------------------------------
# Notes:
#  â€¢ Switch groups on the CLI, e.g.:
#      local/devices=cpu | single_gpu | kaggle_t4 | kaggle_a10 | hpc_a100
#      local/precision=32 | 16-mixed | bf16-mixed | 64
#      local/prefetch_factor=default | fast | debug | heavy
#      local/seed=default | fast | random | kaggle
#      local/persistent_workers=train | val | test (per-stage via @val_loader/@test_loader)
#  â€¢ Increase epochs + enable uncertainty.calibration.corel for leaderboard pushes.
#  â€¢ For smoke/CI runs, override:
#      training.epochs=1 data.num_workers=0 training.limit_train_batches=0.1
#      diagnostics.html_report=false
#  â€¢ `persistent_workers` only applies when `num_workers > 0`.
# ----------------------------------------------------------------------
```
