# configs/train.yaml

# ==============================================================================

# ðŸ›°ï¸ SpectraMind V50 â€” Training Configuration (Hydra-safe, Kaggle/CI-ready)

# ------------------------------------------------------------------------------

# Purpose

# Canonical training config for the V50 pipeline. Composes modular groups

# (data/model/optimizer/trainer/logger/loss) and sets Kaggle/CI-safe defaults

# with full reproducibility (hashes, logs, artifacts).

#

# Quickstart

# spectramind train --config-name train

# spectramind train --config-name train optimizer.lr=3e-4 training.epochs=30

# spectramind train --config-name train model=v50 data=kaggle loss.smoothness.weight=0.1

#

# Principles

# â€¢ CLI-first + Hydra composition for experiment control

# â€¢ Reproducible logs/artifacts; small deltas via overrides (not code edits)

# â€¢ Kaggle runtime guardrails (â‰¤9h), no internet, deterministic when needed

# ==============================================================================

# ----------------------------------------------------------------------

# Compose defaults from config groups (override any on the CLI)

# ----------------------------------------------------------------------

defaults:

# Environment/hardware/precision

* local: default                 # environment (paths, workers, seeds, etc.)
* local/devices: kaggle\_t4       # {cpu|single\_gpu|multi\_gpu|kaggle\_t4|kaggle\_a10|kaggle\_l4|hpc\_a100}
* local/precision: 16-mixed      # {32|16-mixed|bf16-mixed|64}

# Core pipeline groups

* data: nominal                  # data/{nominal|kaggle|debug}.yaml
* model: v50                     # model architecture & heads (FGS1+AIRS fusion)
* model/decoder: decoder         # unified Î¼/Ïƒ decoder config
* optimizer: adamw               # optimizer/{adam|adamw|sgd}.yaml
* loss: gll                      # loss stack; add smoothness/symbolic below
* loss: smoothness
* loss: symbolic
* trainer: kaggle\_safe           # device/loop knobs (Kaggle-safe baseline)
* logger: tensorboard            # lightweight logging by default

# - logger: mlflow               # enable if desired

# - logger: wandb                # enable if desired

* override hydra/job\_logging: default
* override hydra/hydra\_logging: default
* *self*

# ----------------------------------------------------------------------

# Experiment metadata

# ----------------------------------------------------------------------

experiment:
name: "v50\_train"
tag: "baseline"
description: "V50 baseline: FGS1-Mamba + AIRS-GNN, GLL loss + optional priors"
seed: 42
deterministic: false              # set true for CI/self-test reproducibility
cudnn\_benchmark: true             # speed on Kaggle; switch off for strict determinism

# ----------------------------------------------------------------------

# Hydra run/sweep directories and behavior

# ----------------------------------------------------------------------

hydra:
run:
dir: "outputs/\${experiment.name}/\${now:%Y-%m-%d\_%H-%M-%S}"
sweep:
dir: "outputs/\${experiment.name}/multirun/\${now:%Y-%m-%d\_%H-%M-%S}"
subdir: "\${hydra.job.num}"
job:
chdir: true
verbose: false
output\_subdir: ".hydra"

# ----------------------------------------------------------------------

# High-level training controls

# (Precision/AMP are controlled via `local/precision=*` defaults above)

# ----------------------------------------------------------------------

training:
epochs: 20                         # bump for leaderboard training
resume: false
resume\_ckpt: null                  # path to checkpoint to resume
gradient\_accumulation\_steps: 1
grad\_clip\_norm: 1.0

# fast-dev toggles (for CI/smoke)

limit\_train\_batches: null          # e.g., 0.2 for 20% of the data
limit\_val\_batches: null
limit\_test\_batches: null
num\_sanity\_val\_steps: 0

# ----------------------------------------------------------------------

# Data loader knobs (can be overridden by data/\*.yaml)

# ----------------------------------------------------------------------

data:
batch\_size: 64
eval\_batch\_size: 64
num\_workers: 4
pin\_memory: true
prefetch\_factor: 2
persistent\_workers: true

# ----------------------------------------------------------------------

# Optimizer/Scheduler quick knobs (base values in optimizer/\*.yaml)

# ----------------------------------------------------------------------

optimizer:
lr: 3.0e-4
weight\_decay: 0.01
betas: \[0.9, 0.999]
eps: 1.0e-8

scheduler:
name: "cosine"                 # "none"|"cosine"|"onecycle"
warmup\_steps: 500
min\_lr: 1.0e-6

# ----------------------------------------------------------------------

# Loss weights (physics/symbolic regularization)

# ----------------------------------------------------------------------

loss:
gll:
weight: 1.0                  # primary task loss (Î¼,Ïƒ via Gaussian LL)
smoothness:
weight: 0.0                  # set >0 to encourage spectral smoothness
l2\_lambda: 1.0
symbolic:
weight: 0.0                  # set >0 to enforce rule-based priors
mode: "soft"                 # "soft"|"hard"

# ----------------------------------------------------------------------

# Uncertainty (train-time hooks; post-hoc calibration may run after train)

# ----------------------------------------------------------------------

uncertainty:
enable\_sigma\_head: true        # predict per-bin Ïƒ (aleatoric)
epistemic:
mc\_dropout:
enabled: false
passes: 8
ensemble:
enabled: false
members: 3
calibration:
temperature\_scaling:
enabled: false
T: 1.0
corel:
enabled: false             # joint, graph-aware calibration post-train
coverage: 0.9
weight: 0.005

# ----------------------------------------------------------------------

# Checkpointing, early stopping, and artifacts

# ----------------------------------------------------------------------

checkpoint:
dir: "artifacts/checkpoints/\${experiment.name}"
save\_top\_k: 2
monitor: "val/gll"
mode: "min"
every\_n\_epochs: 1
save\_last: true

early\_stopping:
enabled: true
monitor: "val/gll"
mode: "min"
patience: 5
min\_delta: 0.0

# ----------------------------------------------------------------------

# Evaluation & metrics

# ----------------------------------------------------------------------

eval:
frequency: 1                     # evaluate every N epochs
metrics:
\- "gll"                        # Gaussian Log-Likelihood
\- "rmse"
\- "mae"
\- "calibration/error\_vs\_sigma" # Ïƒ-vs-residual diagnostic
num\_workers: \${data.num\_workers}

# ----------------------------------------------------------------------

# Diagnostics hooks (lightweight by default; heavy viz off for Kaggle)

# ----------------------------------------------------------------------

diagnostics:
output\_dir: "outputs/diagnostics/train"
html\_report: false               # keep false on Kaggle; enable locally
plots:
loss\_curves: true
spectra\_examples: true
fft\_power: false
zscore\_hist: true
explainability:
shap\_overlay: false            # heavy on Kaggle; toggle as needed
symbolic\_overlay: false
umap: false
tsne: false
save\_artifacts: true

# ----------------------------------------------------------------------

# Logging backends

# ----------------------------------------------------------------------

logger:
tensorboard:
enabled: true
log\_dir: "logs/\${experiment.name}"
mlflow:
enabled: false
tracking\_uri: null
experiment\_name: "\${experiment.name}"
csv:
enabled: true
dir: "logs/\${experiment.name}/csv"

# ----------------------------------------------------------------------

# Runtime guardrails (Kaggle/CI)

# ----------------------------------------------------------------------

runtime:
kaggle\_safe: true            # disables heavy viz and I/O patterns automatically
max\_hours: 9
device: "auto"               # "cuda"|"cpu"|"auto"
find\_unused\_parameters: false
cudnn\_allow\_tf32: true
torch\_compile:
enabled: false             # enable if stable with current model/PyTorch version
mode: "default"
fail\_fast: false             # set true in CI to abort early on failure

# ----------------------------------------------------------------------

# Inter-config hooks (shared values or convenience)

# ----------------------------------------------------------------------

data\_interface:
num\_classes: 283             # number of spectral bins (AIRS output)
output\_dir: "\${hydra.run.dir}"

# ----------------------------------------------------------------------

# Self-test / pre-flight checks

# ----------------------------------------------------------------------

selftest:
enabled: true
checks:
\- "paths\_exist"            # verify critical paths exist (raw/cals/templates)
\- "dvc\_tracked"            # verify DVC pointers for large artifacts
\- "config\_hash\_log"        # write config hash to v50\_debug\_log.md
\- "dry\_run\_ok"             # pipeline can initialize in dry-run

# ----------------------------------------------------------------------

# CLI-side quality-of-life (read by spectramind CLI)

# ----------------------------------------------------------------------

cli:
dry\_run: false
confirm: true
log\_to\_markdown: "logs/v50\_debug\_log.md"
export\_manifest: true
manifest\_path: "outputs/\${experiment.name}\_manifest.json"

# ----------------------------------------------------------------------

# Notes:

# â€¢ Switch groups on the CLI, e.g.:

# local/devices=cpu | single\_gpu | kaggle\_t4 | kaggle\_a10 | hpc\_a100

# local/precision=32 | 16-mixed | bf16-mixed | 64

# â€¢ Increase epochs + enable uncertainty.calibration.corel for leaderboard pushes.

# â€¢ For smoke/CI runs, override:

# training.epochs=1 data.num\_workers=0 training.limit\_train\_batches=0.1

# diagnostics.html\_report=false

# ----------------------------------------------------------------------
