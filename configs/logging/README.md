# ğŸ“ `/configs/logging` â€” Logging & Experiment Tracking

## 0. Purpose & Scope

The **`/configs/logging`** directory defines how the SpectraMind V50 pipeline records, stores, and visualizes all logs, metrics, and experiment traces.  

Logging is treated as a **first-class scientific artifact**: every run must be auditable, reproducible, and linked back to its exact **config hash**, **Git commit**, and **DVC-tracked data snapshot** [oai_citation:0â€¡SpectraMind V50 Technical Plan for the NeurIPSÂ 2025 Ariel Data Challenge.pdf](file-service://file-6PdU5f5knreHjmSdSauj3w) [oai_citation:1â€¡SpectraMind V50 Project Analysis (NeurIPSÂ 2025 Ariel Data Challenge).pdf](file-service://file-QRDy8Xn69XgxEjZgtZZ8FK).

This directory ensures:
- ğŸ“Š **Metrics & Scalars** â€” Loss curves, calibration scores, violation maps
- ğŸ“‘ **Structured Logs** â€” JSON/CSV + rich console streams
- ğŸ“¦ **Artifacts** â€” Checkpoints, plots, diagnostic reports
- ğŸŒ **Experiment Tracking** â€” MLflow/W&B integration (optional)

---

## 1. Design Philosophy

- **Reproducibility-first** â€” every run logs its full Hydra config, dataset hash, and CLI call [oai_citation:2â€¡SpectraMind V50 Technical Plan for the NeurIPSÂ 2025 Ariel Data Challenge.pdf](file-service://file-6PdU5f5knreHjmSdSauj3w).
- **CLI-driven** â€” logs are written automatically when using `spectramind` subcommands [oai_citation:3â€¡SpectraMind V50 Project Analysis (NeurIPSÂ 2025 Ariel Data Challenge).pdf](file-service://file-QRDy8Xn69XgxEjZgtZZ8FK).
- **UI-light** â€” rich console feedback via [Rich](https://github.com/Textualize/rich), optional HTML dashboards, no heavy GUIs [oai_citation:4â€¡SpectraMind V50 Technical Plan for the NeurIPSÂ 2025 Ariel Data Challenge.pdf](file-service://file-6PdU5f5knreHjmSdSauj3w).
- **Pluggable backends** â€” switch between TensorBoard, MLflow, CSV, or W&B via config overrides [oai_citation:5â€¡Hydra for AI Projects: A Comprehensive Guide.pdf](file-service://file-MpHwv9Z1E3qqzGXaQ3agpL).
- **Separation of concerns** â€” logging configs live here, independent of training code.

---

## 2. Directory Structure

```bash
configs/logging/
â”œâ”€â”€ tensorboard.yaml     # TensorBoard logger
â”œâ”€â”€ mlflow.yaml          # MLflow backend
â”œâ”€â”€ wandb.yaml           # Weights & Biases integration
â”œâ”€â”€ csv.yaml             # Minimal CSV logger
â””â”€â”€ default.yaml         # Safe baseline (CSV + Rich console)


â¸»

3. Usage in Hydra (train.yaml snippet)

defaults:
  - logger: tensorboard     # or mlflow | wandb | csv

Example CLI overrides

Run with MLflow:

spectramind train logger=mlflow logger.mlflow.tracking_uri="http://localhost:5000"

Run with TensorBoard + CSV dual logging:

spectramind train logger=[tensorboard,csv]

Run with W&B:

spectramind train logger=wandb logger.wandb.project="SpectraMindV50"


â¸»

4. Logging Backends

ğŸ–¥ï¸ Console (default, always on)
	â€¢	Rich-enhanced progress bars, live metrics, color-coded diagnostics ï¿¼.
	â€¢	Minimal overhead, Kaggle-safe.

ğŸ“Š TensorBoard (tensorboard.yaml)
	â€¢	Logs scalars, histograms, images.
	â€¢	Compatible with Kaggle & local workflows.

ğŸ§ª MLflow (mlflow.yaml)
	â€¢	Tracks params, metrics, artifacts to MLflow server.
	â€¢	Enables experiment comparisons across runs ï¿¼.

ğŸ“¦ Weights & Biases (wandb.yaml)
	â€¢	Optional cloud experiment tracking.
	â€¢	Supports team collaboration.

ğŸ“‘ CSV (csv.yaml)
	â€¢	Lightweight, filesystem-only logs.
	â€¢	Always safe for Kaggle offline mode.

â¸»

5. Best Practices
	â€¢	Always version configs â€” commit YAMLs to Git, never hard-code logging in code ï¿¼.
	â€¢	Use outputs/ structure â€” Hydra saves each runâ€™s logs + configs in timestamped folders.
	â€¢	Keep Kaggle-safe â€” default to console + CSV; heavy backends (MLflow/W&B) only when external network access is available ï¿¼.
	â€¢	Link logs to physics constraints â€” violations, FFT overlays, symbolic rule scores should also be logged ï¿¼ ï¿¼.
	â€¢	Visualize via dashboards â€” diagnostics HTML reports (generate_html_report.py) consume logs and metrics for interpretability ï¿¼.

â¸»

6. Example Run Lifecycle
	1.	CLI call:

spectramind train --config-name train data=nominal model=v50


	2.	Hydra composes configs (incl. logging/) ï¿¼.
	3.	Logger(s) initialized â†’ logdir: outputs/train/YYYY-MM-DD_HH-MM-SS/.
	4.	Console streams live metrics; scalars saved to TensorBoard/CSV.
	5.	Artifacts (checkpoints, plots) stored and linked in logs.
	6.	Config snapshot + dataset hash written for full reproducibility ï¿¼.

â¸»

7. References
	â€¢	SpectraMind V50 Technical Plan â€” Logging + Hydra integration ï¿¼
	â€¢	SpectraMind V50 Project Analysis â€” Config-driven reproducibility ï¿¼
	â€¢	Hydra for AI Projects: A Comprehensive Guide â€” Config groups + logging ï¿¼
	â€¢	CLI Technical Reference (MCP) â€” Logging standards for reproducibility ï¿¼

â¸»

âœ… With this setup, /configs/logging transforms logs into scientific evidence â€” every result is fully traceable, Kaggle-safe, and NASA-grade reproducible.