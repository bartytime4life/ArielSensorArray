{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c567cf4b",
   "metadata": {},
   "source": [
    "# ðŸ“Š SpectraMind V50 â€” Post-Submission Analysis & Leaderboard Tracking (Notebook 12)\n",
    "\n",
    "**Goal.** Centralize post-submission metadata, track public leaderboard results, and maintain a lightweight, CI-friendly log of submissions and scores.\n",
    "\n",
    "**What this notebook does**\n",
    "1. Pre-flight (detect Kaggle CLI, set paths, read repo/git info)\n",
    "2. Gather **local submission logs** (from prior notebooks) and consolidate into a historical CSV/JSON\n",
    "3. (Optional) **Query Kaggle submissions** via CLI/API â€” DRY-RUN safe\n",
    "4. Merge local logs with Kaggle metadata; compute **deltas** between runs and annotate best scoring submissions\n",
    "5. Emit artifacts: `submissions_history.csv`, `best_submission.json`, and a **Mermaid** trend sketch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf07faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â–‘â–‘ Pre-flight â–‘â–‘\n",
    "import os, sys, json, shutil, subprocess, datetime, pathlib, csv\n",
    "\n",
    "RUN_TS = datetime.datetime.utcnow().strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "RUN_ID = f\"post_submit_{RUN_TS}\"\n",
    "ROOT_OUT = \"/mnt/data/leaderboard_tracking\"\n",
    "ARTIFACTS = os.path.join(ROOT_OUT, RUN_ID)\n",
    "LOGS = os.path.join(ARTIFACTS, \"logs\")\n",
    "os.makedirs(ARTIFACTS, exist_ok=True); os.makedirs(LOGS, exist_ok=True)\n",
    "\n",
    "def which(cmd:str)->bool: return shutil.which(cmd) is not None\n",
    "KAGGLE_PRESENT = which(\"kaggle\")\n",
    "\n",
    "def git_cmd(args):\n",
    "    try:\n",
    "        out = subprocess.check_output([\"git\", *args], stderr=subprocess.STDOUT, timeout=5).decode().strip()\n",
    "        return out\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "env = {\n",
    "    \"python\": sys.version.replace(\"\\n\",\" \"),\n",
    "    \"platform\": sys.platform,\n",
    "    \"kaggle_present\": KAGGLE_PRESENT,\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"paths\": {\"artifacts\": ARTIFACTS, \"logs\": LOGS},\n",
    "    \"git\": {\n",
    "        \"commit\": git_cmd([\"rev-parse\", \"HEAD\"]),\n",
    "        \"branch\": git_cmd([\"rev-parse\", \"--abbrev-ref\", \"HEAD\"]),\n",
    "        \"status\": git_cmd([\"status\", \"--porcelain\"]),\n",
    "    },\n",
    "}\n",
    "with open(os.path.join(ARTIFACTS, \"env.json\"), \"w\") as f:\n",
    "    json.dump(env, f, indent=2)\n",
    "\n",
    "print(\"=== Pre-flight ===\")\n",
    "print(json.dumps(env, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73d5cc1f",
   "metadata": {},
   "source": [
    "## Consolidate local submission logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78a28e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os, json, csv, hashlib\n",
    "\n",
    "# Find prior local logs (from Notebook 11 or pipeline)\n",
    "CANDIDATES = sorted(set(glob.glob(\"/mnt/data/**/submission_log.json\", recursive=True)))\n",
    "print(\"Found local logs:\", len(CANDIDATES))\n",
    "\n",
    "records = []\n",
    "for path in CANDIDATES:\n",
    "    try:\n",
    "        data = json.load(open(path))\n",
    "        # Normalize fields\n",
    "        rec = {\n",
    "            \"source_path\": path,\n",
    "            \"ts_utc\": data.get(\"ts_utc\") or data.get(\"timestamp_utc\"),\n",
    "            \"run_id\": data.get(\"run_id\"),\n",
    "            \"submitted_file\": data.get(\"submitted_file\"),\n",
    "            \"kaggle_present\": data.get(\"kaggle_present\"),\n",
    "            \"submit_rc\": data.get(\"submit_rc\"),\n",
    "        }\n",
    "        # Attach file hash if available\n",
    "        if rec[\"submitted_file\"]:\n",
    "            # try to resolve absolute path\n",
    "            abs_guess = os.path.join(os.path.dirname(path), \"..\", \"package\", rec[\"submitted_file\"])\n",
    "            abs_guess = os.path.abspath(abs_guess)\n",
    "            sha256 = None\n",
    "            if os.path.exists(abs_guess):\n",
    "                h = hashlib.sha256(); \n",
    "                with open(abs_guess, \"rb\") as f:\n",
    "                    h.update(f.read())\n",
    "                sha256 = h.hexdigest()\n",
    "            rec[\"file_sha256\"] = sha256\n",
    "        records.append(rec)\n",
    "    except Exception as e:\n",
    "        print(\"Skip unreadable log:\", path, e)\n",
    "\n",
    "# Write a consolidated CSV snapshot for this run\n",
    "snapshot_csv = os.path.join(ARTIFACTS, \"submissions_snapshot.csv\")\n",
    "with open(snapshot_csv, \"w\", newline=\"\") as f:\n",
    "    w = csv.DictWriter(f, fieldnames=sorted({k for r in records for k in r.keys()}))\n",
    "    w.writeheader()\n",
    "    for r in records: w.writerow(r)\n",
    "\n",
    "print(\"Snapshot rows:\", len(records), \"->\", snapshot_csv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2457ddc",
   "metadata": {},
   "source": [
    "## (Optional) Pull live Kaggle submission metadata â€” DRY-RUN safe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f62c8f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, json, os, shlex\n",
    "\n",
    "COMPETITION = \"ariel-data-challenge-2025\"  # adjust if needed\n",
    "def run_cmd(cmd_list, log_name):\n",
    "    log_path = os.path.join(LOGS, f\"{log_name}.log\")\n",
    "    err_path = os.path.join(LOGS, f\"{log_name}.err\")\n",
    "    if not KAGGLE_PRESENT:\n",
    "        msg = f\"[DRY-RUN] Would execute: {' '.join(shlex.quote(c) for c in cmd_list)}\\n\"\n",
    "        open(log_path, \"w\").write(msg); open(err_path, \"w\").write(\"\")\n",
    "        return 0, msg, \"\"\n",
    "    try:\n",
    "        proc = subprocess.Popen(cmd_list, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        out, err = proc.communicate()\n",
    "        open(log_path, \"wb\").write(out or b\"\"); open(err_path, \"wb\").write(err or b\"\")\n",
    "        return proc.returncode, (out or b\"\").decode(), (err or b\"\").decode()\n",
    "    except Exception as e:\n",
    "        return 99, \"\", str(e)\n",
    "\n",
    "# The Kaggle CLI can list submissions for a competition:\n",
    "# kaggle competitions submissions -c <comp>\n",
    "rc, out, err = run_cmd([\"kaggle\",\"competitions\",\"submissions\",\"-c\", COMPETITION], \"kaggle_submissions_list\")\n",
    "print(\"Submissions rc:\", rc)\n",
    "print(\"stdout (truncated):\", out[:500])\n",
    "print(\"stderr (truncated):\", err[:500])\n",
    "\n",
    "# Save raw listing for audit\n",
    "open(os.path.join(ARTIFACTS, \"kaggle_submissions_raw.txt\"), \"w\").write(out if out else \"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d549d9",
   "metadata": {},
   "source": [
    "## Parse Kaggle CLI table & merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6bdef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, csv, io, json, os\n",
    "\n",
    "kaggle_rows = []\n",
    "table = open(os.path.join(ARTIFACTS, \"kaggle_submissions_raw.txt\")).read() if os.path.exists(os.path.join(ARTIFACTS, \"kaggle_submissions_raw.txt\")) else \"\"\n",
    "\n",
    "# The Kaggle CLI prints a table; extract rows by splitting lines and using simple heuristics.\n",
    "lines = [ln for ln in table.splitlines() if ln.strip()]\n",
    "# Try to detect header separator line (----)\n",
    "sep_idx = None\n",
    "for i, ln in enumerate(lines):\n",
    "    if set(ln.strip()) <= set(\"-|+ \"):\n",
    "        sep_idx = i\n",
    "        break\n",
    "\n",
    "header = []\n",
    "if sep_idx is not None and sep_idx > 0:\n",
    "    header_line = re.sub(r\"\\s+\", \" \", lines[sep_idx-1]).strip()\n",
    "    header = header_line.split(\" \")\n",
    "    data_lines = lines[sep_idx+1:]\n",
    "    for ln in data_lines:\n",
    "        # Normalize whitespace columns (simple heuristic)\n",
    "        parts = re.sub(r\"\\s{2,}\", \" | \", ln).split(\" | \")\n",
    "        if len(parts) >= len(header):\n",
    "            row = dict(zip(header, parts[:len(header)]))\n",
    "            kaggle_rows.append(row)\n",
    "\n",
    "print(\"Parsed Kaggle rows:\", len(kaggle_rows))\n",
    "\n",
    "# Merge strategy: write a history CSV with both local logs and Kaggle columns (where available)\n",
    "history_csv = os.path.join(ARTIFACTS, \"submissions_history.csv\")\n",
    "all_fields = set()\n",
    "for r in kaggle_rows: all_fields.update(r.keys())\n",
    "for rec in records: all_fields.update(rec.keys())\n",
    "all_fields = sorted(all_fields)\n",
    "\n",
    "with open(history_csv, \"w\", newline=\"\") as f:\n",
    "    w = csv.DictWriter(f, fieldnames=all_fields)\n",
    "    w.writeheader()\n",
    "    # Prefer Kaggle rows (live) first, then append local records (with different fields)\n",
    "    for r in kaggle_rows:\n",
    "        w.writerow(r)\n",
    "    for rec in records:\n",
    "        w.writerow(rec)\n",
    "\n",
    "print(\"Wrote consolidated history:\", history_csv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0fa3b7a",
   "metadata": {},
   "source": [
    "## Compute best submission & annotate deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd4114f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, os, json\n",
    "\n",
    "history_csv = os.path.join(ARTIFACTS, \"submissions_history.csv\")\n",
    "best = None\n",
    "rows = []\n",
    "if os.path.exists(history_csv):\n",
    "    with open(history_csv, newline=\"\") as f:\n",
    "        rdr = csv.DictReader(f)\n",
    "        for r in rdr:\n",
    "            rows.append(r)\n",
    "\n",
    "    # Try to identify a PublicScore-like field (Kaggle CLI prints \"PublicScore\" for many comps)\n",
    "    score_field = None\n",
    "    for cand in [\"PublicScore\",\"Score\",\"PublicScore*\",\"Public_Score\",\"publicScore\"]:\n",
    "        if rows and cand in rows[0]:\n",
    "            score_field = cand; break\n",
    "\n",
    "    # Convert scores to float if possible and sort desc\n",
    "    def to_float(x):\n",
    "        try: return float(x)\n",
    "        except: return None\n",
    "\n",
    "    scored = [(r, to_float(r.get(score_field))) for r in rows] if score_field else []\n",
    "    scored = [(r, s) for (r, s) in scored if s is not None]\n",
    "    if scored:\n",
    "        scored.sort(key=lambda t: t[1], reverse=True)\n",
    "        best = {\"score_field\": score_field, \"row\": scored[0][0], \"score\": scored[0][1]}\n",
    "\n",
    "best_path = os.path.join(ARTIFACTS, \"best_submission.json\")\n",
    "json.dump(best or {\"note\":\"no scores found\"}, open(best_path, \"w\"), indent=2)\n",
    "print(\"Best submission summary ->\", best_path)\n",
    "print(json.dumps(best or {}, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d5b0fc",
   "metadata": {},
   "source": [
    "## Trend sketch (Mermaid)\n",
    "\n",
    "> You can paste the following into your README to visualize simple submission flow.\n",
    "\n",
    "```mermaid\n",
    "flowchart TB\n",
    "  A[Local submission logs] --> B[Consolidate snapshot]\n",
    "  B --> C[Pull Kaggle submissions list]\n",
    "  C --> D[Merge to history CSV]\n",
    "  D --> E{Best score?}\n",
    "  E -- yes --> F[Write best_submission.json]\n",
    "  E -- no --> G[No score available]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d6178c",
   "metadata": {},
   "source": [
    "## Browse produced artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5afe3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def tree(path, prefix=\"\"):\n",
    "    items = sorted(os.listdir(path))\n",
    "    lines = []\n",
    "    for i, name in enumerate(items):\n",
    "        full = os.path.join(path, name)\n",
    "        connector = \"â””â”€â”€ \" if i == len(items)-1 else \"â”œâ”€â”€ \"\n",
    "        lines.append(prefix + connector + name)\n",
    "        if os.path.isdir(full):\n",
    "            extension = \"    \" if i == len(items)-1 else \"â”‚   \"\n",
    "            lines.extend(tree(full, prefix + extension))\n",
    "    return lines\n",
    "\n",
    "print(\"ARTIFACTS TREE:\", ARTIFACTS)\n",
    "print(\"\\n\".join(tree(ARTIFACTS)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16aa47b2",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "- Keep this notebook in CI after Notebook 11 to **log submissions automatically**.\n",
    "- If the competition exposes a public submissions API/CSV, swap the CLI table parser for a JSON/CSV endpoint for better reliability.\n",
    "- Extend the merge to include **config hash**, **data version** (from DVC), and Git tag for precise provenance.\n",
    "- Consider a tiny dashboard (static HTML) that renders `submissions_history.csv` and highlights the best score per day.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
