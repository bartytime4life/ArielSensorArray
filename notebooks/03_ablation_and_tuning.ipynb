{
"cells": \[
{
"cell\_type": "markdown",
"metadata": {},
"source": \[
"# ðŸ”§ SpectraMind V50 â€” 03\_ablation\_and\_tuning\n",
"\n",
"A focused notebook to **sweep hyperparameters**, **run ablations**, and **compare results** using the CLI-first pipeline.\n",
"\n",
"**What this does**\n",
"- Tries the dedicated CLI (if available): `spectramind ablate` and `spectramind tune`.\n",
"- If not available, falls back to **Hydra multirun** via `spectramind train -m ...` for quick sweeps.\n",
"- Aggregates run metadata + metrics from artifacts, builds a **leaderboard**, and plots top configs.\n",
"\n",
"> This notebook mirrors the projectâ€™s philosophy: CLI-first orchestration (Typer + Hydra), reproducible outputs, and simple analysis inline."
]
},
{
"cell\_type": "markdown",
"metadata": {},
"source": \[
"## 0) Runtime Helper â€” Resolve `spectramind` & Utilities\n",
"\n",
"Resolves a working launcher (PATH â†’ `poetry run` â†’ `python -m`), and defines helpers:\n",
"- `sm_print(cmd)` â€” pretty-prints the final shell command\n",
"- `sm(cmd, check=False, capture=False)` â€” run command and optionally capture stdout\n",
"- `have_subcommand(name)` â€” detect if a subcommand exists (e.g., `ablate`, `tune`)"
]
},
{
"cell\_type": "code",
"execution\_count": null,
"metadata": {},
"outputs": \[],
"source": \[
"import os, shlex, shutil, subprocess, sys, json, re\n",
"from pathlib import Path\n",
"\n\n",
"def \_resolve\_spectramind\_cmd():\n",
"    if shutil.which("spectramind"):\n",
"        return \["spectramind"]\n",
"    if shutil.which("poetry"):\n",
"        try:\n",
"            out = subprocess.run(\["poetry", "run", "spectramind", "--version"],\n",
"                                 stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
"            if out.returncode == 0:\n",
"                return \["poetry", "run", "spectramind"]\n",
"        except Exception:\n",
"            pass\n",
"    return \[sys.executable, "-m", "spectramind"]\n",
"\n",
"SM = \_resolve\_spectramind\_cmd()\n",
"print("Resolved spectramind launcher:", " ".join(shlex.quote(p) for p in SM))\n",
"\n",
"def sm(cmd: str, check: bool = False, capture: bool = False):\n",
"    args = shlex.split(cmd)\n",
"    return subprocess.run(SM + args,\n",
"                         check=check,\n",
"                         stdout=subprocess.PIPE if capture else None,\n",
"                         stderr=subprocess.STDOUT if capture else None,\n",
"                         text=True)\n",
"\n",
"def sm\_print(cmd: str):\n",
"    print("\$", " ".join(\[*SM, *shlex.split(cmd)]))\n",
"\n",
"def have\_subcommand(name: str) -> bool:\n",
"    """Return True if `spectramind name --help` returns 0."""\n",
"    out = sm(f"{name} --help", capture=True)\n",
"    return out.returncode == 0 if out else False\n",
"\n",
"# Workspace paths\n",
"BASE = Path("outputs")\n",
"ABLATE\_DIR = BASE / "ablate\_quick"\n",
"TUNE\_DIR = BASE / "tune\_quick"\n",
"for d in \[ABLATE\_DIR, TUNE\_DIR]:\n",
"    d.mkdir(parents=True, exist\_ok=True)"
]
},
{
"cell\_type": "markdown",
"metadata": {},
"source": \[
"## 1) Sanity â€” CLI Present & Help Snippets"
]
},
{
"cell\_type": "code",
"execution\_count": null,
"metadata": {},
"outputs": \[],
"source": \[
"sm\_print("--version")\n",
"v = sm("--version", capture=True)\n",
"print((v.stdout or "").strip())\n",
"\n",
"sm\_print("--help")\n",
"h = sm("--help", capture=True)\n",
"print("\n".join((h.stdout or "").splitlines()\[:30]))\n",
"\n",
"HAS\_ABLATE = have\_subcommand("ablate")\n",
"HAS\_TUNE = have\_subcommand("tune")\n",
"print({"HAS\_ABLATE": HAS\_ABLATE, "HAS\_TUNE": HAS\_TUNE})"
]
},
{
"cell\_type": "markdown",
"metadata": {},
"source": \[
"## 2) Quick Ablation Grid\n",
"\n",
"We try **`spectramind ablate`** first. If unavailable, we fall back to a minimal **Hydra multirun** with `spectramind train -m ...`.\n",
"\n",
"The example grid is deliberately tiny (fast):\n",
"- `training.lr âˆˆ {1e-3, 3e-4}`\n",
"- `model.dropout âˆˆ {0.0, 0.1}`\n",
"- `training.epochs=1` (fast dev)\n",
"\n",
"Artifacts are written under `outputs/ablate_quick/`."
]
},
{
"cell\_type": "code",
"execution\_count": null,
"metadata": {},
"outputs": \[],
"source": \[
"if HAS\_ABLATE:\n",
"    # Try a very simple, implementation-agnostic call.\n",
"    # Your projectâ€™s ablation engine likely supports richer flags; this minimal call lets defaults choose the grid.\n",
"    cmd = f"ablate --outdir {ABLATE\_DIR.as\_posix()} --fast"\n",
"    sm\_print(cmd)\n",
"    out = sm(cmd, capture=True)\n",
"    print(out.stdout or "(no output)")\n",
"else:\n",
"    # Hydra multirun fallback (2Ã—2 = 4 runs), with fast dev settings for speed\n",
"    grid = (\n",
"        "-m "\n",
"        "training.lr=0.001,0.0003 "\n",
"        "model.dropout=0.0,0.1 "\n",
"        "training.epochs=1 "\n",
"        f"hydra.run.dir={ABLATE\_DIR.as\_posix()}/multirun"\n",
"    )\n",
"    cmd = f"train {grid}"\n",
"    sm\_print(cmd)\n",
"    out = sm(cmd, capture=True)\n",
"    print(out.stdout or "(no output)")\n",
"\n",
"print("\nArtifacts (first 30 files):")\n",
"for p in list(ABLATE\_DIR.rglob("*"))\[:30]:\n",
"    print("-", p)"
]
},
{
"cell\_type": "markdown",
"metadata": {},
"source": \[
"## 3) Aggregate Results â†’ Leaderboard\n",
"\n",
"We comb outputs for **metrics JSON/CSV** or an **ablation leaderboard**. Common places:\n",
"- `**/metrics.json`, `**/diagnostic_summary.json`, `**/*leaderboard*.csv|json`.\n",
"\n",
"We parse what we find and build a Pandas table with **config + key metrics** (e.g., GLL, RMSE)."
]
},
{
"cell\_type": "code",
"execution\_count": null,
"metadata": {},
"outputs": \[],
"source": \[
"import pandas as pd\n",
"import json\n",
"\n",
"def \_load\_json(path: Path):\n",
"    try:\n",
"        return json.loads(path.read\_text())\n",
"    except Exception:\n",
"        return None\n",
"\n",
"def \_collect\_runs(root: Path):\n",
"    rows = \[]\n",
"    # Look for generic leaderboard CSV/JSON first\n",
"    leader\_csv = list(root.rglob("*leaderboard*.csv"))\n",
"    if leader\_csv:\n",
"        for p in leader\_csv:\n",
"            try:\n",
"                df = pd.read\_csv(p)\n",
"                df\["\_source"] = str(p)\n",
"                rows.append(df)\n",
"            except Exception:\n",
"                pass\n",
"        if rows:\n",
"            return pd.concat(rows, ignore\_index=True)\n",
"    \n",
"    # Otherwise stitch from scattered metrics\n",
"    data = \[]\n",
"    for m in root.rglob("metrics.json"):\n",
"        jd = \_load\_json(m)\n",
"        if not isinstance(jd, dict):\n",
"            continue\n",
"        rec = {"\_metrics\_path": str(m)}\n",
"        # Common fields (if present)\n",
"        for k in \["gll", "rmse", "mae", "score", "val\_gll", "val\_rmse", "val\_mae"]:\n",
"            if k in jd:\n",
"                rec\[k] = jd\[k]\n",
"        # Recover config if nearby\n",
"        cfg = None\n",
"        for cfg\_name in \["config.yaml", ".hydra/config.yaml", "hydra/config.yaml"]:\n",
"            c = m.parent / cfg\_name\n",
"            if c.exists():\n",
"                cfg = c\n",
"                break\n",
"        if cfg and cfg.suffix == ".yaml":\n",
"            try:\n",
"                # Minimal YAML-to-dict (safe), avoid adding full YAML deps\n",
"                import yaml  # If available; otherwise weâ€™ll keep the path only\n",
"                with open(cfg, "r") as fh:\n",
"                    cfgd = yaml.safe\_load(fh)\n",
"                # Flatten a few common fields\n",
"                rec\["training.lr"] = cfgd.get("training", {}).get("lr")\n",
"                rec\["training.epochs"] = cfgd.get("training", {}).get("epochs")\n",
"                rec\["model.dropout"] = cfgd.get("model", {}).get("dropout")\n",
"            except Exception:\n",
"                rec\["\_config\_path"] = str(cfg)\n",
"        data.append(rec)\n",
"    return pd.DataFrame(data) if data else pd.DataFrame()\n",
"\n",
"df\_ablate = \_collect\_runs(ABLATE\_DIR)\n",
"if len(df\_ablate) == 0:\n",
"    print("No metrics/leaderboards found in ablation outputs.")\n",
"else:\n",
"    display(df\_ablate.head(20))\n",
"    out\_csv = ABLATE\_DIR / "leaderboard\_aggregated.csv"\n",
"    df\_ablate.to\_csv(out\_csv, index=False)\n",
"    print("Saved:", out\_csv)"
]
},
{
"cell\_type": "markdown",
"metadata": {},
"source": \[
"## 4) Quick Plots (Top-K)\n",
"\n",
"We visualize the top runs by an available metric. If **`gll`** exists we sort ascending; otherwise try **`rmse`**, then fallback to **`score`** (descending)."
]
},
{
"cell\_type": "code",
"execution\_count": null,
"metadata": {},
"outputs": \[],
"source": \[
"import matplotlib.pyplot as plt\n",
"\n",
"def \_pick\_sort\_key(df: pd.DataFrame):\n",
"    if "gll" in df.columns:\n",
"        return ("gll", True)   # ascending better\n",
"    if "rmse" in df.columns:\n",
"        return ("rmse", True)\n",
"    if "score" in df.columns:\n",
"        return ("score", False) # descending better\n",
"    return (None, True)\n",
"\n",
"if len(df\_ablate) > 0:\n",
"    key, asc = \_pick\_sort\_key(df\_ablate)\n",
"    if key is None:\n",
"        print("No sortable metric found (gll/rmse/score).")\n",
"    else:\n",
"        top = df\_ablate.sort\_values(key, ascending=asc).head(10)\n",
"        plt.figure(figsize=(8, 4))\n",
"        plt.bar(range(len(top)), top\[key].values)\n",
"        plt.xticks(range(len(top)), \[f"run{i}" for i in range(len(top))], rotation=45, ha="right")\n",
"        plt.title(f"Top 10 by {key} ({'asc' if asc else 'desc'})")\n",
"        plt.tight\_layout()\n",
"        plt.show()\n",
"        display(top)"
]
},
{
"cell\_type": "markdown",
"metadata": {},
"source": \[
"## 5) Targeted Tuning (Optional)\n",
"\n",
"If the CLI exposes **`spectramind tune`**, try a tiny run. Otherwise, run a fallback **Hydra multirun** again with a slightly different grid.\n",
"\n",
"- Example tiny grid: `optimizer.name âˆˆ {adam, adamw}`, `training.lr âˆˆ {1e-3, 7e-4}` with `epochs=1`.\n",
"- Artifacts under `outputs/tune_quick/`."
]
},
{
"cell\_type": "code",
"execution\_count": null,
"metadata": {},
"outputs": \[],
"source": \[
"if HAS\_TUNE:\n",
"    cmd = f"tune --outdir {TUNE\_DIR.as\_posix()} --fast"\n",
"    sm\_print(cmd)\n",
"    out = sm(cmd, capture=True)\n",
"    print(out.stdout or "(no output)")\n",
"else:\n",
"    grid = (\n",
"        "-m "\n",
"        "optimizer.name=adam,adamw "\n",
"        "training.lr=0.001,0.0007 "\n",
"        "training.epochs=1 "\n",
"        f"hydra.run.dir={TUNE\_DIR.as\_posix()}/multirun"\n",
"    )\n",
"    cmd = f"train {grid}"\n",
"    sm\_print(cmd)\n",
"    out = sm(cmd, capture=True)\n",
"    print(out.stdout or "(no output)")\n",
"\n",
"print("\nArtifacts (first 30 files):")\n",
"for p in list(TUNE\_DIR.rglob("*"))\[:30]:\n",
"    print("-", p)"
]
},
{
"cell\_type": "markdown",
"metadata": {},
"source": \[
"## 6) Collect Tuning Results & Compare with Ablation\n",
"\n",
"We reuse the aggregator to collect tuning results, then optionally **merge** with ablation and show **overall top configs**."
]
},
{
"cell\_type": "code",
"execution\_count": null,
"metadata": {},
"outputs": \[],
"source": \[
"df\_tune = \_collect\_runs(TUNE\_DIR)\n",
"if len(df\_tune) == 0:\n",
"    print("No metrics/leaderboards found in tuning outputs.")\n",
"else:\n",
"    display(df\_tune.head(20))\n",
"    out\_csv = TUNE\_DIR / "leaderboard\_aggregated.csv"\n",
"    df\_tune.to\_csv(out\_csv, index=False)\n",
"    print("Saved:", out\_csv)\n",
"\n",
"if len(df\_ablate) and len(df\_tune):\n",
"    both = pd.concat(\[\n",
"        df\_ablate.assign(\_group="ablate"),\n",
"        df\_tune.assign(\_group="tune")\n",
"    ], ignore\_index=True)\n",
"    key, asc = \_pick\_sort\_key(both)\n",
"    if key:\n",
"        top\_all = both.sort\_values(key, ascending=asc).head(15)\n",
"        display(top\_all)\n",
"        out\_csv\_all = BASE / "leaderboard\_all\_aggregated.csv"\n",
"        top\_all.to\_csv(out\_csv\_all, index=False)\n",
"        print("Saved:", out\_csv\_all)\n",
"    else:\n",
"        print("No common sortable metric to rank across both groups.")"
]
},
{
"cell\_type": "markdown",
"metadata": {},
"source": \[
"## 7) Next Steps\n",
"\n",
"- Increase grid size and epochs for serious search (respecting runtime).\n",
"- Add domain-aware knobs (e.g., symbolic loss weights, smoothness penalties, uncertainty calibration settings).\n",
"- Persist CSV/JSON leaderboards alongside run hashes and config snapshots for reproducibility.\n",
"- Feed top configs into diagnostics (`spectramind diagnose dashboard`) and compare **explainability** and **symbolic overlays**.\n",
"\n",
"*Tip:* Keep CLI calls deterministic (seeded), log all overrides, and prefer CSV/JSON artifacts for automated comparisons."
]
}
],
"metadata": {
"kernelspec": {
"display\_name": "Python 3",
"language": "python",
"name": "python3"
},
"language\_info": {
"name": "python",
"version": "3.10",
"mimetype": "text/x-python",
"codemirror\_mode": {
"name": "ipython",
"version": 3
},
"pygments\_lexer": "ipython3",
"nbconvert\_exporter": "python",
"file\_extension": ".py"
}
},
"nbformat": 4,
"nbformat\_minor": 5
}
