{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b587d59d",
   "metadata": {},
   "source": [
    "# ðŸ¤ SpectraMind V50 â€” Hugging Face Integration & Transfer Learning (Notebook 07)\n",
    "\n",
    "**Goal.** Integrate **Hugging Face** models into the SpectraMind V50 pipeline and demonstrate **parameterâ€‘efficient fineâ€‘tuning (PEFT/LoRA)** in a CLIâ€‘first, Hydraâ€‘safe flow. This notebook follows the physicsâ€‘informed work in 06 and extends the pipeline with pretrained backbones and transfer learning.\n",
    "\n",
    "**What youâ€™ll do**\n",
    "1. Preâ€‘flight & environment capture (CLI presence, run IDs)\n",
    "2. HF environment check (Transformers / Accelerate / PEFT) and graceful fallbacks\n",
    "3. Compose Hydra overrides to select a **HF model** (e.g., ViT/TimeSformer) and **LoRA** knobs\n",
    "4. Train via `spectramind train` with HF + PEFT overrides\n",
    "5. Diagnose & compare against the custom SSM+GNN baseline\n",
    "6. Artifact tree, Mermaid sketch, and next steps\n",
    "\n",
    "> The notebook **degrades gracefully**: if `spectramind` or HF libs are not installed, it runs **DRYâ€‘RUN** and still produces configs/logs/placeholder artifacts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21ab53f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â–‘â–‘ Pre-flight: env, run IDs, paths, CLI detection â–‘â–‘\n",
    "import os, sys, json, platform, shutil, subprocess, datetime, pathlib\n",
    "\n",
    "RUN_TS = datetime.datetime.utcnow().strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "RUN_ID = f\"huggingface_transfer_{RUN_TS}\"\n",
    "ROOT_OUT = \"/mnt/data/hf_transfer\"\n",
    "ARTIFACTS = os.path.join(ROOT_OUT, RUN_ID)\n",
    "LOGS = os.path.join(ARTIFACTS, \"logs\")\n",
    "CFG_OUT = os.path.join(ARTIFACTS, \"configs\")\n",
    "DIAG_OUT = os.path.join(ARTIFACTS, \"diagnostics\")\n",
    "for p in (ROOT_OUT, ARTIFACTS, LOGS, CFG_OUT, DIAG_OUT):\n",
    "    os.makedirs(p, exist_ok=True)\n",
    "\n",
    "def which(cmd: str) -> bool:\n",
    "    return shutil.which(cmd) is not None\n",
    "\n",
    "CLI_PRESENT = which(\"spectramind\")\n",
    "DRY_RUN = not CLI_PRESENT\n",
    "\n",
    "def git_cmd(args):\n",
    "    try:\n",
    "        out = subprocess.check_output([\"git\", *args], stderr=subprocess.STDOUT, timeout=5).decode().strip()\n",
    "        return out\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "env = {\n",
    "    \"python\": sys.version.replace(\"\\n\", \" \"),\n",
    "    \"platform\": platform.platform(),\n",
    "    \"cli_present\": CLI_PRESENT,\n",
    "    \"dry_run\": DRY_RUN,\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"paths\": {\"artifacts\": ARTIFACTS, \"logs\": LOGS, \"configs\": CFG_OUT, \"diagnostics\": DIAG_OUT},\n",
    "    \"git\": {\n",
    "        \"commit\": git_cmd([\"rev-parse\", \"HEAD\"]),\n",
    "        \"branch\": git_cmd([\"rev-parse\", \"--abbrev-ref\", \"HEAD\"]),\n",
    "        \"status\": git_cmd([\"status\", \"--porcelain\"]),\n",
    "    },\n",
    "}\n",
    "with open(os.path.join(ARTIFACTS, \"env.json\"), \"w\") as f:\n",
    "    json.dump(env, f, indent=2)\n",
    "\n",
    "print(\"=== SpectraMind V50 â€” Notebook 07 ===\")\n",
    "print(json.dumps(env, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b398cf2b",
   "metadata": {},
   "source": [
    "## Hugging Face environment check (Transformers / Accelerate / PEFT)\n",
    "\n",
    "We try importing `transformers`, `accelerate`, and `peft`. If any are missing, we **donâ€™t install** them here (to keep the notebook reproducible/airâ€‘gapped) but proceed in **DRYâ€‘RUN**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdef4e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing = []\n",
    "try:\n",
    "    import transformers  # type: ignore\n",
    "    tr_ok = True\n",
    "except Exception:\n",
    "    tr_ok = False\n",
    "    missing.append(\"transformers\")\n",
    "try:\n",
    "    import accelerate  # type: ignore\n",
    "    acc_ok = True\n",
    "except Exception:\n",
    "    acc_ok = False\n",
    "    missing.append(\"accelerate\")\n",
    "try:\n",
    "    import peft  # type: ignore\n",
    "    peft_ok = True\n",
    "except Exception:\n",
    "    peft_ok = False\n",
    "    missing.append(\"peft\")\n",
    "\n",
    "print(\"HF libs â€” transformers:\", tr_ok, \"| accelerate:\", acc_ok, \"| peft:\", peft_ok)\n",
    "if missing:\n",
    "    print(\"[NOTE] Missing libs:\", \", \".join(missing), \" â€” continuing with DRYâ€‘RUN semantics where required.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509965d3",
   "metadata": {},
   "source": [
    "## Compose Hydra overrides: select HF backbone + LoRA\n",
    "\n",
    "Common toggles:\n",
    "- `model=hf_vit` or `model=hf_timesformer` (assumes your repo has these config groups)\n",
    "- `peft.lora.enable=true`, with rank/alpha/dropout as tunables\n",
    "- Batch/epochs kept conservative by default; use ablations later for sweeps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8f3646",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os\n",
    "\n",
    "# Sensible defaults (adjust to your config structure)\n",
    "overrides = {\n",
    "    # Switch to an HF model group (example names; match to your repoâ€™s configs/)\n",
    "    \"model\": \"hf_vit\",                 # or \"hf_timesformer\"\n",
    "    # Enable PEFT/LoRA adapters\n",
    "    \"peft.lora.enable\": \"true\",\n",
    "    \"peft.lora.r\": \"16\",\n",
    "    \"peft.lora.alpha\": \"32\",\n",
    "    \"peft.lora.dropout\": \"0.05\",\n",
    "    # Data and training\n",
    "    \"data\": \"ariel_nominal\",\n",
    "    \"training.max_epochs\": \"6\",\n",
    "    \"training.batch_size\": \"16\",\n",
    "    \"training.seed\": \"1337\",\n",
    "    # Optional: mixed precision / accelerate flags if your code supports these hydra keys\n",
    "    \"training.mixed_precision\": \"fp16\",\n",
    "}\n",
    "\n",
    "cfg_file = os.path.join(CFG_OUT, \"hf_peft_overrides.json\")\n",
    "with open(cfg_file, \"w\") as f:\n",
    "    json.dump(overrides, f, indent=2)\n",
    "print(\"Saved overrides ->\", cfg_file)\n",
    "print(json.dumps(overrides, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "511ca14d",
   "metadata": {},
   "source": [
    "## Helper: robust CLI runner (DRYâ€‘RUN when CLI not present)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69791af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shlex, time\n",
    "\n",
    "def run_cli(cmd_list, log_name=\"run\"):\n",
    "    log_path = os.path.join(LOGS, f\"{log_name}.log\")\n",
    "    err_path = os.path.join(LOGS, f\"{log_name}.err\")\n",
    "    start = time.time()\n",
    "    result = {\"cmd\": cmd_list, \"dry_run\": DRY_RUN, \"returncode\": 0, \"stdout\": \"\", \"stderr\": \"\"}\n",
    "    if DRY_RUN:\n",
    "        msg = f\"[DRYâ€‘RUN] Would execute: {' '.join(shlex.quote(c) for c in cmd_list)}\\n\"\n",
    "        result[\"stdout\"] = msg\n",
    "        with open(log_path, \"w\") as f: f.write(msg)\n",
    "        with open(err_path, \"w\") as f: f.write(\"\")\n",
    "        place = os.path.join(ARTIFACTS, \"dry_run_placeholder.txt\")\n",
    "        with open(place, \"a\") as f: f.write(msg)\n",
    "        return result\n",
    "\n",
    "    with open(log_path, \"wb\") as out, open(err_path, \"wb\") as err:\n",
    "        try:\n",
    "            proc = subprocess.Popen(cmd_list, stdout=out, stderr=err, env=os.environ.copy())\n",
    "            proc.wait()\n",
    "            result[\"returncode\"] = proc.returncode\n",
    "        except Exception as e:\n",
    "            result[\"returncode\"] = 99\n",
    "            with open(err_path, \"ab\") as errf:\n",
    "                errf.write(str(e).encode())\n",
    "\n",
    "    try:\n",
    "        result[\"stdout\"] = open(log_path, \"r\").read()\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        result[\"stderr\"] = open(err_path, \"r\").read()\n",
    "    except Exception:\n",
    "        pass\n",
    "    result[\"elapsed_sec\"] = round(time.time() - start, 3)\n",
    "    print(f\"[rc={result['returncode']}] logs: {log_path}\")\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3060d9f3",
   "metadata": {},
   "source": [
    "## Train with Hugging Face + PEFT/LoRA (CLIâ€‘first)\n",
    "\n",
    "We pass the overrides to `spectramind train` and let Hydra compose the full config. This aligns with the CLIâ€‘first/hydra architecture and reproducibility practices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97f7164",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = [\n",
    "    \"spectramind\", \"train\",\n",
    "    \"--config-name\", \"config_v50.yaml\",\n",
    "    \"+outputs.root_dir=\" + ARTIFACTS,\n",
    "]\n",
    "for k, v in overrides.items():\n",
    "    cmd.append(f\"+{k}={v}\")\n",
    "# Optional fast mode, if supported by your code:\n",
    "cmd += [\"+training.fast_mode=true\"]\n",
    "\n",
    "res_train = run_cli(cmd, log_name=\"01_train_hf_peft\")\n",
    "print(res_train[\"stdout\"][:500])\n",
    "if res_train[\"returncode\"] not in (0, None):\n",
    "    print(\"Training non-zero return code:\", res_train[\"returncode\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74976b58",
   "metadata": {},
   "source": [
    "## Diagnostics & comparison vs custom SSM+GNN\n",
    "\n",
    "We run diagnostics and (optionally) a comparison routine (e.g., validation metrics table or overlay plots) to quantify transfer benefits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708b6c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Symbolic / general diagnostics (adjust to your CLI):\n",
    "cmd_diag = [\"spectramind\", \"diagnose\", \"dashboard\",\n",
    "            \"--out\", os.path.join(DIAG_OUT, \"diagnostic_report_hf_v1.html\")]\n",
    "res_diag = run_cli(cmd_diag, log_name=\"02_diagnose_dashboard\")\n",
    "print(res_diag[\"stdout\"][:500])\n",
    "\n",
    "# (Optional) If you have a comparison subcommand:\n",
    "cmd_cmp = [\"spectramind\", \"diagnose\", \"compare\",\n",
    "           \"--models\", \"baseline_v50,hf_vit\",\n",
    "           \"--out\", os.path.join(DIAG_OUT, \"compare_baseline_vs_hf.json\")]\n",
    "res_cmp = run_cli(cmd_cmp, log_name=\"03_diagnose_compare\")\n",
    "print(res_cmp[\"stdout\"][:500])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a54ad7",
   "metadata": {},
   "source": [
    "## Browse produced artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdef7f17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def tree(path, prefix=\"\"):\n",
    "    items = sorted(os.listdir(path))\n",
    "    lines = []\n",
    "    for i, name in enumerate(items):\n",
    "        full = os.path.join(path, name)\n",
    "        connector = \"â””â”€â”€ \" if i == len(items)-1 else \"â”œâ”€â”€ \"\n",
    "        lines.append(prefix + connector + name)\n",
    "        if os.path.isdir(full):\n",
    "            extension = \"    \" if i == len(items)-1 else \"â”‚   \"\n",
    "            lines.extend(tree(full, prefix + extension))\n",
    "    return lines\n",
    "\n",
    "print(\"ARTIFACTS TREE:\", ARTIFACTS)\n",
    "print(\"\\n\".join(tree(ARTIFACTS)))\n",
    "dash_path = os.path.join(DIAG_OUT, \"diagnostic_report_hf_v1.html\")\n",
    "print(\"\\nDashboard:\", dash_path if os.path.exists(dash_path) else \"(not found)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d980b35",
   "metadata": {},
   "source": [
    "## Pipeline sketch (Mermaid)\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "  A[Pretrained HF backbone] --> B[PEFT/LoRA fineâ€‘tune (Hydra)]\n",
    "  B --> C[Predict Î¼, Ïƒ]\n",
    "  C --> D[Diagnostics & overlays]\n",
    "  D --> E[Compare vs SSM+GNN baseline]\n",
    "  E --> F[Report / CI]\n",
    "```\n",
    "\n",
    "## Next steps\n",
    "- Try **TimeSformer** or additional HF backbones; tune LoRA rank/alpha/dropout via `spectramind ablate`.\n",
    "- Enable **Accelerate** multiâ€‘GPU or mixed precision for faster runs if supported by your config.\n",
    "- Publish the fineâ€‘tuned model to a private registry (or Hugging Face Hub) for controlled reuse.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
