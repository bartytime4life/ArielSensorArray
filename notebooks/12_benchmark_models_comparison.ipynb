{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": { "display_name": "Python 3", "language": "python", "name": "python3" },
    "language_info": { "name": "python", "version": "3.10" },
    "title": "12_benchmark_models_comparison.ipynb",
    "authors": ["SpectraMind V50 Team"],
    "spectramind": {
      "role": "benchmark/comparison",
      "cli_first": true,
      "outputs_dir": "outputs/notebooks/12_benchmark_models_comparison",
      "reproducibility": { "hydra": true, "dvc": true, "logs": true }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 12 · Benchmark Models Comparison (SpectraMind V50)\n",
        "\n",
        "Mission‑grade notebook to **compare SpectraMind V50** against baseline/benchmark models (e.g., Kaggle baselines) using **CLI/Hydra artifacts only** (no ad‑hoc training code).\n",
        "\n",
        "### What this does\n",
        "1) Auto‑discovers V50 run folders under `outputs/` and optionally loads **baseline predictions** (paths provided below).\n",
        "2) Harvests **metrics** (Val GLL/Loss/Coverage) from each run.\n",
        "3) Optionally computes **evaluation GLL** from predictions and labels (if available).\n",
        "4) Produces a **leaderboard** (CSV/MD/HTML) and comparison plots.\n",
        "5) Generates quick **per‑planet overlays** (predictions vs labels) for a small sample.\n",
        "\n",
        "**Contract**: Thin orchestration. We only read outputs written by the pipeline/CLI; any regeneration is done by calling the CLI (optional cells). All artifacts are written to `outputs/notebooks/12_benchmark_models_comparison/`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": { "tags": ["init"] },
      "execution_count": null,
      "outputs": [],
      "source": [
        "import os, sys, json, shutil, subprocess, platform, zipfile, textwrap\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from typing import Optional, List, Dict\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set_context('notebook'); sns.set_style('whitegrid')\n",
        "\n",
        "ROOT = Path.cwd().resolve()\n",
        "NB_OUT = ROOT / 'outputs' / 'notebooks' / '12_benchmark_models_comparison'\n",
        "NB_OUT.mkdir(parents=True, exist_ok=True)\n",
        "OUT_ROOT = ROOT / 'outputs'\n",
        "\n",
        "ENV = {\n",
        "    'python': platform.python_version(),\n",
        "    'platform': platform.platform()\n",
        "}\n",
        "(NB_OUT/'env_snapshot.json').write_text(json.dumps(ENV, indent=2))\n",
        "\n",
        "print('ROOT   :', ROOT)\n",
        "print('OUT    :', OUT_ROOT)\n",
        "print('NB_OUT :', NB_OUT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parameters\n",
        "Configure comparison inputs.\n",
        "\n",
        "- **V50_RUN_TAGS**: optional substrings to filter SpectraMind V50 runs under `outputs/`.\n",
        "- **BASELINES**: mapping of friendly names to prediction file(s). Provide long‑format (`planet_id,wavelength_index,mu`) or wide (`mu_000..`) CSV paths.\n",
        "- **LABELS_HINT**: path to labels CSV if you want to compute evaluation GLL (long format: `planet_id,wavelength_index,y`)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": { "tags": ["params"] },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Filter V50 runs by tags (set to [] to include all)\n",
        "V50_RUN_TAGS: List[str] = []  # e.g., ['v50', 'mamba', 'gat']\n",
        "\n",
        "# Baseline predictions mapping (edit paths to your artifacts)\n",
        "BASELINES: Dict[str, Path] = {\n",
        "    # 'kaggle_baseline_1': ROOT/'benchmarks'/'kaggle'/'baseline_0_329'/'predictions.csv',\n",
        "    # 'kaggle_model_X'  : ROOT/'benchmarks'/'kaggle'/'model_X'/'predictions.csv',\n",
        "}\n",
        "\n",
        "# Labels (optional, used to compute evaluation GLL)\n",
        "LABELS_HINT: Optional[Path] = None  # e.g., ROOT/'outputs'/'val_labels.csv' or ROOT/'data'/'labels'/'val_labels.csv'\n",
        "\n",
        "print('V50_RUN_TAGS:', V50_RUN_TAGS)\n",
        "print('BASELINES   :', {k:str(v) for k,v in BASELINES.items()})\n",
        "print('LABELS_HINT :', LABELS_HINT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Discovery & utilities\n",
        "We collect V50 runs and read each run's metrics/config if available; for baselines we load predictions only."
      ]
    },
    {
      "cell_type": "code",
      "metadata": { "tags": ["utils"] },
      "execution_count": null,
      "outputs": [],
      "source": [
        "def list_run_dirs(outputs_root: Path) -> List[Path]:\n",
        "    \"\"\"List run directories under outputs/, supporting date and runs/ layouts.\"\"\"\n",
        "    runs = []\n",
        "    # Date layout\n",
        "    for d in outputs_root.glob('20*'):\n",
        "        if d.is_dir():\n",
        "            runs.extend([p for p in d.iterdir() if p.is_dir()])\n",
        "    # runs/ layout\n",
        "    runs_root = outputs_root/'runs'\n",
        "    if runs_root.exists():\n",
        "        runs.extend([p for p in runs_root.iterdir() if p.is_dir()])\n",
        "    # unique by path\n",
        "    uniq, seen = [], set()\n",
        "    for p in runs:\n",
        "        if p not in seen:\n",
        "            uniq.append(p); seen.add(p)\n",
        "    return uniq\n",
        "\n",
        "def match_tags(run_dir: Path, tags: List[str]) -> bool:\n",
        "    if not tags: return True\n",
        "    name = run_dir.name.lower()\n",
        "    return any(t.lower() in name for t in tags)\n",
        "\n",
        "def find_artifacts(run_dir: Path) -> dict:\n",
        "    m = {\n",
        "        'config': None,\n",
        "        'metrics_csv': None,\n",
        "        'predictions_csv': None\n",
        "    }\n",
        "    for p in [run_dir/'config.yaml', run_dir/'.hydra'/'config.yaml']:\n",
        "        if p.exists(): m['config'] = p\n",
        "    for p in [run_dir/'metrics.csv', run_dir/'training_metrics.csv']:\n",
        "        if p.exists(): m['metrics_csv'] = p\n",
        "    # Find a predictions.csv in run directory or below\n",
        "    preds = sorted(run_dir.rglob('predictions.csv'))\n",
        "    if preds: m['predictions_csv'] = preds[-1]\n",
        "    return m\n",
        "\n",
        "def load_metrics(path: Optional[Path]) -> Optional[pd.DataFrame]:\n",
        "    if path and path.exists():\n",
        "        try: return pd.read_csv(path)\n",
        "        except Exception: return None\n",
        "    return None\n",
        "\n",
        "def newest_csv(paths: List[Path]) -> Optional[Path]:\n",
        "    cands = []\n",
        "    for h in paths:\n",
        "        if not h.exists():\n",
        "            continue\n",
        "        if h.is_file() and h.suffix.lower() == '.csv':\n",
        "            cands.append(h)\n",
        "        elif h.is_dir():\n",
        "            cands.extend(h.rglob('*.csv'))\n",
        "    if not cands: return None\n",
        "    return sorted(cands, key=lambda p: p.stat().st_mtime)[-1]\n",
        "\n",
        "def long_from_predictions_csv(path: Path) -> pd.DataFrame:\n",
        "    df = pd.read_csv(path)\n",
        "    cols = {c.lower(): c for c in df.columns}\n",
        "    # long\n",
        "    if {'planet_id','wavelength_index','mu'}.issubset(cols):\n",
        "        return df.rename(columns={cols['planet_id']:'planet_id', cols['wavelength_index']:'wavelength_index', cols['mu']:'mu'})[['planet_id','wavelength_index','mu']]\n",
        "    # wide\n",
        "    mu_cols = [c for c in df.columns if str(c).startswith('mu_')]\n",
        "    if mu_cols:\n",
        "        id_col = None\n",
        "        for k in ('planet_id','id','row_id','sample_id'):\n",
        "            if k in cols: id_col = cols[k]; break\n",
        "        if id_col is None:\n",
        "            df = df.copy(); df.insert(0,'planet_id', np.arange(len(df)))\n",
        "        else:\n",
        "            df = df.rename(columns={id_col:'planet_id'})\n",
        "        rows = []\n",
        "        for _,r in df.iterrows():\n",
        "            pid = r['planet_id']\n",
        "            for j,mc in enumerate(sorted(mu_cols)):\n",
        "                rows.append({'planet_id': pid, 'wavelength_index': j, 'mu': float(r[mc])})\n",
        "        return pd.DataFrame(rows)\n",
        "    raise ValueError(f'Unsupported predictions schema: {path}')\n",
        "\n",
        "def load_labels(path: Optional[Path]) -> Optional[pd.DataFrame]:\n",
        "    if path is None: return None\n",
        "    if not path.exists(): return None\n",
        "    df = pd.read_csv(path)\n",
        "    c = {x.lower(): x for x in df.columns}\n",
        "    if {'planet_id','wavelength_index','y'}.issubset(c):\n",
        "        return df.rename(columns={c['planet_id']:'planet_id', c['wavelength_index']:'wavelength_index', c['y']:'y'})[['planet_id','wavelength_index','y']]\n",
        "    # wide case (rare) – convert\n",
        "    num_cols = [col for col in df.columns if np.issubdtype(df[col].dtype, np.number)]\n",
        "    if num_cols:\n",
        "        if 'planet_id' in c:\n",
        "            df = df.rename(columns={c['planet_id']:'planet_id'})\n",
        "        else:\n",
        "            df = df.copy(); df.insert(0, 'planet_id', np.arange(len(df)))\n",
        "        rows = []\n",
        "        for _,r in df.iterrows():\n",
        "            pid = r['planet_id']\n",
        "            for j,col in enumerate(num_cols):\n",
        "                rows.append({'planet_id': pid, 'wavelength_index': j, 'y': float(r[col])})\n",
        "        return pd.DataFrame(rows)\n",
        "    return None\n",
        "\n",
        "def gll_from_long(pred_long: pd.DataFrame, labels_long: pd.DataFrame, sigma_col: Optional[str] = 'sigma') -> Optional[float]:\n",
        "    if labels_long is None: return None\n",
        "    df = pred_long.copy()\n",
        "    df = df.merge(labels_long, on=['planet_id','wavelength_index'], how='inner')\n",
        "    if df.empty: return None\n",
        "    mu = df['mu'].to_numpy(float)\n",
        "    y  = df['y'].to_numpy(float)\n",
        "    if sigma_col in df.columns:\n",
        "        s = df[sigma_col].to_numpy(float)\n",
        "        s = np.where(s<=0, np.nan, s)\n",
        "        term = ((mu - y)**2)/(s**2) + np.log(2*np.pi*(s**2))\n",
        "        gll = -0.5*np.nanmean(term)\n",
        "    else:\n",
        "        # Fallback: scaled negative MSE as a proxy (not true GLL)\n",
        "        mse = np.nanmean((mu - y)**2)\n",
        "        gll = -mse\n",
        "    return float(gll)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Harvest V50 runs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": { "tags": ["v50"] },
      "execution_count": null,
      "outputs": [],
      "source": [
        "RUNS = [p for p in list_run_dirs(OUT_ROOT) if match_tags(p, V50_RUN_TAGS)]\n",
        "print(f'Found {len(RUNS)} V50 runs (after tag filter).')\n",
        "\n",
        "LABELS = load_labels(LABELS_HINT)\n",
        "if LABELS is not None:\n",
        "    print('Loaded labels:', LABELS.shape)\n",
        "\n",
        "v50_rows = []\n",
        "for rd in RUNS:\n",
        "    art = find_artifacts(rd)\n",
        "    met = load_metrics(art['metrics_csv'])\n",
        "    pred_path = art['predictions_csv']\n",
        "    eval_gll = None\n",
        "    if pred_path and pred_path.exists():\n",
        "        try:\n",
        "            pred_long = long_from_predictions_csv(pred_path)\n",
        "            eval_gll = gll_from_long(pred_long, LABELS) if LABELS is not None else None\n", 
        "        except Exception as e:\n",
        "            print(f'GLL eval failed for {rd.name}:', e)\n",
        "    row = {'run_dir': str(rd.relative_to(ROOT)), 'type': 'V50', 'eval_gll': eval_gll}\n",
        "    if met is not None and len(met):\n",
        "        last = met.iloc[-1]\n",
        "        row['val_gll'] = last.get('val_gll', np.nan)\n",
        "        row['val_loss'] = last.get('val_loss', np.nan)\n",
        "        row['val_coverage'] = last.get('val_coverage', np.nan)\n",
        "    v50_rows.append(row)\n",
        "\n",
        "V50 = pd.DataFrame(v50_rows)\n",
        "display(V50.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load baselines\n",
        "We ingest baseline predictions and compute evaluation GLL (if labels provided)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": { "tags": ["baselines"] },
      "execution_count": null,
      "outputs": [],
      "source": [
        "baseline_rows = []\n",
        "for name, path in BASELINES.items():\n",
        "    if not path.exists():\n",
        "        print(f'Baseline {name} not found at {path}'); continue\n",
        "    pred_long = long_from_predictions_csv(path)\n",
        "    eval_gll = gll_from_long(pred_long, LABELS) if LABELS is not None else None\n",
        "    baseline_rows.append({'run_dir': str(path.relative_to(ROOT)), 'type': 'baseline', 'name': name, 'eval_gll': eval_gll})\n",
        "\n",
        "BL = pd.DataFrame(baseline_rows)\n",
        "display(BL.head() if not BL.empty else BL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Leaderboard\n",
        "We concatenate V50 and baselines and sort by **eval_gll** (if available) else **val_gll** (descending is better)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": { "tags": ["leaderboard"] },
      "execution_count": null,
      "outputs": [],
      "source": [
        "LB = pd.concat([\n",
        "    V50.assign(name=V50['run_dir'].apply(lambda s: Path(s).name)),\n",
        "    BL\n",
        "], ignore_index=True)\n",
        "\n",
        "def sort_key(df):\n",
        "    if 'eval_gll' in df and df['eval_gll'].notna().any():\n",
        "        return df.sort_values(['eval_gll'], ascending=False)\n",
        "    elif 'val_gll' in df and df['val_gll'].notna().any():\n",
        "        return df.sort_values(['val_gll'], ascending=False)\n",
        "    else:\n",
        "        return df\n",
        "\n",
        "LB = sort_key(LB).reset_index(drop=True)\n",
        "display(LB.head(20))\n",
        "\n",
        "LB_CSV = NB_OUT/'benchmark_leaderboard.csv'\n",
        "LB.to_csv(LB_CSV, index=False)\n",
        "print('Wrote:', LB_CSV)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plots\n",
        "- **Bar**: eval_gll (or val_gll) for top entries.\n",
        "- **Overlay**: example planet predictions vs labels for selected models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": { "tags": ["plots"] },
      "execution_count": null,
      "outputs": [],
      "source": [
        "plt.figure(figsize=(11,4))\n",
        "TOPK = min(12, len(LB))\n",
        "df = LB.head(TOPK).copy()\n",
        "metric = 'eval_gll' if df['eval_gll'].notna().any() else ('val_gll' if 'val_gll' in df else None)\n",
        "if metric is not None:\n",
        "    sns.barplot(x=metric, y='name', data=df, orient='h', hue='type', dodge=False)\n",
        "    plt.title(f'Top {TOPK} models by {metric}')\n",
        "    plt.xlabel(metric); plt.ylabel('model/run')\n",
        "    plt.tight_layout(); plt.savefig(NB_OUT/'bar_top_gll.png', dpi=150); plt.close()\n",
        "    print('Saved:', NB_OUT/'bar_top_gll.png')\n",
        "else:\n",
        "    print('No GLL metric available for plotting.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Overlays: predictions vs labels (sample planet)\n",
        "If labels are present, we pick a planet and overlay spectra from the few best models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": { "tags": ["overlay"] },
      "execution_count": null,
      "outputs": [],
      "source": [
        "if LABELS is not None and not LABELS.empty and not LB.empty:\n",
        "    # Choose top 3\n",
        "    top = LB.head(min(3,len(LB)))\n",
        "    # Choose a planet id present in labels\n",
        "    pid = LABELS['planet_id'].iloc[0]\n",
        "\n",
        "    plt.figure(figsize=(10,4))\n",
        "    # plot labels\n",
        "    y_df = LABELS[LABELS['planet_id']==pid].sort_values('wavelength_index')\n",
        "    plt.plot(y_df['wavelength_index'], y_df['y'], color='k', lw=1.6, label='labels (y)')\n",
        "\n",
        "    # Add each model\n",
        "    for _,r in top.iterrows():\n",
        "        if r['type']=='V50':\n",
        "            run_dir = ROOT / r['run_dir']\n",
        "            pred_path = find_artifacts(run_dir)['predictions_csv']\n",
        "        else:\n",
        "            pred_path = ROOT / r['run_dir']\n",
        "        try:\n",
        "            p_long = long_from_predictions_csv(pred_path)\n",
        "            p_pid = p_long[p_long['planet_id']==pid].sort_values('wavelength_index')\n",
        "            if len(p_pid):\n",
        "                plt.plot(p_pid['wavelength_index'], p_pid['mu'], lw=1.2, label=r.get('name', Path(r['run_dir']).name))\n",
        "        except Exception as e:\n",
        "            print('Overlay failed for', r.get('name', r['run_dir']), ':', e)\n",
        "\n",
        "    plt.xlabel('wavelength index'); plt.ylabel('value')\n",
        "    plt.title(f'Overlay — planet {pid}')\n",
        "    plt.legend(); plt.tight_layout()\n",
        "    plt.savefig(NB_OUT/'overlay_planet_sample.png', dpi=150); plt.close()\n",
        "    print('Saved:', NB_OUT/'overlay_planet_sample.png')\n",
        "else:\n",
        "    print('Labels not available or leaderboard empty; skip overlay.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Export Markdown & HTML leaderboards\n",
        "Useful for PRs/Wikis/Kaggle writeups."
      ]
    },
    {
      "cell_type": "code",
      "metadata": { "tags": ["export"] },
      "execution_count": null,
      "outputs": [],
      "source": [
        "def md_table(df: pd.DataFrame, cols: List[str]) -> str:\n",
        "    cols = [c for c in cols if c in df.columns]\n",
        "    if not cols: return '(no data)'\n",
        "    md = ['| ' + ' | '.join(cols) + ' |', '| ' + ' | '.join(['---']*len(cols)) + ' |']\n",
        "    for _,row in df.iterrows():\n",
        "        md.append('| ' + ' | '.join([str(row.get(c,'')) for c in cols]) + ' |')\n",
        "    return '\\n'.join(md)\n",
        "\n",
        "if not LB.empty:\n",
        "    cols = ['name','type','run_dir','eval_gll','val_gll','val_loss','val_coverage']\n",
        "    md = '# Benchmark Leaderboard\\n\\n' + md_table(LB.head(25), cols)\n",
        "    (NB_OUT/'benchmark_leaderboard.md').write_text(md)\n",
        "    print('Wrote:', NB_OUT/'benchmark_leaderboard.md')\n",
        "\n",
        "    html = ['<html><head><meta charset=\"utf-8\"><title>Benchmark Leaderboard</title></head><body>',\n",
        "            '<h1>Benchmark Leaderboard</h1>', '<pre>', md, '</pre>', '</body></html>']\n",
        "    (NB_OUT/'benchmark_leaderboard.html').write_text('\\n'.join(html), encoding='utf-8')\n",
        "    print('Wrote:', NB_OUT/'benchmark_leaderboard.html')\n",
        "else:\n",
        "    print('Leaderboard empty — no export.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optional: DVC add\n",
        "Register notebook outputs for full reproducibility."
      ]
    },
    {
      "cell_type": "code",
      "metadata": { "tags": ["dvc"] },
      "execution_count": null,
      "outputs": [],
      "source": [
        "if shutil.which('dvc'):\n",
        "    try:\n",
        "        subprocess.run(['dvc','add', str(NB_OUT)], check=False)\n",
        "        subprocess.run(['git','add', f'{NB_OUT}.dvc', '.gitignore'], check=False)\n",
        "        subprocess.run(['dvc','status'], check=False)\n",
        "        print('DVC add done (non‑blocking).')\n",
        "    except Exception as e:\n",
        "        print('DVC step failed (non‑blocking):', e)\n",
        "else:\n",
        "    print('DVC not found; skipping.')"
      ]
    }
  ]
}