{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": { "display_name": "Python 3", "language": "python", "name": "python3" },
    "language_info": { "name": "python", "version": "3.10" },
    "title": "10_kaggle_submission_pipeline.ipynb",
    "authors": ["SpectraMind V50 Team"],
    "spectramind": {
      "role": "orchestration/kaggle_handoff",
      "cli_first": true,
      "outputs_dir": "outputs/notebooks/10_kaggle_submission",
      "reproducibility": { "hydra": true, "dvc": true, "logs": true }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 10 · Kaggle Submission Pipeline (SpectraMind V50)\n",
        "\n",
        "Mission‑grade **handoff notebook** to produce a Kaggle‑ready submission bundle using the official **CLI + Hydra** artifacts only (no ad‑hoc pipeline code).\n",
        "\n",
        "### What this does\n",
        "1) Detects environment (local vs Kaggle kernels) and sets safe, offline defaults.\n",
        "2) Locates model predictions under `outputs/` and converts them to the competition’s expected schema.\n",
        "3) Validates the file (best‑effort + optional CLI validator) and **packages** a `submission.zip`.\n",
        "4) Exports a tiny **manifest** + README for audit. (Optional) DVC‑adds artifacts.\n",
        "\n",
        "### Contract\n",
        "- **CLI‑first**: we only *read* artifacts that your CLI wrote; any regeneration (predict/validate) is invoked via the CLI cells.\n",
        "- **Offline‑friendly**: No internet dependence (works in Kaggle where internet is often disabled).\n",
        "- **Deterministic outputs**: Everything goes to `outputs/notebooks/10_kaggle_submission/`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": { "tags": ["init"] },
      "execution_count": null,
      "outputs": [],
      "source": [
        "import os, sys, json, shutil, zipfile, platform, subprocess, textwrap\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "ROOT = Path.cwd().resolve()\n",
        "NB_OUT = ROOT / 'outputs' / 'notebooks' / '10_kaggle_submission'\n",
        "NB_OUT.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Detect Kaggle notebook environment\n",
        "IS_KAGGLE = Path('/kaggle/working').exists()\n",
        "WORK_DIR = Path('/kaggle/working') if IS_KAGGLE else NB_OUT\n",
        "\n",
        "print('ROOT      :', ROOT)\n",
        "print('NB_OUT    :', NB_OUT)\n",
        "print('IS_KAGGLE :', IS_KAGGLE)\n",
        "print('WORK_DIR  :', WORK_DIR)\n",
        "\n",
        "# Try to locate CLI (optional; only used for validation or to regenerate predictions)\n",
        "CLI = shutil.which('spectramind') or (f\"{sys.executable} {ROOT/'spectramind.py'}\" if (ROOT/'spectramind.py').exists() else f\"{sys.executable} -m spectramind\")\n",
        "print('CLI       :', CLI)\n",
        "\n",
        "ENV_SNAPSHOT = {\n",
        "    'python'   : platform.python_version(),\n",
        "    'platform' : platform.platform(),\n",
        "    'kaggle'   : bool(IS_KAGGLE),\n",
        "}\n",
        "(NB_OUT/'env_snapshot.json').write_text(json.dumps(ENV_SNAPSHOT, indent=2))\n",
        "print('Saved env snapshot.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parameters\n",
        "Edit these if you need a custom source filename or competition slug (for local testing). On Kaggle, we **do not submit** from this notebook; we only produce `submission.csv`/`submission.zip` in `/kaggle/working`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": { "tags": ["params"] },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# A descriptive name for this submission artifact\n",
        "SUBMISSION_NAME = f\"spectramind_v50_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}\"\n",
        "\n",
        "# Competition slug (used only for local Kaggle CLI workflows; not required in Kaggle Kernels)\n",
        "COMPETITION_SLUG = \"ariel-data-challenge-2025\"  # adjust to actual\n",
        "\n",
        "# Where we expect predictions (auto-discovery if None)\n",
        "PRED_SOURCE_HINTS = [\n",
        "    ROOT/'outputs'/'predictions'/'predictions.csv',\n",
        "    ROOT/'outputs'/'predictions.csv',\n",
        "    ROOT/'outputs'/'runs',  # will scan timestamped folders\n",
        "]\n",
        "\n",
        "print('SUBMISSION_NAME   :', SUBMISSION_NAME)\n",
        "print('COMPETITION_SLUG  :', COMPETITION_SLUG)\n",
        "print('PRED_SOURCE_HINTS :', [str(p) for p in PRED_SOURCE_HINTS])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Locate predictions\n",
        "We look for a predictions table under `outputs/`. Flexible schema handling:\n",
        "- **Wide**: columns like `mu_000..mu_282`.\n",
        "- **Long**: `planet_id,wavelength_index,mu` → we pivot to wide if the competition requires a single‑row per ID.\n",
        "\n",
        "If your competition expects a very specific format (e.g., `row_id,prediction`), adapt the mapping cell below accordingly or invoke the CLI validator cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": { "tags": ["discover"] },
      "execution_count": null,
      "outputs": [],
      "source": [
        "def find_predictions(hints):\n",
        "    # if a hint is a file, take it; if a dir, scan inside for *.csv (prefer newest)\n",
        "    candidates = []\n",
        "    for h in hints:\n",
        "        if not h.exists():\n",
        "            continue\n",
        "        if h.is_file() and h.suffix.lower() == '.csv':\n",
        "            candidates.append(h)\n",
        "        elif h.is_dir():\n",
        "            candidates += sorted(h.rglob('*.csv'))\n",
        "    # Prefer files containing 'pred' in name, fallback to any csv\n",
        "    candidates = sorted(candidates, key=lambda p: (('pred' in p.name.lower())*-1, p.stat().st_mtime), reverse=True)\n",
        "    return candidates[0] if candidates else None\n",
        "\n",
        "PRED_FILE = find_predictions(PRED_SOURCE_HINTS)\n",
        "print('Selected prediction file:', PRED_FILE)\n",
        "if PRED_FILE is None:\n",
        "    raise FileNotFoundError('No predictions CSV found under outputs/. Generate predictions first (e.g., 04_predict_v50_demo).')\n",
        "\n",
        "pred_df = pd.read_csv(PRED_FILE)\n",
        "print('pred_df shape:', pred_df.shape)\n",
        "pred_df.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Map to competition schema\n",
        "Adjust this transform to match the **official submission format**. Typical cases:\n",
        "- **Already matches**: your file already has `Id,Prediction` or the required wide columns → just rename.\n",
        "- **Long → wide**: `planet_id,wavelength_index,mu` → pivot to one row per planet (or row_id), with ordered columns `mu_000..mu_282`.\n",
        "\n",
        "Below we implement a robust mapper with a few heuristics. If it can’t infer, it will raise with clear guidance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": { "tags": ["map"] },
      "execution_count": null,
      "outputs": [],
      "source": [
        "def to_competition_schema(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    cols = {c.lower(): c for c in df.columns}\n",
        "\n",
        "    # CASE 0: Already in Kaggle shape (common names); just standardize column names\n",
        "    if {'id','prediction'}.issubset(set(cols)):\n",
        "        out = df.rename(columns={cols['id']:'Id', cols['prediction']:'Prediction'})[['Id','Prediction']]\n",
        "        return out\n",
        "\n",
        "    # CASE 1: Wide mu_000.. columns present\n", 
        "    mu_cols = [c for c in df.columns if str(c).startswith('mu_')]\n",
        "    if mu_cols:\n",
        "        # Use the first ID-ish column as Id\n",
        "        id_col = None\n",
        "        for k in ('id','row_id','planet_id','sample_id'):\n",
        "            if k in cols:\n",
        "                id_col = cols[k]; break\n",
        "        if id_col is None:\n",
        "            df = df.copy()\n",
        "            df.insert(0,'Id', np.arange(len(df)))\n",
        "        else:\n",
        "            df = df.rename(columns={id_col:'Id'})\n",
        "        # Wide format expected by some challenges → rename to canonical\n",
        "        # If competition expects just one column 'Prediction', replace this block accordingly\n",
        "        ordered = ['Id'] + sorted(mu_cols)\n",
        "        return df[ordered]\n",
        "\n",
        "    # CASE 2: Long → Pivot (planet_id, wavelength_index, mu)\n",
        "    if {'planet_id','wavelength_index','mu'}.issubset(set(cols)):\n",
        "        g = df.rename(columns={cols['planet_id']:'planet_id', cols['wavelength_index']:'wavelength_index', cols['mu']:'mu'})\n",
        "        # Create zero-padded column names mu_000..mu_XXX\n",
        "        W = int(g['wavelength_index'].max())+1\n",
        "        g['mu_col'] = g['wavelength_index'].astype(int).map(lambda i: f'mu_{i:03d}')\n",
        "        wide = g.pivot_table(index='planet_id', columns='mu_col', values='mu', aggfunc='mean')\n",
        "        wide = wide.reset_index().rename(columns={'planet_id':'Id'})\n",
        "        # Ensure all mu_000..mu_(W-1) exist\n",
        "        all_mu = [f'mu_{i:03d}' for i in range(W)]\n",
        "        for c in all_mu:\n",
        "            if c not in wide.columns:\n",
        "                wide[c] = np.nan\n",
        "        return wide[['Id'] + all_mu]\n",
        "\n",
        "    raise ValueError('Unrecognized predictions schema. Expected either (Id,Prediction), wide mu_000.., or long (planet_id,wavelength_index,mu).')\n",
        "\n",
        "SUB_DF = to_competition_schema(pred_df)\n",
        "print('Submission shape:', SUB_DF.shape)\n",
        "SUB_DF.head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) File‑level sanity checks\n",
        "We ensure required columns exist, check for NaNs/inf, and impose minimal ordering constraints. For strict validation, use the CLI validator cell below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": { "tags": ["validate_local"] },
      "execution_count": null,
      "outputs": [],
      "source": [
        "def basic_checks(df: pd.DataFrame):\n",
        "    # Has an Id column\n",
        "    if 'Id' not in df.columns:\n",
        "        raise AssertionError('Submission must contain Id column.')\n",
        "    # No duplicates\n",
        "    if df['Id'].duplicated().any():\n",
        "        raise AssertionError('Duplicate Id values found.')\n",
        "    # No infs; allow NaNs only if competition rules permit (here we disallow by default)\n",
        "    if np.isinf(df.select_dtypes(include=[np.number]).to_numpy()).any():\n",
        "        raise AssertionError('Found inf values in numeric columns.')\n",
        "    if df.select_dtypes(include=[np.number]).isna().any().any():\n",
        "        print('WARNING: Found NaNs; if the competition disallows NaN, fill or impute before packaging.')\n",
        "    # Column order: keep Id first\n",
        "    cols = df.columns.tolist()\n",
        "    if cols[0] != 'Id':\n",
        "        df = df[['Id'] + [c for c in cols if c != 'Id']]\n",
        "    return df\n",
        "\n",
        "SUB_DF = basic_checks(SUB_DF)\n",
        "print('Post‑check shape:', SUB_DF.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### (Optional) CLI validation\n",
        "If your repo provides a validator (e.g., `spectramind submit validate --bundle` or a `validate_submission.py`), invoke it here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": { "tags": ["validate_cli"] },
      "execution_count": null,
      "outputs": [],
      "source": [
        "RUN_CLI_VALIDATOR = False  # set True to enable\n",
        "if RUN_CLI_VALIDATOR:\n",
        "    TMP_CSV = NB_OUT / f\"{SUBMISSION_NAME}_submission.csv\"\n",
        "    SUB_DF.to_csv(TMP_CSV, index=False)\n",
        "    print('Wrote candidate CSV:', TMP_CSV)\n",
        "    try:\n",
        "        cmd = [CLI, 'submit', 'validate', f'submission={str(TMP_CSV)}']\n",
        "        print('Running:', ' '.join(cmd))\n",
        "        subprocess.run(cmd, check=True)\n",
        "    except Exception as e:\n",
        "        print('Validator failed (non‑blocking):', e)\n",
        "else:\n",
        "    print('CLI validation disabled. Set RUN_CLI_VALIDATOR=True to run your repo validator.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Write `submission.csv` and `submission.zip`\n",
        "On Kaggle kernels, we place these under `/kaggle/working/` so the “Submit to Competition” button can find them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": { "tags": ["package"] },
      "execution_count": null,
      "outputs": [],
      "source": [
        "SUBMISSION_CSV = WORK_DIR / 'submission.csv'\n",
        "SUBMISSION_ZIP = WORK_DIR / 'submission.zip'\n",
        "\n",
        "SUB_DF.to_csv(SUBMISSION_CSV, index=False)\n",
        "print('Saved:', SUBMISSION_CSV, f'({SUBMISSION_CSV.stat().st_size/1024:.1f} KiB)')\n",
        "\n",
        "with zipfile.ZipFile(SUBMISSION_ZIP, mode='w', compression=zipfile.ZIP_DEFLATED) as zf:\n",
        "    zf.write(SUBMISSION_CSV, arcname='submission.csv')\n",
        "print('Saved:', SUBMISSION_ZIP, f'({SUBMISSION_ZIP.stat().st_size/1024:.1f} KiB)')\n",
        "\n",
        "# Also copy artifacts into NB_OUT for versioning outside Kaggle env\n",
        "if IS_KAGGLE:\n",
        "    shutil.copy2(SUBMISSION_CSV, NB_OUT/SUBMISSION_CSV.name)\n",
        "    shutil.copy2(SUBMISSION_ZIP, NB_OUT/SUBMISSION_ZIP.name)\n",
        "    print('Copied artifacts to NB_OUT for archiving.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Submission manifest & README\n",
        "We store a tiny manifest + README that explains how this bundle was produced (config‑as‑data)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": { "tags": ["manifest"] },
      "execution_count": null,
      "outputs": [],
      "source": [
        "MANIFEST = {\n",
        "    'submission_name' : SUBMISSION_NAME,\n",
        "    'created_utc'     : datetime.utcnow().isoformat(timespec='seconds') + 'Z',\n",
        "    'kaggle_env'      : IS_KAGGLE,\n",
        "    'source_pred_file': str(PRED_FILE.relative_to(ROOT)) if PRED_FILE.exists() else None,\n",
        "    'output_csv'      : str(SUBMISSION_CSV),\n",
        "    'output_zip'      : str(SUBMISSION_ZIP)\n",
        "}\n",
        "(NB_OUT/'submission_manifest.json').write_text(json.dumps(MANIFEST, indent=2))\n",
        "\n",
        "README_TXT = f\"\"\"\n",
        "# SpectraMind V50 — Kaggle Submission Bundle\n",
        "\n",
        "Submission: {SUBMISSION_NAME}\n",
        "\n",
        "Artifacts:\n",
        "- submission.csv  → main file\n",
        "- submission.zip  → zipped submission.csv\n",
        "- submission_manifest.json → provenance info\n",
        "\n",
        "Notes:\n",
        "- Generated offline using the SpectraMind V50 CLI artifacts under outputs/.\n",
        "- If running on Kaggle, the files also reside in /kaggle/working/ for the 'Submit' button.\n",
        "- To re‑generate predictions, use the 04_predict_v50_demo notebook or the CLI predict command.\n",
        "\"\"\"\n",
        "(NB_OUT/'README_submission.txt').write_text(README_TXT.strip())\n",
        "print('Wrote manifest and README to', NB_OUT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### (Optional) DVC add\n",
        "If your repository tracks notebook outputs with DVC, register the artifacts below (non‑blocking if DVC is absent)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": { "tags": ["dvc"] },
      "execution_count": null,
      "outputs": [],
      "source": [
        "if shutil.which('dvc'):\n",
        "    try:\n",
        "        subprocess.run(['dvc','add', str(NB_OUT)], check=False)\n",
        "        subprocess.run(['git','add', f'{NB_OUT}.dvc', '.gitignore'], check=False)\n",
        "        subprocess.run(['dvc','status'], check=False)\n",
        "        print('DVC add done (non‑blocking).')\n",
        "    except Exception as e:\n",
        "        print('DVC step failed (non‑blocking):', e)\n",
        "else:\n",
        "    print('DVC not found; skipping.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## 6) (Optional) Local Kaggle CLI submit\n",
        "For **local** runs (not Kaggle kernels), you can submit with the Kaggle CLI if you’ve configured API credentials (`~/.kaggle/kaggle.json`). This is **disabled** by default and **not needed** inside Kaggle kernels.\n",
        "\n",
        "**Warning:** Do **not** execute this on kernels; use the UI’s _Submit to Competition_ button.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": { "tags": ["submit_local"] },
      "execution_count": null,
      "outputs": [],
      "source": [
        "RUN_LOCAL_KAGGLE_SUBMIT = False  # set True for local-only\n",
        "if RUN_LOCAL_KAGGLE_SUBMIT and not IS_KAGGLE:\n",
        "    try:\n",
        "        # Choose CSV or ZIP depending on competition rules\n",
        "        bundle = SUBMISSION_ZIP if SUBMISSION_ZIP.exists() else SUBMISSION_CSV\n",
        "        cmd = [\n",
        "            'kaggle','competitions','submit','-c', COMPETITION_SLUG,\n",
        "            '-f', str(bundle), '-m', SUBMISSION_NAME\n",
        "        ]\n",
        "        print('Running:', ' '.join(cmd))\n",
        "        subprocess.run(cmd, check=True)\n",
        "    except Exception as e:\n",
        "        print('Local Kaggle submit failed (non‑blocking):', e)\n",
        "else:\n",
        "    print('Local Kaggle submit disabled or running inside Kaggle; skipping.')"
      ]
    }
  ]
}