{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 07 · SHAP Explainability & Attention Overlays (SpectraMind V50)\n",
    "\n",
    "**Purpose.** Quantify and visualize feature influence on predicted transmission spectra using SHAP, and (optionally) fuse with saved attention weights from the spectral GNN branch. Artifacts are saved to `outputs/notebooks/07_shap_explainability/`.\n",
    "\n",
    "**What this notebook does**\n",
    "1. Loads a trained V50 model checkpoint or scripted model from the latest (or chosen) Hydra run directory.\n",
    "2. Loads a small validation/sample set and runs forward predictions.\n",
    "3. Computes SHAP values (Deep/Gradient/Kernel fallback) and exports:\n",
    "   - Beeswarm summary\n",
    "   - Per‑wavelength bar/heatmap\n",
    "   - Spectrum‑overlay plots (|SHAP| vs. wavelength on top of \\u03bc/\\u03c3 predictions)\n",
    "4. If available, fuses SHAP saliency with saved spectral attention weights to highlight consensus regions.\n",
    "5. Captures environment (package versions, git/DVC/Hydra config snapshot) for reproducibility.\n",
    "\n",
    "**Pre‑requisites**\n",
    "- You have at least one completed training/prediction run under `outputs/` (Hydra timestamped folder), e.g. `outputs/2025-08-18/11-52-30/` with `checkpoints/` or a scripted model file and `config.yaml`.\n",
    "- Optional: saved artifacts such as `wavelengths.npy`, `attn_weights.npy`, and a small validation tensor/Parquet for quick analysis.\n",
    "\n",
    "**Reproducibility note**\n",
    "We capture versions, seeds, and resolve the active Hydra run folder. All figures/reports are saved deterministically based on `RANDOM_SEED` unless SHAP estimator randomness is inherent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "setup"
    ]
   },
   "outputs": [],
   "source": [
    "import os, sys, json, glob, time, math, random, platform, subprocess, pathlib\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Optional installs (no-ops if already present)\n",
    "try:\n",
    "    import shap  # noqa: F401\n",
    "except Exception:\n",
    "    %pip -q install shap\n",
    "    import shap\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    from torch import nn\n", 
    "except Exception:\n",
    "    %pip -q install torch --extra-index-url https://download.pytorch.org/whl/cpu\n",
    "    import torch\n",
    "    from torch import nn\n",
    "\n",
    "RANDOM_SEED = 1234\n",
    "np.random.seed(RANDOM_SEED)\n",
    "random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
    "\n",
    "# Paths\n",
    "ROOT = Path.cwd()\n",
    "OUT_NOTEBOOK_DIR = ROOT / \"outputs\" / \"notebooks\" / \"07_shap_explainability\"\n",
    "OUT_NOTEBOOK_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Python:\", platform.python_version())\n",
    "print(\"Torch:\", torch.__version__)\n",
    "import shap as _shap; print(\"SHAP:\", _shap.__version__)\n",
    "print(\"CWD:\", ROOT)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hydra"
    ]
   },
   "outputs": [],
   "source": [
    "# --- Find latest Hydra run (or pick one) ---\n",
    "def find_latest_hydra_run(outputs_root=ROOT/\"outputs\"):\n",
    "    candidates = []\n",
    "    for d in sorted(outputs_root.rglob(\".hydra\")):\n",
    "        run_dir = d.parent\n",
    "        # Prefer runs that contain checkpoints or a scripted model\n", 
    "        score = int((run_dir / \"checkpoints\").exists()) + int(any(run_dir.glob(\"*.pt\"))) + int(any(run_dir.glob(\"*.pth\")))\n",
    "        mtime = run_dir.stat().st_mtime\n",
    "        candidates.append((score, mtime, run_dir))\n",
    "    if not candidates:\n",
    "        return None\n",
    "    candidates.sort(reverse=True)\n",
    "    return candidates[0][2]\n",
    "\n",
    "RUN_DIR = find_latest_hydra_run()\n",
    "print(\"Detected RUN_DIR:\", RUN_DIR)\n",
    "if RUN_DIR is None:\n",
    "    print(\"⚠️ No Hydra run found under outputs/. You can set RUN_DIR manually below.\")\n",
    "\n",
    "# Manual override example:\n",
    "# RUN_DIR = ROOT / \"outputs\" / \"2025-08-18\" / \"11-52-30\"\n",
    "\n",
    "# Load run config if present\n",
    "CFG_PATH = None\n",
    "if RUN_DIR is not None:\n",
    "    for p in [RUN_DIR/\"config.yaml\", RUN_DIR/\".hydra\"/\"config.yaml\"]:\n",
    "        if p.exists():\n",
    "            CFG_PATH = p; break\n",
    "\n",
    "if CFG_PATH:\n",
    "    print(\"Using config:\", CFG_PATH)\n",
    "    print(\"\\n--- Hydra Config (head) ---\")\n",
    "    print(\"\\n\".join(Path(CFG_PATH).read_text().splitlines()[:50]))\n",
    "else:\n",
    "    print(\"⚠️ Could not locate config.yaml (will proceed with defaults).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "model"
    ]
   },
   "outputs": [],
   "source": [
    "# --- Load model (scripted .pt/.pth or Lightning/PyTorch checkpoint) ---\n",
    "def _try_scripted_model(run_dir):\n",
    "    if run_dir is None:\n",
    "        return None\n",
    "    for ext in (\"*.pt\", \"*.pth\"):\n",
    "        cands = list(run_dir.glob(ext))\n",
    "        if cands:\n",
    "            try:\n",
    "                m = torch.jit.load(str(cands[0]), map_location=\"cpu\")\n",
    "                m.eval()\n",
    "                print(\"Loaded scripted model:\", cands[0].name)\n",
    "                return m\n",
    "            except Exception as e:\n",
    "                print(\"Scripted load failed:\", e)\n",
    "    return None\n",
    "\n",
    "def _try_state_dict(run_dir):\n",
    "    # Example: load via model class in repository\n",
    "    if run_dir is None:\n",
    "        return None\n",
    "    ck = None\n",
    "    for p in list((run_dir/\"checkpoints\").glob(\"*.ckpt\")) + list((run_dir/\"checkpoints\").glob(\"*.pth\")):\n",
    "        ck = p; break\n",
    "    if ck is None:\n",
    "        return None\n",
    "    try:\n",
    "        # Attempt to import model class\n",
    "        sys.path.insert(0, str(ROOT))\n",
    "        try:\n",
    "            from src.model.v50_model import V50Model  # adjust to your repo\n",
    "            model = V50Model()\n",
    "            sd = torch.load(str(ck), map_location=\"cpu\")\n",
    "            # Lightning or raw dict\n",
    "            state_dict = sd.get(\"state_dict\", sd)\n",
    "            # Strip prefixes if needed\n",
    "            clean_sd = {k.replace(\"model.\", \"\"): v for k, v in state_dict.items()}\n",
    "            missing, unexpected = model.load_state_dict(clean_sd, strict=False)\n",
    "            print(\"Loaded state_dict from\", ck.name, \"missing:\", len(missing), \"unexpected:\", len(unexpected))\n",
    "            model.eval()\n",
    "            return model\n",
    "        except Exception as ie:\n",
    "            print(\"Repo model import failed:\", ie)\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(\"State_dict load failed:\", e)\n",
    "        return None\n",
    "\n",
    "model = _try_scripted_model(RUN_DIR)\n",
    "if model is None:\n",
    "    model = _try_state_dict(RUN_DIR)\n",
    "\n",
    "if model is None:\n",
    "    # Fallback: define a tiny surrogate so the notebook remains runnable for demo\n",
    "    class TinySurrogate(nn.Module):\n",
    "        def __init__(self, in_dim=512, out_dim=283):\n",
    "            super().__init__()\n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(in_dim, 256), nn.ReLU(),\n",
    "                nn.Linear(256, 256), nn.ReLU(),\n",
    "                nn.Linear(256, out_dim)\n",
    "            )\n",
    "        def forward(self, x):\n",
    "            return self.net(x)\n",
    "    model = TinySurrogate()\n",
    "    print(\"⚠️ Using TinySurrogate demo model (no trained weights found).\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device).eval();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "data"
    ]
   },
   "outputs": [],
   "source": [
    "# --- Load a small sample batch ---\n",
    "def load_sample_inputs(run_dir):\n",
    "    # Try common locations in this repo structure, adjust as needed\n",
    "    candidates = [\n",
    "        run_dir/\"cache\"/\"val_inputs.pt\" if run_dir else None,\n",
    "        run_dir/\"pred_inputs.pt\" if run_dir else None,\n",
    "        ROOT/\"data\"/\"processed\"/\"val_inputs.pt\",\n",
    "        ROOT/\"data\"/\"processed\"/\"val.parquet\"\n",
    "    ]\n",
    "    for c in candidates:\n",
    "        if c is None or not c.exists():\n",
    "            continue\n",
    "        if c.suffix == \".pt\":\n",
    "            t = torch.load(str(c), map_location=\"cpu\")\n",
    "            if isinstance(t, torch.Tensor):\n",
    "                return t[:32]  # small subset\n",
    "        elif c.suffix == \".parquet\":\n",
    "            df = pd.read_parquet(c)\n",
    "            x = torch.tensor(df.values, dtype=torch.float32)\n",
    "            return x[:32]\n",
    "    # Fallback synthetic demo\n",
    "    x = torch.randn(32, 512)\n",
    "    return x\n",
    "\n",
    "X = load_sample_inputs(RUN_DIR)\n",
    "X = X.to(device)\n",
    "print(\"Sample batch:\", tuple(X.shape))\n",
    "\n",
    "# Wavelengths (attempt to load, else synthesize 283 channels)\n",
    "def load_wavelengths(run_dir):\n",
    "    for p in [ROOT/\"data\"/\"meta\"/\"wavelengths.npy\", ROOT/\"data\"/\"meta\"/\"wavelengths.csv\", run_dir/\"wavelengths.npy\" if run_dir else None]:\n",
    "        if p is not None and p.exists():\n",
    "            if p.suffix == \".npy\":\n",
    "                return np.load(p)\n",
    "            else:\n",
    "                return pd.read_csv(p).values.squeeze()\n",
    "    return np.linspace(0.8, 5.0, 283)  # µm synthetic grid\n",
    "\n",
    "WAVELENGTHS = load_wavelengths(RUN_DIR)\n",
    "print(\"Wavelength samples:\", WAVELENGTHS[:5], \"...\", WAVELENGTHS[-5:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "predict"
    ]
   },
   "outputs": [],
   "source": [
    "# --- Forward pass (µ/σ if your head is probabilistic; here we assume deterministic head) ---\n",
    "with torch.no_grad():\n",
    "    Y_pred = model(X)  # [B, 283] expected\n",
    "Y_pred = Y_pred.detach().cpu().numpy()\n",
    "print(\"Pred shape:\", Y_pred.shape)\n",
    "\n",
    "# Save quick preview\n",
    "fig, ax = plt.subplots(1, 1, figsize=(9, 4))\n",
    "ax.plot(WAVELENGTHS, Y_pred[0], lw=1.5)\n",
    "ax.set_xlabel(\"Wavelength [µm]\")\n",
    "ax.set_ylabel(\"Predicted transmission\")\n",
    "ax.set_title(\"Prediction preview (sample 0)\")\n",
    "fig.tight_layout()\n",
    "fig.savefig(OUT_NOTEBOOK_DIR/\"preview_prediction.png\", dpi=150)\n",
    "plt.close(fig)\n",
    "print(\"Saved:\", OUT_NOTEBOOK_DIR/\"preview_prediction.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute SHAP values\n",
    "We attempt DeepExplainer (for typical PyTorch nets). If that fails, we try GradientExplainer; if that also fails, we fall back to KernelExplainer on a subsample (slower, but robust)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "shap"
    ]
   },
   "outputs": [],
   "source": [
    "def compute_shap(model, X, max_bg=64, method_order=(\"deep\", \"gradient\", \"kernel\")):\n",
    "    model.eval()\n",
    "    X_cpu = X.detach().cpu()\n",
    "    # background for SHAP\n",
    "    bg = X_cpu[: min(max_bg, len(X_cpu))]\n",
    "    explainer, shap_values = None, None\n",
    "    for m in method_order:\n",
    "        try:\n",
    "            if m == \"deep\":\n",
    "                e = shap.DeepExplainer(model, bg.to(model.device))\n",
    "                sv = e.shap_values(X)\n",
    "                return e, sv\n",
    "            if m == \"gradient\":\n",
    "                e = shap.GradientExplainer(model, bg.to(model.device))\n",
    "                sv = e.shap_values(X)\n",
    "                return e, sv\n",
    "            if m == \"kernel\":\n",
    "                f = lambda inp: model(torch.tensor(inp, dtype=X.dtype, device=model.device)).detach().cpu().numpy()\n",
    "                e = shap.KernelExplainer(f, shap.kmeans(bg.numpy(), min(20, len(bg))))\n",
    "                sv = e.shap_values(X_cpu.numpy(), nsamples=100)\n",
    "                return e, sv\n",
    "        except Exception as exc:\n",
    "            print(f\"{m} explainer failed:\", exc)\n",
    "            continue\n",
    "    raise RuntimeError(\"All SHAP methods failed.\")\n",
    "\n",
    "explainer, shap_vals = compute_shap(model, X)\n",
    "print(type(explainer))\n",
    "if isinstance(shap_vals, list):  # some explainers return list per output; take first/mean\n",
    "    try:\n",
    "        shap_array = np.stack(shap_vals, axis=0)  # [O, B, in_dim] or similar\n",
    "        shap_vals_mean = shap_array.mean(axis=0)\n",
    "    except Exception:\n",
    "        shap_vals_mean = np.array(shap_vals[0])\n",
    "else:\n",
    "    shap_vals_mean = np.array(shap_vals)\n",
    "\n",
    "print(\"SHAP shape:\", shap_vals_mean.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "plots"
    ]
   },
   "outputs": [],
   "source": [
    "# --- SHAP summary (beeswarm) ---\n",
    "try:\n",
    "    shap.summary_plot(shap_vals_mean, X.detach().cpu().numpy(), show=False, max_display=25)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUT_NOTEBOOK_DIR/\"shap_summary_beeswarm.png\", dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(\"Saved:\", OUT_NOTEBOOK_DIR/\"shap_summary_beeswarm.png\")\n",
    "except Exception as e:\n",
    "    print(\"Beeswarm failed:\", e)\n",
    "\n",
    "# --- Mean |SHAP| per input feature (bar) ---\n",
    "mean_abs = np.mean(np.abs(shap_vals_mean), axis=0)\n",
    "fig, ax = plt.subplots(1,1, figsize=(9,3))\n",
    "ax.plot(mean_abs, lw=1)\n",
    "ax.set_title(\"Mean |SHAP| per input feature\")\n",
    "ax.set_xlabel(\"Input feature index\")\n",
    "ax.set_ylabel(\"Mean |SHAP|\")\n",
    "fig.tight_layout()\n",
    "fig.savefig(OUT_NOTEBOOK_DIR/\"shap_mean_abs_per_feature.png\", dpi=150)\n",
    "plt.close(fig)\n",
    "print(\"Saved:\", OUT_NOTEBOOK_DIR/\"shap_mean_abs_per_feature.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHAP over wavelength (overlay)\n",
    "If your model input already aligns per-wavelength (e.g., features map directly to 283 spectral channels), we can overlay |SHAP| on the predicted spectrum. If inputs are not 1:1 with wavelengths, set or derive a mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "overlay"
    ]
   },
   "outputs": [],
   "source": [
    "# --- Attempt to interpret last 283 features as wavelength-local features ---\n",
    "def to_wavelength_shap(mean_abs, expected=283):\n",
    "    if len(mean_abs) >= expected:\n",
    "        return mean_abs[-expected:]\n",
    "    # If shorter, interpolate to 283\n",
    "    x_old = np.linspace(0, 1, len(mean_abs))\n",
    "    x_new = np.linspace(0, 1, expected)\n",
    "    return np.interp(x_new, x_old, mean_abs)\n",
    "\n",
    "wl_shap = to_wavelength_shap(mean_abs, expected=283)\n",
    "\n",
    "fig, ax1 = plt.subplots(1,1, figsize=(10,4))\n",
    "ax1.plot(WAVELENGTHS, Y_pred[0], color='C0', lw=1.5, label='Predicted spectrum (sample 0)')\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(WAVELENGTHS, wl_shap, color='C3', lw=1.0, alpha=0.9, label='|SHAP| (approx per-λ)')\n",
    "ax1.set_xlabel(\"Wavelength [µm]\")\n",
    "ax1.set_ylabel(\"Transmission\")\n",
    "ax2.set_ylabel(\"|SHAP|\")\n",
    "ax1.set_title(\"Spectrum & |SHAP| overlay\")\n",
    "fig.tight_layout()\n",
    "fig.savefig(OUT_NOTEBOOK_DIR/\"overlay_spectrum_shap.png\", dpi=150)\n",
    "plt.close(fig)\n",
    "print(\"Saved:\", OUT_NOTEBOOK_DIR/\"overlay_spectrum_shap.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SHAP × Attention fusion (optional)\n",
    "If your spectral GNN saved attention weights per wavelength (e.g., `attn_weights.npy` of shape `[heads, 283]` or `[283]`), we can blend them with |SHAP| to highlight consensus regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "attention"
    ]
   },
   "outputs": [],
   "source": [
    "def load_attention(run_dir):\n",
    "    if run_dir is None:\n",
    "        return None\n",
    "    for p in [run_dir/\"attn_weights.npy\", run_dir/\"artifacts\"/\"attn_weights.npy\"]:\n",
    "        if p.exists():\n",
    "            attn = np.load(p)\n",
    "            if attn.ndim == 2:\n",
    "                attn = attn.mean(axis=0)\n",
    "            return attn\n",
    "    return None\n",
    "\n",
    "attn = load_attention(RUN_DIR)\n",
    "if attn is not None and len(attn) != 283:\n",
    "    # interpolate to 283\n",
    "    x_old = np.linspace(0, 1, len(attn))\n",
    "    x_new = np.linspace(0, 1, 283)\n",
    "    attn = np.interp(x_new, x_old, attn)\n",
    "\n",
    "if attn is not None:\n",
    "    attn_norm = (attn - attn.min()) / (attn.max() - attn.min() + 1e-12)\n",
    "    shap_norm = wl_shap / (wl_shap.max() + 1e-12)\n",
    "    fusion = 0.5*attn_norm + 0.5*shap_norm\n",
    "\n",
    "    fig, ax = plt.subplots(1,1, figsize=(10,3.5))\n",
    "    ax.plot(WAVELENGTHS, shap_norm, lw=1.0, label='|SHAP| (norm)')\n",
    "    ax.plot(WAVELENGTHS, attn_norm, lw=1.0, label='Attention (norm)')\n",
    "    ax.plot(WAVELENGTHS, fusion, lw=2.0, label='Fusion', color='C2')\n",
    "    ax.set_title('SHAP × Attention consensus')\n",
    "    ax.set_xlabel('Wavelength [µm]')\n",
    "    ax.legend()\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(OUT_NOTEBOOK_DIR/\"shap_attention_fusion.png\", dpi=150)\n",
    "    plt.close(fig)\n",
    "    print(\"Saved:\", OUT_NOTEBOOK_DIR/\"shap_attention_fusion.png\")\n",
    "else:\n",
    "    print(\"No attention weights found (skipping fusion plot).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HTML report export (optional)\n",
    "Exports a lightweight HTML with inline PNGs and (if possible) a SHAP force plot snippet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "report"
    ]
   },
   "outputs": [],
   "source": [
    "report_path = OUT_NOTEBOOK_DIR/\"shap_report.html\"\n",
    "pngs = [\n",
    "    \"preview_prediction.png\",\n",
    "    \"shap_summary_beeswarm.png\",\n",
    "    \"shap_mean_abs_per_feature.png\",\n",
    "    \"overlay_spectrum_shap.png\",\n",
    "    \"shap_attention_fusion.png\"\n",
    "]\n",
    "html = [\n",
    "    \"<html><head><meta charset='utf-8'><title>SHAP Report</title></head><body>\",\n",
    "    f\"<h1>SHAP Report · {time.strftime('%Y-%m-%d %H:%M:%S')}</h1>\",\n",
    "]\n",
    "if CFG_PATH:\n",
    "    html.append(\"<h2>Hydra Config (head)</h2><pre>\")\n",
    "    head = \"\\n\".join(Path(CFG_PATH).read_text().splitlines()[:80])\n",
    "    html.append(head)\n",
    "    html.append(\"</pre>\")\n",
    "\n",
    "for png in pngs:\n",
    "    p = OUT_NOTEBOOK_DIR/png\n",
    "    if p.exists():\n",
    "        html.append(f\"<h3>{png}</h3><img src='{png}' style='max-width:100%;'>\")\n",
    "\n",
    "html.append(\"</body></html>\")\n",
    "Path(report_path).write_text(\"\\n\".join(html), encoding=\"utf-8\")\n",
    "print(\"Saved:\", report_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Optional) Generate predictions via CLI before explaining\n",
    "If you haven’t produced predictions/artifacts yet, and your environment exposes the CLI, you can trigger a fast run below (adjust flags to your config)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "cli"
    ]
   },
   "outputs": [],
   "source": [
    "USE_CLI = False  # set True to enable\n",
    "if USE_CLI:\n",
    "    cmd = [\n",
    "        \"spectramind\", \"predict\",\n",
    "        \"predict.fast_dev_run=True\",\n",
    "        \"loader.limit=64\"\n",
    "    ]\n",
    "    print(\"Running:\", \" \".join(cmd))\n",
    "    try:\n",
    "        subprocess.run(cmd, check=True)\n",
    "    except Exception as e:\n",
    "        print(\"CLI failed:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment capture & finishing up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "env"
    ]
   },
   "outputs": [],
   "source": [
    "def pip_freeze():\n",
    "    try:\n",
    "        out = subprocess.check_output([sys.executable, \"-m\", \"pip\", \"freeze\"], text=True)\n",
    "        Path(OUT_NOTEBOOK_DIR/\"pip_freeze.txt\").write_text(out)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "ok = pip_freeze()\n",
    "print(\"Saved pip_freeze:\", ok)\n",
    "meta = {\n",
    "    \"timestamp\": time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    \"python\": platform.python_version(),\n",
    "    \"torch\": torch.__version__,\n",
    "    \"shap\": _shap.__version__,\n",
    "    \"device\": str(device),\n",
    "    \"run_dir\": str(RUN_DIR) if RUN_DIR else None,\n",
    "    \"config\": str(CFG_PATH) if CFG_PATH else None,\n",
    "}\n",
    "Path(OUT_NOTEBOOK_DIR/\"meta.json\").write_text(json.dumps(meta, indent=2))\n",
    "print(json.dumps(meta, indent=2))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "py3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}