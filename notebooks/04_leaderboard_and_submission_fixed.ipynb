{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14d706d0",
   "metadata": {},
   "source": [
    "# üèÅ SpectraMind V50 ‚Äî 04 ¬∑ Leaderboard & Submission\n",
    "\n",
    "This notebook closes the loop for the **NeurIPS 2025 Ariel Data Challenge**:\n",
    "\n",
    "- Aggregate **ablation/tuning** results and pick a best run\n",
    "- Run **self-test** to validate the pipeline\n",
    "- Generate a **submission** (Œº, œÉ) with CLI\n",
    "- Perform basic **schema checks** on the submission\n",
    "- (Optional) **Upload** to Kaggle via API\n",
    "- Append an entry to `v50_debug_log.md` for reproducibility\n",
    "\n",
    "> CLI-first, Hydra-driven, DVC-aware. Cells auto-skip gracefully if CLI isn't on PATH.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d84a13",
   "metadata": {},
   "source": [
    "## üîß Environment & Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70c6bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json, os, sys, platform, shutil\n",
    "from datetime import datetime\n",
    "\n",
    "ROOT = Path.cwd()\n",
    "ART = ROOT / \"artifacts\"\n",
    "ABLATE_DIR = ART / \"ablation\"\n",
    "DIAG_DIR = ART / \"diagnostics\"\n",
    "SUBMIT_DIR = ART / \"submission\"\n",
    "SUBMIT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "LEADERBOARD_JSON = ABLATE_DIR / \"leaderboard.json\"\n",
    "LEADERBOARD_CSV  = ABLATE_DIR / \"leaderboard.csv\"\n",
    "BEST_RUN_JSON    = ABLATE_DIR / \"best_run.json\"   # optional\n",
    "SUBMISSION_CSV   = SUBMIT_DIR / \"submission.csv\"\n",
    "BUNDLE_ZIP       = SUBMIT_DIR / \"submission_bundle.zip\"\n",
    "LOG_MD           = ROOT / \"v50_debug_log.md\"\n",
    "\n",
    "env = {\n",
    "    \"timestamp\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "    \"python\": sys.version.replace(\"\\n\", \" \"),\n",
    "    \"platform\": platform.platform(),\n",
    "    \"cwd\": str(ROOT),\n",
    "    \"paths\": {\n",
    "        \"ART\": str(ART),\n",
    "        \"ABLATE_DIR\": str(ABLATE_DIR),\n",
    "        \"DIAG_DIR\": str(DIAG_DIR),\n",
    "        \"SUBMIT_DIR\": str(SUBMIT_DIR),\n",
    "        \"LEADERBOARD_JSON\": str(LEADERBOARD_JSON),\n",
    "        \"LEADERBOARD_CSV\": str(LEADERBOARD_CSV),\n",
    "        \"BEST_RUN_JSON\": str(BEST_RUN_JSON),\n",
    "        \"SUBMISSION_CSV\": str(SUBMISSION_CSV),\n",
    "        \"BUNDLE_ZIP\": str(BUNDLE_ZIP),\n",
    "        \"LOG_MD\": str(LOG_MD),\n",
    "    }\n",
    "}\n",
    "print(json.dumps(env, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c91eb6c",
   "metadata": {},
   "source": [
    "## ü©∫ CLI sanity (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28545e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil, subprocess\n",
    "\n",
    "def check_cli(cmd=\"spectramind\", args=[\"--version\"]):\n",
    "    exe = shutil.which(cmd)\n",
    "    if not exe:\n",
    "        print(\"‚ö†Ô∏è 'spectramind' CLI not found on PATH. Notebook will still run, but CLI calls will be skipped.\")\n",
    "        return {\"available\": False}\n",
    "    try:\n",
    "        out = subprocess.check_output([cmd] + args, stderr=subprocess.STDOUT, text=True, timeout=30)\n",
    "        print(out)\n",
    "        return {\"available\": True, \"output\": out}\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è CLI call failed: {e}\")\n",
    "        return {\"available\": True, \"error\": str(e)}\n",
    "\n",
    "cli_info = check_cli()\n",
    "cli_info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460934da",
   "metadata": {},
   "source": [
    "## üìä Aggregate ablation & pick best run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3af15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, math, json\n",
    "from pathlib import Path\n",
    "\n",
    "best = None\n",
    "rows = []\n",
    "\n",
    "# Try CSV first\n",
    "if LEADERBOARD_CSV.exists():\n",
    "    try:\n",
    "        import pandas as pd\n",
    "        df = pd.read_csv(LEADERBOARD_CSV)\n",
    "        print(\"Loaded ablation leaderboard CSV:\", LEADERBOARD_CSV, \"shape=\", df.shape)\n",
    "        # Try common metric keys, prefer lower-is-better GLL if present\n",
    "        metric_key = None\n",
    "        for k in [\"gll\", \"GLL\", \"score\", \"val_gll\", \"val_score\", \"mean_gll\"]:\n",
    "            if k in df.columns:\n",
    "                metric_key = k\n",
    "                break\n",
    "        if metric_key:\n",
    "            # lower GLL better; if score, assume higher is better and invert for sort flag\n",
    "            ascending = True if \"gll\" in metric_key.lower() else False\n",
    "            df_sorted = df.sort_values(metric_key, ascending=ascending)\n",
    "            best = df_sorted.iloc[0].to_dict()\n",
    "            display_cols = [c for c in df.columns if c in [\"run_id\", \"short_id\", \"config\", metric_key]]\n",
    "            print(\"Top 5 by\", metric_key)\n",
    "            display(df_sorted[display_cols].head(5))\n",
    "        else:\n",
    "            print(\"No known metric column found; showing head:\")\n",
    "            display(df.head())\n",
    "    except Exception as e:\n",
    "        print(\"CSV parse failed:\", e)\n",
    "\n",
    "# Fallback to JSON\n",
    "if best is None and LEADERBOARD_JSON.exists():\n",
    "    try:\n",
    "        with open(LEADERBOARD_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        # Expect list of dicts; pick by gll or score\n",
    "        metric_key = None\n",
    "        if isinstance(data, list) and data:\n",
    "            keys = list(data[0].keys())\n",
    "            for k in [\"gll\", \"GLL\", \"score\", \"val_gll\", \"val_score\", \"mean_gll\"]:\n",
    "                if k in keys:\n",
    "                    metric_key = k\n",
    "                    break\n",
    "        if metric_key:\n",
    "            def sort_key(d):\n",
    "                v = d.get(metric_key, math.inf)\n",
    "                try: return float(v)\n",
    "                except: return math.inf\n",
    "            # lower gll better; if score, higher better\n",
    "            reverse = False if \"gll\" in metric_key.lower() else True\n",
    "            data_sorted = sorted(data, key=sort_key, reverse=reverse)\n",
    "            best = data_sorted[0]\n",
    "            print(\"Top 3 candidates by\", metric_key)\n",
    "            for i, d in enumerate(data_sorted[:3]):\n",
    "                rid = d.get(\"run_id\") or d.get(\"short_id\") or f\"r{i}\"\n",
    "                print(i+1, rid, metric_key, d.get(metric_key))\n",
    "        else:\n",
    "            print(\"JSON leaderboard present but no known metric. Keys:\", list(data[0].keys()) if isinstance(data, list) and data else None)\n",
    "    except Exception as e:\n",
    "        print(\"JSON parse failed:\", e)\n",
    "\n",
    "# Persist best run selection (optional)\n",
    "if best:\n",
    "    with open(BEST_RUN_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(best, f, indent=2)\n",
    "    print(\"Best run saved to:\", BEST_RUN_JSON)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No ablation leaderboard found; proceeding without an explicit best-run selection.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a224fcc",
   "metadata": {},
   "source": [
    "## ‚úÖ Run pipeline self-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b5b0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, shutil\n",
    "\n",
    "def run_selftest():\n",
    "    exe = shutil.which(\"spectramind\")\n",
    "    if not exe:\n",
    "        print(\"‚ö†Ô∏è spectramind CLI not found; skipping self-test.\")\n",
    "        return False\n",
    "    # Try deep test first, fallback to fast\n",
    "    cmds = [\n",
    "        [\"spectramind\", \"test\", \"--deep\"],\n",
    "        [\"spectramind\", \"test\"]\n",
    "    ]\n",
    "    for cmd in cmds:\n",
    "        try:\n",
    "            print(\"Running:\", \" \".join(cmd))\n",
    "            subprocess.check_call(cmd, timeout=1800)\n",
    "            print(\"‚úÖ Self-test passed:\", \" \".join(cmd))\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(\"Self-test variant failed:\", e)\n",
    "    print(\"‚ö†Ô∏è All self-test variants failed.\")\n",
    "    return False\n",
    "\n",
    "_ = run_selftest()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56050b6a",
   "metadata": {},
   "source": [
    "## üì¶ Generate submission (Œº/œÉ) via CLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1703e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, shutil\n",
    "from pathlib import Path\n",
    "\n",
    "def generate_submission(out_csv: Path):\n",
    "    exe = shutil.which(\"spectramind\")\n",
    "    if not exe:\n",
    "        print(\"‚ö†Ô∏è spectramind CLI not found; skipping submission generation.\")\n",
    "        return False\n",
    "    out_csv.parent.mkdir(parents=True, exist_ok=True)\n",
    "    cmd = [\"spectramind\", \"submit\", \"--out\", str(out_csv), \"--include-diagnostics\", \"true\"]\n",
    "    # If BEST_RUN_JSON exists and includes a config or short_id, pass it through as override if supported\n",
    "    if BEST_RUN_JSON.exists():\n",
    "        try:\n",
    "            import json\n",
    "            best = json.loads(BEST_RUN_JSON.read_text())\n",
    "            # attempt a generic override key if available\n",
    "            if \"config\" in best and isinstance(best[\"config\"], str):\n",
    "                cmd += [\"--config\", best[\"config\"]]\n",
    "            elif \"short_id\" in best:\n",
    "                cmd += [\"--run-id\", str(best[\"short_id\"])]\n",
    "        except Exception as e:\n",
    "            print(\"Note: couldn't parse BEST_RUN_JSON for overrides:\", e)\n",
    "    print(\"Running:\", \" \".join(cmd))\n",
    "    try:\n",
    "        subprocess.check_call(cmd, timeout=6*3600)  # allow long run\n",
    "        print(\"‚úÖ Submission written:\", out_csv)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(\"‚ùå Submission generation failed:\", e)\n",
    "        return False\n",
    "\n",
    "ok = generate_submission(SUBMISSION_CSV)\n",
    "ok\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f35e2e",
   "metadata": {},
   "source": [
    "## üîé Basic submission schema check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e396fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def validate_submission(path: Path):\n",
    "    if not path.exists():\n",
    "        print(\"‚ùå No submission file found at\", path)\n",
    "        return False\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "    except Exception as e:\n",
    "        print(\"‚ùå CSV read failed:\", e)\n",
    "        return False\n",
    "\n",
    "    print(\"Submission shape:\", df.shape)\n",
    "    # Very generic checks: non-empty, numeric columns present\n",
    "    if df.empty:\n",
    "        print(\"‚ùå Submission is empty.\")\n",
    "        return False\n",
    "\n",
    "    # Try to detect numeric prediction columns (Œº/œÉ); require at least a few numeric columns\n",
    "    num_cols = [c for c in df.columns if np.issubdtype(df[c].dtype, np.number)]\n",
    "    if len(num_cols) < 10:\n",
    "        print(\"‚ö†Ô∏è Fewer than 10 numeric columns detected; ensure Œº/œÉ columns are included.\")\n",
    "    else:\n",
    "        # Check no NaNs in prediction columns\n",
    "        nan_counts = df[num_cols].isna().sum().sum()\n",
    "        if nan_counts > 0:\n",
    "            print(f\"‚ö†Ô∏è Found {nan_counts} NaNs in numeric columns; please investigate.\")\n",
    "\n",
    "    # Optional: enforce monotonic ID or required column name heuristics if known\n",
    "    print(\"‚úÖ Basic checks completed.\")\n",
    "    return True\n",
    "\n",
    "_ = validate_submission(SUBMISSION_CSV)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607c39ef",
   "metadata": {},
   "source": [
    "## üóúÔ∏è Bundle submission + diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8a9be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "def bundle_zip(bundle: Path, submission: Path, extras: list[Path] = None):\n",
    "    extras = extras or []\n",
    "    with zipfile.ZipFile(bundle, \"w\", compression=zipfile.ZIP_DEFLATED) as z:\n",
    "        if submission.exists():\n",
    "            z.write(submission, submission.name)\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è No submission to include.\")\n",
    "        for p in extras:\n",
    "            if p.exists():\n",
    "                if p.is_file():\n",
    "                    z.write(p, p.name)\n",
    "                else:\n",
    "                    # add directory contents flatly\n",
    "                    for child in p.rglob(\"*\"):\n",
    "                        if child.is_file():\n",
    "                            arc = child.relative_to(p.parent)\n",
    "                            z.write(child, str(arc))\n",
    "    print(\"Bundle written:\", bundle)\n",
    "\n",
    "extras = [DIAG_DIR] if DIAG_DIR.exists() else []\n",
    "bundle_zip(BUNDLE_ZIP, SUBMISSION_CSV, extras)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d935819f",
   "metadata": {},
   "source": [
    "## ‚§¥Ô∏è (Optional) Push to Kaggle via API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ba8fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell attempts a Kaggle upload if configured.\n",
    "# Prerequisites:\n",
    "#  - Install kaggle:  pip install kaggle\n",
    "#  - Place Kaggle credentials (~/.kaggle/kaggle.json) or set KAGGLE_USERNAME / KAGGLE_KEY env vars\n",
    "#  - Set COMPETITION to the exact competition slug\n",
    "\n",
    "import os, subprocess, shutil\n",
    "\n",
    "COMPETITION = os.environ.get(\"KAGGLE_COMPETITION\", \"ariel-data-challenge-2025\")\n",
    "\n",
    "def kaggle_submit(csv_path: Path, message: str = \"SpectraMind V50 submission\"):\n",
    "    if not csv_path.exists():\n",
    "        print(\"‚ö†Ô∏è No CSV to submit.\")\n",
    "        return False\n",
    "    # Try kaggle CLI\n",
    "    exe = shutil.which(\"kaggle\")\n",
    "    if not exe:\n",
    "        print(\"‚ÑπÔ∏è kaggle CLI not found; skipping Kaggle upload.\")\n",
    "        return False\n",
    "    try:\n",
    "        cmd = [\"kaggle\", \"competitions\", \"submit\", \"-c\", COMPETITION, \"-f\", str(csv_path), \"-m\", message]\n",
    "        print(\"Running:\", \" \".join(cmd))\n",
    "        subprocess.check_call(cmd, timeout=600)\n",
    "        print(\"‚úÖ Kaggle submission attempted.\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(\"‚ö†Ô∏è Kaggle submission failed:\", e)\n",
    "        return False\n",
    "\n",
    "# Disabled by default; uncomment to attempt submission\n",
    "# _ = kaggle_submit(SUBMISSION_CSV, \"SpectraMind V50 auto-generated submission\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c2fde6",
   "metadata": {},
   "source": [
    "## üßæ Append run metadata to `v50_debug_log.md`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e9ec20",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "entry = f\"\"\"### Notebook: 04_leaderboard_and_submission.ipynb\n",
    "- timestamp: {datetime.now().isoformat(timespec=\"seconds\")}\n",
    "- cwd: {ROOT}\n",
    "- python: {platform.python_version()}\n",
    "- actions:\n",
    "  - ablation_leaderboard_csv: {LEADERBOARD_CSV.exists()}\n",
    "  - ablation_leaderboard_json: {LEADERBOARD_JSON.exists()}\n",
    "  - best_run_json: {BEST_RUN_JSON.exists()}\n",
    "  - submission_csv_exists: {SUBMISSION_CSV.exists()}\n",
    "  - bundle_zip_exists: {BUNDLE_ZIP.exists()}\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    with open(LOG_MD, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(entry + \"\\n\")\n",
    "    print(f\"Appended notebook log entry to {LOG_MD}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è Could not append to {LOG_MD}: {e}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
