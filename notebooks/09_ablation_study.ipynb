{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": { "display_name": "Python 3", "language": "python", "name": "python3" },
    "language_info": { "name": "python", "version": "3.10" },
    "title": "09_ablation_study.ipynb",
    "authors": ["SpectraMind V50 Team"],
    "spectramind": {
      "role": "orchestration/ablation",
      "cli_first": true,
      "outputs_dir": "outputs/notebooks/09_ablation_study",
      "reproducibility": { "hydra": true, "dvc": true, "logs": true }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 09 · Ablation Study (SpectraMind V50)\n",
        "\n",
        "Mission‑grade **ablation orchestrator** for V50 using the **CLI + Hydra** only (no ad‑hoc training code). This notebook:\n",
        "\n",
        "1) Defines a small **grid of Hydra overrides** for symbolic/architecture flags (ablation candidates).\n",
        "2) Invokes the official CLI ablation runner (e.g., `spectramind ablate ...`) to execute multiple runs.\n",
        "3) Collects all **run metrics** from `outputs/` and compiles a **leaderboard** (CSV/MD/HTML).\n",
        "4) Renders comparison plots (e.g., Val GLL, coverage metrics) and exports them under `outputs/notebooks/09_ablation_study/`.\n",
        "5) Optionally registers artifacts with **DVC** for reproducibility.\n",
        "\n",
        "_Notebook contract_: **Thin orchestration** over CLI/Hydra; all artifacts written under `outputs/` and tracked as appropriate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": { "tags": ["init"] },
      "execution_count": null,
      "outputs": [],
      "source": [
        "import os, sys, json, subprocess, platform, shutil, time\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set_context('notebook'); sns.set_style('whitegrid')\n",
        "\n",
        "ROOT = Path.cwd()\n",
        "NB_OUT = ROOT / 'outputs' / 'notebooks' / '09_ablation_study'\n",
        "NB_OUT.mkdir(parents=True, exist_ok=True)\n",
        "OUT_ROOT = ROOT / 'outputs'\n",
        "LOGS = ROOT / 'logs'\n",
        "LOGS.mkdir(exist_ok=True, parents=True)\n",
        "\n",
        "print('ROOT:', ROOT)\n",
        "print('NB_OUT:', NB_OUT)\n",
        "print('OUT_ROOT:', OUT_ROOT)\n",
        "print('Python:', platform.python_version())\n",
        "\n",
        "def sh(cmd, check=True, cwd=None, env=None):\n",
        "    print('\\n$', cmd)\n",
        "    r = subprocess.run(cmd, shell=True, cwd=cwd or ROOT, env=env)\n",
        "    if check and r.returncode != 0:\n",
        "        raise RuntimeError(f'Command failed ({r.returncode}): {cmd}')\n",
        "    return r.returncode\n",
        "\n",
        "# Try to find the CLI\n",
        "CLI = shutil.which('spectramind')\n",
        "if CLI is None:\n",
        "    if (ROOT/'spectramind.py').exists():\n",
        "        CLI = f\"{sys.executable} {ROOT/'spectramind.py'}\"\n",
        "    else:\n",
        "        CLI = f\"{sys.executable} -m spectramind\"\n",
        "print('CLI:', CLI)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Parameters — Ablation Grid\n",
        "Edit these lists to control the set of overrides. The ablation runner will combine them (Cartesian product) or accept explicit lists depending on your CLI.\n",
        "\n",
        "**Examples**\n",
        "- Toggle symbolic losses or weights\n",
        "- Switch AIRS encoder (e.g., GAT vs baseline)\n",
        "- Adjust smoothness weight, nonneg cap, batch size/optimizer minor tweaks\n",
        "\n",
        "> Keep the grid **small** when testing interactively (e.g., 3–6 configs). For a larger grid, run headless via CI."
      ]
    },
    {
      "cell_type": "code",
      "metadata": { "tags": ["grid"] },
      "execution_count": null,
      "outputs": [],
      "source": [
        "RUN_TS = datetime.utcnow().strftime('%Y%m%d_%H%M%S')\n",
        "ABLATE_TAG = f\"ablate_{RUN_TS}\"\n",
        "ABLATE_OUTDIR = OUT_ROOT / 'ablate' / ABLATE_TAG\n",
        "ABLATE_OUTDIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Grid candidates (Hydra overrides); keep short for demo\n",
        "SYMBOLIC_ON = [True, False]                      # turn symbolic loss on/off\n",
        "SMOOTH_W = [0.0, 0.1]                            # smoothness weight\n",
        "ENCODER = ['airs_gnn_gat', 'airs_gnn_baseline']  # encoder variants (example names)\n",
        "\n",
        "# Build override strings for each point\n",
        "def build_override(symbolic_on, smooth_w, encoder):\n",
        "    return [\n",
        "        f\"symbolic.enabled={'true' if symbolic_on else 'false'}\",\n",
        "        f\"symbolic.smooth_w={smooth_w}\",\n",
        "        f\"model.airs_encoder={encoder}\",\n",
        "        # Demo: fast run settings (adapt to your code)\n",
        "        \"training.fast_dev_run=true\",\n",
        "        # Tag & output dir\n",
        "        f\"run.tag={ABLATE_TAG}\"\n",
        "    ]\n",
        "\n",
        "GRID = [build_override(a,b,c) for a in SYMBOLIC_ON for b in SMOOTH_W for c in ENCODER]\n",
        "print(f\"Grid size: {len(GRID)}\")\n",
        "for ex in GRID[:3]:\n",
        "    print('example overrides:', ex)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run Ablations via CLI\n",
        "This cell calls the official ablation entry point. Your repository may expose one of these shapes:\n",
        "\n",
        "1. `spectramind ablate` with arguments like `--grid-json` or repeated `+override` flags.\n",
        "2. `spectramind train ...` executed in a loop (fallback mode shown below if no ablate subcommand exists).\n",
        "\n",
        "All runs should write into `outputs/` with a consistent naming/tagging so we can harvest metrics afterward."
      ]
    },
    {
      "cell_type": "code",
      "metadata": { "tags": ["ablate"] },
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Try a native 'ablate' subcommand first; if not, fall back to looping 'train'.\n",
        "def run_ablation_native(overrides_grid):\n",
        "    # If your CLI supports passing a JSON/CSV of overrides, prepare it here.\n",
        "    # For demo, we'll fall back immediately.\n",
        "    return False\n",
        "\n",
        "def run_ablation_loop(overrides_grid):\n",
        "    for i, ov in enumerate(overrides_grid, start=1):\n",
        "        tag = f\"{ABLATE_TAG}_cfg{i:02d}\"\n",
        "        cmd = [\n",
        "            CLI, 'train',\n",
        "            f\"hydra.run.dir=outputs/runs/{tag}\",\n",
        "            f\"hydra.job.name={tag}\",\n",
        "        ] + ov\n",
        "        sh(\" \".join(cmd), check=True)\n",
        "    return True\n",
        "\n",
        "ok = run_ablation_native(GRID)\n",
        "if not ok:\n",
        "    print(\"Native ablate not available; running looped trains...\")\n",
        "    run_ablation_loop(GRID)\n",
        "\n",
        "print(\"\\nAblation batch complete. Tag:\", ABLATE_TAG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Harvest Metrics & Build Leaderboard\n",
        "We walk `outputs/` for runs carrying this ablation tag, parse metrics/configs, and assemble a leaderboard."
      ]
    },
    {
      "cell_type": "code",
      "metadata": { "tags": ["harvest"] },
      "execution_count": null,
      "outputs": [],
      "source": [
        "def find_runs_by_tag(tag):\n",
        "    # Accept layouts like outputs/runs/<tag>*, outputs/YYYY-MM-DD/<time>_<tag>\n",
        "    found = []\n",
        "    # layout A: outputs/runs/\n",
        "    runs_root = OUT_ROOT / 'runs'\n",
        "    if runs_root.exists():\n",
        "        for p in runs_root.glob(f\"{tag}*\"):\n",
        "            if p.is_dir():\n",
        "                found.append(p)\n",
        "    # layout B: date-based\n",
        "    for date_dir in OUT_ROOT.glob(\"20*\"):\n",
        "        if date_dir.is_dir():\n",
        "            for run_dir in date_dir.iterdir():\n",
        "                if run_dir.is_dir() and tag in run_dir.name:\n",
        "                    found.append(run_dir)\n",
        "    # unique\n",
        "    uniq = []\n",
        "    seen = set()\n",
        "    for p in found:\n",
        "        if p not in seen:\n",
        "            uniq.append(p); seen.add(p)\n",
        "    return sorted(uniq)\n",
        "\n",
        "def load_config_snapshot(run_dir: Path):\n",
        "    candidates = [run_dir/'config.yaml', run_dir/'.hydra'/'config.yaml']\n",
        "    for c in candidates:\n",
        "        if c.exists():\n",
        "            return c.read_text(errors='ignore')\n",
        "    return None\n",
        "\n",
        "def load_metrics_csv(run_dir: Path):\n",
        "    for p in [run_dir/'metrics.csv', run_dir/'training_metrics.csv']:\n",
        "        if p.exists():\n",
        "            try:\n",
        "                return pd.read_csv(p)\n",
        "            except Exception:\n",
        "                pass\n",
        "    return None\n",
        "\n",
        "RUNS = find_runs_by_tag(ABLATE_TAG)\n",
        "print(f\"Found {len(RUNS)} runs for tag {ABLATE_TAG}\")\n",
        "\n",
        "rows = []\n",
        "for r in RUNS:\n",
        "    cfg_txt = load_config_snapshot(r)\n",
        "    met = load_metrics_csv(r)\n",
        "    # Heuristic final metrics (adapt to your schema)\n",
        "    if met is not None and len(met):\n",
        "        last = met.iloc[-1]\n",
        "        row = {\n",
        "            'run_dir': str(r.relative_to(ROOT)),\n",
        "            'val_loss': last.get('val_loss', np.nan),\n",
        "            'val_gll': last.get('val_gll', np.nan),\n",
        "            'val_coverage': last.get('val_coverage', np.nan),\n",
        "            'config_head': (\"\\n\".join(cfg_txt.splitlines()[:25]) if cfg_txt else None)\n",
        "        }\n",
        "        rows.append(row)\n",
        "\n",
        "LB = pd.DataFrame(rows)\n",
        "if LB.empty:\n",
        "    print('No metrics harvested. Check your metrics file names/columns.')\n",
        "else:\n",
        "    # rank by Val GLL (descending: higher better); fall back to -val_loss\n",
        "    if 'val_gll' in LB:\n",
        "        LB = LB.sort_values(['val_gll'], ascending=False)\n",
        "    elif 'val_loss' in LB:\n",
        "        LB = LB.sort_values(['val_loss'], ascending=True)\n",
        "    LB.reset_index(drop=True, inplace=True)\n",
        "    display(LB.head())\n",
        "\n",
        "lb_csv = NB_OUT / 'ablation_leaderboard.csv'\n",
        "LB.to_csv(lb_csv, index=False)\n",
        "print('Wrote leaderboard CSV:', lb_csv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plots: Leaderboard & Metric Trends\n",
        "Simple visualizations for top‑k configurations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": { "tags": ["plots"] },
      "execution_count": null,
      "outputs": [],
      "source": [
        "if not LB.empty:\n",
        "    TOPK = min(10, len(LB))\n",
        "    df = LB.head(TOPK).copy()\n",
        "    plt.figure(figsize=(10,4))\n",
        "    if 'val_gll' in df:\n",
        "        sns.barplot(x='val_gll', y='run_dir', data=df, orient='h', color='tab:blue')\n",
        "        plt.title('Top Val GLL (higher is better)')\n",
        "        plt.xlabel('Val GLL'); plt.ylabel('Run')\n",
        "    elif 'val_loss' in df:\n",
        "        sns.barplot(x='val_loss', y='run_dir', data=df, orient='h', color='tab:orange')\n",
        "        plt.title('Top Val Loss (lower is better)')\n",
        "        plt.xlabel('Val Loss'); plt.ylabel('Run')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(NB_OUT/'leaderboard_bar.png', dpi=150)\n",
        "    plt.close()\n",
        "    print('Saved:', NB_OUT/'leaderboard_bar.png')\n",
        "else:\n",
        "    print('Leaderboard empty; skipping plots.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Export Markdown & HTML Leaderboard\n",
        "Convenient artifacts for PRs/Wikis/Kaggle writeups."
      ]
    },
    {
      "cell_type": "code",
      "metadata": { "tags": ["export"] },
      "execution_count": null,
      "outputs": [],
      "source": [
        "def to_markdown_table(df: pd.DataFrame) -> str:\n",
        "    cols = [c for c in ['run_dir','val_gll','val_loss','val_coverage'] if c in df.columns]\n",
        "    md = ['| ' + ' | '.join(cols) + ' |', '| ' + ' | '.join(['---']*len(cols)) + ' |']\n",
        "    for _,row in df.iterrows():\n",
        "        md.append('| ' + ' | '.join([str(row.get(c,'')) for c in cols]) + ' |')\n",
        "    return '\\n'.join(md)\n",
        "\n",
        "if not LB.empty:\n",
        "    md_text = '# Ablation Leaderboard\\n\\n' + to_markdown_table(LB.head(min(25,len(LB))))\n",
        "    (NB_OUT/'ablation_leaderboard.md').write_text(md_text)\n",
        "    print('Wrote:', NB_OUT/'ablation_leaderboard.md')\n",
        "\n",
        "    # Minimal HTML wrapper\n",
        "    html = ['<html><head><meta charset=\"utf-8\"><title>Ablation Leaderboard</title></head><body>',\n",
        "            '<h1>Ablation Leaderboard</h1>',\n",
        "            '<pre>', md_text, '</pre>',\n",
        "            '</body></html>']\n",
        "    (NB_OUT/'ablation_leaderboard.html').write_text('\\n'.join(html), encoding='utf-8')\n",
        "    print('Wrote:', NB_OUT/'ablation_leaderboard.html')\n",
        "else:\n",
        "    print('No rows to export.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optional: DVC Registration\n",
        "If your project uses DVC, add the notebook outputs so runs are fully tracked."
      ]
    },
    {
      "cell_type": "code",
      "metadata": { "tags": ["dvc"] },
      "execution_count": null,
      "outputs": [],
      "source": [
        "if shutil.which('dvc'):\n",
        "    try:\n",
        "        sh(f\"dvc add {NB_OUT}\", check=False)\n",
        "        sh(f\"git add {NB_OUT}.dvc .gitignore\", check=False)\n",
        "        sh(\"dvc status\", check=False)\n",
        "    except Exception as e:\n",
        "        print('DVC step failed (non-blocking):', e)\n",
        "else:\n",
        "    print('DVC not found; skipping.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "### Tips\n",
        "- Scale the grid and **remove `training.fast_dev_run=true`** when running actual ablations.\n",
        "- Use CI to run larger grids and publish the generated **MD/HTML leaderboard** as artifacts.\n",
        "- To include **symbolic diagnostics** in the leaderboard, ensure your training logs export the corresponding metrics (e.g., average violation norms) and add columns here.\n",
        "\n",
        "**Artifacts written to**: `outputs/notebooks/09_ablation_study/`"
      ]
    }
  ]
}