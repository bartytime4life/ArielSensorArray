{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b9a7839",
   "metadata": {},
   "source": [
    "# SpectraMind V50 — Master Walkthrough (One Notebook)\n",
    "\n",
    "End‑to‑end **single notebook** that mirrors the CLI‑first SpectraMind V50 pipeline and bundles the physics demos.\n",
    "\n",
    "**Contents**\n",
    "1. Environment & CLI check\n",
    "2. Quickstart smoke (optional)\n",
    "3. Calibration (pipeline‑safe)\n",
    "4. Train (fast path)\n",
    "5. Predict / Submit bundle (dry‑run)\n",
    "6. Diagnostics (summary + HTML/PNG artifacts)\n",
    "7. Uncertainty calibration sketch (educational)\n",
    "8. Radiation & Noise Modeling (educational)\n",
    "9. Gravitational Lensing Demo (educational)\n",
    "10. Appendix — CLI cheat‑sheet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45822c50",
   "metadata": {
    "tags": [
     "env",
     "init"
    ]
   },
   "outputs": [],
   "source": [
    "import os, sys, json, shutil, subprocess, platform, textwrap\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ROOT = Path.cwd().resolve()\n",
    "NB_OUT = ROOT / 'outputs' / 'notebooks' / 'master_walkthrough'\n",
    "NB_OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "ENV = {\n",
    "    'python': platform.python_version(),\n",
    "    'platform': platform.platform(),\n",
    "    'time': datetime.utcnow().isoformat()+'Z',\n",
    "}\n",
    "(NB_OUT/'env_snapshot.json').write_text(json.dumps(ENV, indent=2))\n",
    "\n",
    "def have(cmd):\n",
    "    return shutil.which(cmd) is not None\n",
    "\n",
    "CLI = 'spectramind'\n",
    "print('ROOT:', ROOT)\n",
    "print('NB_OUT:', NB_OUT)\n",
    "print('CLI present?', have(CLI))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd550fdb",
   "metadata": {},
   "source": [
    "## 1) Environment & CLI check\n",
    "This notebook calls the unified CLI when available. If the `spectramind` CLI is not on PATH, cells will **dry‑run** with safe fallbacks so the notebook remains self‑contained."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e7565cf",
   "metadata": {},
   "source": [
    "## 2) Quickstart smoke (optional)\n",
    "Runs a tiny smoke test to ensure configs & paths resolve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253affad",
   "metadata": {
    "tags": [
     "quickstart"
    ]
   },
   "outputs": [],
   "source": [
    "if have(CLI):\n",
    "    try:\n",
    "        print('> spectramind test --fast')\n",
    "        _ = subprocess.run([CLI, 'test', '--fast'], check=False)\n",
    "    except Exception as e:\n",
    "        print('Smoke skipped:', e)\n",
    "else:\n",
    "    print('CLI not found: quickstart smoke skipped (ok).')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a59d56",
   "metadata": {},
   "source": [
    "## 3) Calibration (pipeline‑safe)\n",
    "Calibrate raw frames to science‑ready light‑curves. Uses a minimal subset for speed if supported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f606e7e8",
   "metadata": {
    "tags": [
     "calibration"
    ]
   },
   "outputs": [],
   "source": [
    "if have(CLI):\n",
    "    try:\n",
    "        print('> spectramind calibrate --sample 5')\n",
    "        _ = subprocess.run([CLI, 'calibrate', '--sample', '5'], check=False)\n",
    "    except Exception as e:\n",
    "        print('Calibration skipped:', e)\n",
    "else:\n",
    "    print('CLI not found: emitting placeholder calibrated artifact…')\n",
    "    # Create a tiny placeholder CSV representing a calibrated spectrum\n",
    "    wl = np.arange(283)\n",
    "    mu = 0.01 + 0.002*np.exp(-0.5*((wl/283-0.35)/0.08)**2)\n",
    "    pd.DataFrame({'wavelength_index': wl, 'mu': mu}).to_csv(NB_OUT/'calibrated_placeholder.csv', index=False)\n",
    "    print('Wrote', (NB_OUT/'calibrated_placeholder.csv').as_posix())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745c5d6d",
   "metadata": {},
   "source": [
    "## 4) Train (fast path)\n",
    "Fast‑dev run to validate shapes, configs, and logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc556668",
   "metadata": {
    "tags": [
     "train"
    ]
   },
   "outputs": [],
   "source": [
    "if have(CLI):\n",
    "    try:\n",
    "        print('> spectramind train --epochs 1 --fast-dev')\n",
    "        _ = subprocess.run([CLI, 'train', '--epochs', '1', '--fast-dev'], check=False)\n",
    "    except Exception as e:\n",
    "        print('Train skipped:', e)\n",
    "else:\n",
    "    print('CLI not found: training step skipped (ok).')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3dcf65",
   "metadata": {},
   "source": [
    "## 5) Predict / Submit bundle (dry‑run)\n",
    "Bundle predictions and submission manifest (dry‑run when CLI is absent)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57835c6d",
   "metadata": {
    "tags": [
     "submit"
    ]
   },
   "outputs": [],
   "source": [
    "if have(CLI):\n",
    "    try:\n",
    "        print('> spectramind submit --dry-run')\n",
    "        _ = subprocess.run([CLI, 'submit', '--dry-run'], check=False)\n",
    "    except Exception as e:\n",
    "        print('Submit skipped:', e)\n",
    "else:\n",
    "    print('CLI not found: creating a placeholder submission.csv…')\n",
    "    # Minimal placeholder\n",
    "    df = pd.DataFrame({'planet_id':[0], **{f'mu_{i}':[0.0] for i in range(283)}})\n",
    "    df.to_csv(NB_OUT/'submission_placeholder.csv', index=False)\n",
    "    print('Wrote', (NB_OUT/'submission_placeholder.csv').as_posix())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94eb2d0c",
   "metadata": {},
   "source": [
    "## 6) Diagnostics (summary)\n",
    "If the CLI saved HTML/PNG reports in outputs, link them here; otherwise produce a tiny overview plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77157cb8",
   "metadata": {
    "tags": [
     "diagnostics"
    ]
   },
   "outputs": [],
   "source": [
    "diag_png = NB_OUT/'quick_diag.png'\n",
    "wl = np.arange(283)\n",
    "mu = 0.01 + 0.002*np.exp(-0.5*((wl/283-0.30)/0.06)**2) + 0.0015*np.exp(-0.5*((wl/283-0.70)/0.05)**2)\n",
    "plt.figure(figsize=(10,3))\n",
    "plt.plot(wl, mu, lw=1.5)\n",
    "plt.title('Quick diagnostic spectrum (placeholder)')\n",
    "plt.xlabel('wavelength index'); plt.ylabel('μ (arb)')\n",
    "plt.tight_layout(); plt.savefig(diag_png, dpi=150); plt.close()\n",
    "print('Saved', diag_png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5917a1a",
   "metadata": {},
   "source": [
    "## 7) Uncertainty calibration (educational sketch)\n",
    "Plot a synthetic **σ vs residual** calibration check to illustrate the idea (not pipeline data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b972536",
   "metadata": {
    "tags": [
     "uncertainty"
    ]
   },
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "n = 1000\n",
    "pred_sigma = np.clip(np.random.lognormal(mean=-5.0, sigma=0.5, size=n), 1e-6, 5e-2)\n",
    "residual = pred_sigma * np.random.normal(0,1,size=n)\n",
    "plt.figure(figsize=(5,4))\n",
    "plt.scatter(pred_sigma, np.abs(residual), s=6, alpha=0.4)\n",
    "plt.plot([pred_sigma.min(), pred_sigma.max()], [pred_sigma.min(), pred_sigma.max()], lw=2)\n",
    "plt.xscale('log'); plt.yscale('log')\n",
    "plt.xlabel('predicted σ'); plt.ylabel('|residual|')\n",
    "plt.title('Calibration sketch (ideal: points along y=x)')\n",
    "plt.tight_layout(); plt.savefig(NB_OUT/'uncertainty_calibration_sketch.png', dpi=150); plt.close()\n",
    "print('Saved uncertainty_calibration_sketch.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab0915ed",
   "metadata": {},
   "source": [
    "## 8) Radiation & Noise Modeling (educational)\n",
    "Demonstrate Poisson shot noise, Gaussian read noise, 1/f noise, and cosmic ray spikes on a base spectrum; then compare FFT/autocorr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d72f03",
   "metadata": {
    "tags": [
     "radiation_noise"
    ]
   },
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(1234)\n",
    "x = np.linspace(0,1,283)\n",
    "mu_base = 0.01 + 0.002*np.exp(-0.5*((x-0.35)/0.08)**2) + 0.0015*np.exp(-0.5*((x-0.75)/0.05)**2)\n",
    "\n",
    "def add_shot(mu, scale_counts=3e5):\n",
    "    lam = np.clip(scale_counts*np.maximum(mu,0), 0, None)\n",
    "    return rng.poisson(lam)/scale_counts\n",
    "def add_read(mu, sigma=2e-4):\n",
    "    return mu + rng.normal(0.0, sigma, size=mu.shape)\n",
    "def add_1f(mu, alpha=1.0, amp=2e-4):\n",
    "    n=len(mu); white=rng.normal(0,1,n)\n",
    "    f=np.fft.rfftfreq(n); spec=np.fft.rfft(white)\n",
    "    shaping=1.0/np.maximum(f,1e-6)**alpha\n",
    "    shaped=spec*shaping\n",
    "    x_ifft=np.fft.irfft(shaped, n)\n",
    "    x_ifft=amp*x_ifft/np.std(x_ifft)\n",
    "    return mu + x_ifft\n",
    "def add_cr(mu, n_hits=3, amp=0.01, kernel=(1.0,0.5)):\n",
    "    y=mu.copy(); W=len(mu); hits=rng.choice(W, size=min(n_hits,W), replace=False)\n",
    "    for h in hits:\n",
    "        for k,a in enumerate(kernel):\n",
    "            idx=h+k\n",
    "            if idx<W:\n",
    "                y[idx]+=a*amp\n",
    "    return y, hits\n",
    "\n",
    "noisy_shot   = add_shot(mu_base)\n",
    "noisy_read   = add_read(mu_base)\n",
    "noisy_1f     = add_1f(mu_base)\n",
    "noisy_cr, H  = add_cr(mu_base, n_hits=4, amp=0.01)\n",
    "\n",
    "fig, ax = plt.subplots(2,2, figsize=(12,6), sharex=True)\n",
    "ax=ax.ravel()\n",
    "ax[0].plot(mu_base, lw=1.2, label='base'); ax[0].plot(noisy_shot, lw=0.9, label='shot'); ax[0].legend(); ax[0].set_title('Shot')\n",
    "ax[1].plot(mu_base, lw=1.2, label='base'); ax[1].plot(noisy_read, lw=0.9, label='read'); ax[1].legend(); ax[1].set_title('Read')\n",
    "ax[2].plot(mu_base, lw=1.2, label='base'); ax[2].plot(noisy_1f, lw=0.9, label='1/f'); ax[2].legend(); ax[2].set_title('1/f')\n",
    "ax[3].plot(mu_base, lw=1.2, label='base'); ax[3].plot(noisy_cr, lw=0.9, label='cosmic'); ax[3].scatter(H, noisy_cr[H], s=22)\n",
    "ax[3].legend(); ax[3].set_title('Cosmic rays')\n",
    "for a in ax: a.set_ylabel('μ (arb)')\n",
    "ax[2].set_xlabel('wavelength index'); ax[3].set_xlabel('wavelength index')\n",
    "fig.tight_layout(); fig.savefig(NB_OUT/'noise_panels.png', dpi=150); plt.close(fig)\n",
    "print('Saved noise_panels.png')\n",
    "\n",
    "def fft_power(y):\n",
    "    y=y-np.nanmean(y)\n",
    "    fy=np.fft.rfft(y); p=np.abs(fy)**2; f=np.fft.rfftfreq(len(y))\n",
    "    return f, p\n",
    "def acorr(y):\n",
    "    y=y-np.nanmean(y)\n",
    "    r=np.correlate(y,y,mode='full'); r=r[r.size//2:]\n",
    "    if r[0]!=0: r=r/r[0]\n",
    "    l=np.arange(r.size); return l,r\n",
    "\n",
    "series={'base':mu_base,'shot':noisy_shot,'read':noisy_read,'1f':noisy_1f,'cr':noisy_cr}\n",
    "fig, ax = plt.subplots(2,2, figsize=(12,6)); ax=ax.ravel()\n",
    "for i,(name, y) in enumerate(series.items()):\n",
    "    f,p=fft_power(y)\n",
    "    ax[0].semilogy(f[1:], p[1:], lw=1, label=name)\n",
    "ax[0].legend(); ax[0].set_title('FFT power (log)'); ax[0].set_xlabel('freq'); ax[0].set_ylabel('power')\n",
    "for i,(name, y) in enumerate(series.items()):\n",
    "    l,r = acorr(y)\n",
    "    ax[1].plot(l, r, lw=1, label=name)\n",
    "ax[1].legend(); ax[1].set_title('Autocorrelation'); ax[1].set_xlabel('lag'); ax[1].set_ylabel('norm acorr')\n",
    "fig.tight_layout(); fig.savefig(NB_OUT/'fft_autocorr_compare.png', dpi=150); plt.close(fig)\n",
    "print('Saved fft_autocorr_compare.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912d25f7",
   "metadata": {},
   "source": [
    "## 9) Gravitational Lensing Demo (educational)\n",
    "Show how time‑varying microlensing **A(t)** during a transit plus wavelength‑dependent weights can bias an effective spectrum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b282505",
   "metadata": {
    "tags": [
     "lensing"
    ]
   },
   "outputs": [],
   "source": [
    "def A_point_lens(u):\n",
    "    u = np.asarray(u, float)\n",
    "    return (u*u + 2) / (u * np.sqrt(u*u + 4))\n",
    "def u_of_t(t, t0=0.0, u0=0.2, tE=0.25):\n",
    "    return np.sqrt(u0*u0 + ((t - t0)/tE)**2)\n",
    "\n",
    "N_EXP=200\n",
    "t = np.linspace(-0.8, 0.8, N_EXP)\n",
    "A = A_point_lens(u_of_t(t, t0=0.0, u0=0.20, tE=0.25))\n",
    "plt.figure(figsize=(8,3)); plt.plot(t, A, lw=1.6)\n",
    "plt.title('Microlensing magnification A(t)'); plt.xlabel('time [arb]'); plt.ylabel('A')\n",
    "plt.tight_layout(); plt.savefig(NB_OUT/'magnification_curve.png', dpi=150); plt.close()\n",
    "print('Saved magnification_curve.png')\n",
    "\n",
    "wl = np.arange(283)\n",
    "mu = 0.01 + 0.002*np.exp(-0.5*((wl/283-0.30)/0.06)**2) + 0.0015*np.exp(-0.5*((wl/283-0.70)/0.05)**2)\n",
    "W = wl / wl.max(); W = 0.5 + 0.5*(W - W.min())/(W.ptp()+1e-12)\n",
    "phase_per_wl = np.linspace(-0.15, 0.15, len(wl))\n",
    "A_per_wl = np.interp(phase_per_wl, t, A)\n",
    "mu_lensed = mu * (1.0 + (A_per_wl - 1.0)*W)\n",
    "res = mu_lensed - mu\n",
    "fig, ax = plt.subplots(2,1, figsize=(10,6), sharex=True)\n",
    "ax[0].plot(wl, mu, lw=1.4, label='base μ'); ax[0].plot(wl, mu_lensed, lw=1.2, label='effective μ'); ax[0].legend(); ax[0].set_ylabel('μ')\n",
    "ax[1].plot(wl, res, lw=1.2, color='tab:red'); ax[1].axhline(0, color='k', lw=0.7)\n",
    "ax[1].set_xlabel('wavelength index'); ax[1].set_ylabel('Δμ (arb)')\n",
    "fig.suptitle('Effective spectrum under microlensing + wavelength weights')\n",
    "fig.tight_layout(); fig.savefig(NB_OUT/'effective_spectrum_lensing.png', dpi=150); plt.close(fig)\n",
    "print('Saved effective_spectrum_lensing.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4378630c",
   "metadata": {},
   "source": [
    "## 10) Appendix — CLI cheat‑sheet\n",
    "```\n",
    "spectramind test --fast\n",
    "spectramind calibrate --sample 5\n",
    "spectramind train --epochs 1 --fast-dev\n",
    "spectramind submit --dry-run\n",
    "spectramind diagnose --open-html\n",
    "```\n",
    "\n",
    "Artifacts written to `outputs/notebooks/master_walkthrough/`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b3378ea",
   "metadata": {},
   "source": [
    "---\n",
    "# 🔗 Stitched Modules (auto-imported)\n",
    "The following sections were imported from the uploaded notebooks to keep a single master file."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341ad8fc",
   "metadata": {},
   "source": [
    "## 00_quickstart.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9695323",
   "metadata": {},
   "source": [
    "# 🚀 SpectraMind V50 — Quickstart Notebook\n",
    "\n",
    "Fast path to validate your **environment → configs → pipeline** for the **NeurIPS 2025 Ariel Data Challenge**.\n",
    "\n",
    "This notebook mirrors our **CLI-first, Hydra-driven, reproducibility-focused** workflow (Typer CLI + Hydra configs + DVC) so you can smoke-test the stack in minutes without re-implementing any logic.\n",
    "\n",
    "⚠️ **Note:** All steps default to `--fast`, `--dry-run`, or sample subsets. Switch to full runs only after these checks pass.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ff044c",
   "metadata": {},
   "source": [
    "## 0. Runtime Helper — Resolve `spectramind` Launcher\n",
    "\n",
    "Some environments expose the CLI differently. This helper resolves in order:\n",
    "1. `spectramind` (on PATH)\n",
    "2. `poetry run spectramind`\n",
    "3. `python -m spectramind`\n",
    "\n",
    "Provides `sm(cmd)` and `sm_print(cmd)` helpers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8580b59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shlex, shutil, subprocess, sys\n",
    "\n",
    "def _resolve_spectramind_cmd():\n",
    "    if shutil.which(\"spectramind\"):\n",
    "        return [\"spectramind\"]\n",
    "    if shutil.which(\"poetry\"):\n",
    "        try:\n",
    "            out = subprocess.run([\"poetry\", \"run\", \"spectramind\", \"--version\"],\n",
    "                                 stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
    "            if out.returncode == 0:\n",
    "                return [\"poetry\", \"run\", \"spectramind\"]\n",
    "        except Exception:\n",
    "            pass\n",
    "    return [sys.executable, \"-m\", \"spectramind\"]\n",
    "\n",
    "SM = _resolve_spectramind_cmd()\n",
    "print(\"Resolved spectramind launcher:\", \" \".join(shlex.quote(p) for p in SM))\n",
    "\n",
    "def sm(cmd: str, check=False, capture=False):\n",
    "    args = shlex.split(cmd)\n",
    "    res = subprocess.run(SM + args,\n",
    "                         check=check,\n",
    "                         stdout=subprocess.PIPE if capture else None,\n",
    "                         stderr=subprocess.STDOUT if capture else None,\n",
    "                         text=True)\n",
    "    return res\n",
    "\n",
    "def sm_print(cmd: str):\n",
    "    print(\"$\", \" \".join(SM + shlex.split(cmd)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175d05fc",
   "metadata": {},
   "source": [
    "## 1. Repo Sanity Checks\n",
    "Validate core tooling (Python, Poetry, Git, DVC) and GPU snapshot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8231cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python --version\n",
    "!pip --version\n",
    "!poetry --version || echo '⚠️ Poetry not found'\n",
    "!git --version\n",
    "!dvc --version || echo '⚠️ DVC not found'\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    print(\"PyTorch:\", torch.__version__)\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"CUDA ✓ — device:\", torch.cuda.get_device_name(0))\n",
    "    else:\n",
    "        print(\"CUDA not available (CPU)\")\n",
    "except Exception:\n",
    "    print(\"PyTorch not installed (ok for quickstart)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b526303",
   "metadata": {},
   "source": [
    "## 2. CLI Smoke Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3666efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_print(\"--version\")\n",
    "out = sm(\"--version\", capture=True)\n",
    "print(out.stdout)\n",
    "\n",
    "sm_print(\"--help\")\n",
    "out = sm(\"--help\", capture=True)\n",
    "print(\"\\n\".join(out.stdout.splitlines()[:20]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ec4211",
   "metadata": {},
   "source": [
    "## 3. Hydra Config Composition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1790282",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = \"train model=airs_gnn optimizer=adamw training.fast_dev_run=true\"\n",
    "sm_print(cmd)\n",
    "sm(cmd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19437e13",
   "metadata": {},
   "source": [
    "## 4. Mini End-to-End Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd99cc95",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cmd in [\n",
    "    \"test --fast\",\n",
    "    \"calibrate --sample 3 --fast\",\n",
    "    \"train --epochs 1 training.fast_dev_run=true\",\n",
    "    \"diagnose dashboard --no-umap --no-tsne --outdir outputs/diagnostics_quick\",\n",
    "    \"submit --dry-run\"\n",
    "]:\n",
    "    sm_print(cmd)\n",
    "    sm(cmd)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0898dc7c",
   "metadata": {},
   "source": [
    "## 5. Cheat Sheet — Common Workflows\n",
    "```bash\n",
    "spectramind train model=fgs1_mamba optimizer=adamw training.epochs=50\n",
    "spectramind calibrate --sample 10\n",
    "spectramind submit --config configs/config_v50.yaml\n",
    "```\n",
    "Logs land in `logs/v50_debug_log.md`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f5aa50",
   "metadata": {},
   "source": [
    "## 6. DVC Data Tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d394a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "set -euo pipefail\n",
    "if command -v dvc >/dev/null 2>&1; then\n",
    "  echo \"DVC detected — pulling latest data...\"\n",
    "  dvc pull || echo '⚠️ non-fatal'\n",
    "  echo \"Optionally recompute: dvc repro\"\n",
    "else\n",
    "  echo \"DVC not installed — skipping\"\n",
    "fi\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "486a6628",
   "metadata": {},
   "source": [
    "## 7. Python Helper — Run CLI Inside Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edf16a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = sm(\"--version\", capture=True)\n",
    "print(res.stdout)\n",
    "\n",
    "res = sm(\"train training.fast_dev_run=true\", capture=True)\n",
    "for line in (res.stdout or \"\").splitlines():\n",
    "    if \"loss\" in line.lower() or \"gll\" in line.lower():\n",
    "        print(line)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b84216",
   "metadata": {},
   "source": [
    "## 8. Where to Look Afterwards\n",
    "- Outputs: `outputs/`\n",
    "- Logs: `logs/v50_debug_log.md`\n",
    "- Diagnostics HTML: from `diagnose dashboard`\n",
    "- Configs: `configs/`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b607781d",
   "metadata": {},
   "source": [
    "## 01_pipeline_calibrate_train_predict.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db95f98",
   "metadata": {},
   "source": [
    "# 🧪 SpectraMind V50 — 01_pipeline_calibrate_train_predict\n",
    "\n",
    "Tiny **calibrate → train → predict** pipeline to sanity-check the stack with fast settings.\n",
    "\n",
    "This runs entirely via the **CLI** (Typer + Hydra), keeping the workflow reproducible and config-driven."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39cac99f",
   "metadata": {},
   "source": [
    "## 0) Runtime Helper — Resolve `spectramind`\n",
    "\n",
    "Resolves in order:\n",
    "1. `spectramind` (PATH)\n",
    "2. `poetry run spectramind`\n",
    "3. `python -m spectramind`\n",
    "\n",
    "Exposes helpers: `sm(cmd)`, `sm_print(cmd)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784f42cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shlex, shutil, subprocess, sys, glob, json, textwrap\n",
    "from pathlib import Path\n",
    "\n",
    "def _resolve_spectramind_cmd():\n",
    "    if shutil.which(\"spectramind\"):\n",
    "        return [\"spectramind\"]\n",
    "    if shutil.which(\"poetry\"):\n",
    "        try:\n",
    "            out = subprocess.run([\"poetry\", \"run\", \"spectramind\", \"--version\"],\n",
    "                                 stdout=subprocess.PIPE, stderr=subprocess.STDOUT, text=True)\n",
    "            if out.returncode == 0:\n",
    "                return [\"poetry\", \"run\", \"spectramind\"]\n",
    "        except Exception:\n",
    "            pass\n",
    "    return [sys.executable, \"-m\", \"spectramind\"]\n",
    "\n",
    "SM = _resolve_spectramind_cmd()\n",
    "print(\"Resolved spectramind launcher:\", \" \".join(shlex.quote(p) for p in SM))\n",
    "\n",
    "def sm(cmd: str, check=False, capture=False):\n",
    "    args = shlex.split(cmd)\n",
    "    res = subprocess.run(SM + args,\n",
    "                         check=check,\n",
    "                         stdout=subprocess.PIPE if capture else None,\n",
    "                         stderr=subprocess.STDOUT if capture else None,\n",
    "                         text=True)\n",
    "    return res\n",
    "\n",
    "def sm_print(cmd: str):\n",
    "    print(\"$\", \" \".join(SM + shlex.split(cmd)))\n",
    "\n",
    "Path(\"outputs\").mkdir(exist_ok=True, parents=True)\n",
    "Path(\"outputs/preds_quick\").mkdir(exist_ok=True, parents=True)\n",
    "Path(\"outputs/diagnostics_quick\").mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a50fc55",
   "metadata": {},
   "source": [
    "## 1) Sanity Checks\n",
    "Python/Poetry/Git/DVC and optional CUDA snapshot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4fd5fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python --version\n",
    "!pip --version\n",
    "!poetry --version || echo '⚠️ Poetry not found (ok if not using Poetry)'\n",
    "!git --version\n",
    "!dvc --version || echo '⚠️ DVC not found (ok for quick run)'\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "    print(\"PyTorch:\", torch.__version__)\n",
    "    print(\"CUDA available:\", torch.cuda.is_available())\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"CUDA device:\", torch.cuda.get_device_name(0))\n",
    "except Exception:\n",
    "    print(\"PyTorch not installed — continuing\")\n",
    "\n",
    "sm_print(\"--version\")\n",
    "out = sm(\"--version\", capture=True)\n",
    "print(out.stdout if out.stdout else \"(no output)\")\n",
    "\n",
    "sm_print(\"--help\")\n",
    "out = sm(\"--help\", capture=True)\n",
    "print(\"\\n\".join(out.stdout.splitlines()[:20]) if out.stdout else \"(no output)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "419b28d3",
   "metadata": {},
   "source": [
    "## 2) (Optional) DVC Pull\n",
    "If your repo uses DVC for data/artifacts, pull latest (non-fatal if absent)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f019191d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "set -euo pipefail\n",
    "if command -v dvc >/dev/null 2>&1; then\n",
    "  echo \"DVC detected — pulling data (if remote configured)...\"\n",
    "  dvc pull || echo '⚠️ dvc pull non-fatal'\n",
    "else\n",
    "  echo \"DVC not installed — skipping\"\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abea5bae",
   "metadata": {},
   "source": [
    "## 3) Calibrate (fast, sampled)\n",
    "Run calibration on a small sample for speed. Adjust `--sample` as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91eef3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "sm_print(\"calibrate --sample 5 --fast\")\n",
    "cal = sm(\"calibrate --sample 5 --fast\", capture=True)\n",
    "print(cal.stdout or \"(no output)\")\n",
    "\n",
    "# List key outputs if your pipeline writes calibrated artifacts\n",
    "!ls -lah outputs || true\n",
    "!ls -lah outputs/* || true 2>/dev/null || true\n",
    "!find outputs -maxdepth 2 -type f | head -n 20 || true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1fde18",
   "metadata": {},
   "source": [
    "## 4) Train (tiny)\n",
    "One quick epoch / fast-dev-run for smoke validation. Override Hydra config inline if desired."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631091cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_cmd = \"train --epochs 1 training.fast_dev_run=true\"\n",
    "sm_print(train_cmd)\n",
    "tr = sm(train_cmd, capture=True)\n",
    "print(tr.stdout or \"(no output)\")\n",
    "\n",
    "# Peek at any saved checkpoints/metrics\n",
    "!find outputs -maxdepth 3 -type f \\( -name \"*.pt\" -o -name \"*.ckpt\" -o -name \"metrics*.json\" -o -name \"*log*.json\" \\) | head -n 20 || true\n",
    "!tail -n 50 logs/v50_debug_log.md 2>/dev/null || true\n",
    "!tail -n 50 outputs/metrics.json 2>/dev/null || true\n",
    "!tail -n 50 outputs/train/metrics.json 2>/dev/null || true\n",
    "!find outputs -maxdepth 3 -type f -name \"*.json\" | head -n 10 || true\n",
    "!find outputs -maxdepth 3 -type f -name \"*.yaml\" | head -n 10 || true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4431551",
   "metadata": {},
   "source": [
    "## 5) Predict\n",
    "Generate quick predictions (location may vary by your CLI). This writes predictions to `outputs/preds_quick/`.\n",
    "\n",
    "- If your CLI exposes `predict`, use that.\n",
    "- If predictions are created by `submit --dry-run`, run that and parse the produced CSV/ZIP accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7558d1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dir = Path(\"outputs/preds_quick\")\n",
    "\n",
    "# Try `predict` first\n",
    "predict_cmds = [\n",
    "    \"predict --outdir outputs/preds_quick --fast\",\n",
    "    # Fallback: some repos produce predictions as part of submit dry-run\n",
    "    \"submit --dry-run\"\n",
    "]\n",
    "\n",
    "ok = False\n",
    "for cmd in predict_cmds:\n",
    "    sm_print(cmd)\n",
    "    out = sm(cmd, capture=True)\n",
    "    print(out.stdout or \"(no output)\")\n",
    "    # Heuristic: check if something landed in preds_quick or a submission file exists\n",
    "    csvs = list(pred_dir.glob(\"**/*.csv\"))\n",
    "    subs = list(Path(\".\").glob(\"**/submission*.csv\"))\n",
    "    if csvs or subs:\n",
    "        ok = True\n",
    "        break\n",
    "\n",
    "if not ok:\n",
    "    print(\"⚠️ Could not detect predictions in expected locations. Inspect CLI outputs above.\")\n",
    "\n",
    "print(\"\\nDiscovered prediction files:\")\n",
    "for p in pred_dir.glob(\"**/*.csv\"):\n",
    "    print(\"-\", p)\n",
    "for p in Path(\".\").glob(\"**/submission*.csv\"):\n",
    "    print(\"-\", p)\n",
    "\n",
    "!ls -lah outputs/preds_quick 2>/dev/null || true\n",
    "!find outputs -maxdepth 2 -type f -name \"*.csv\" | head -n 20 || true\n",
    "!find . -maxdepth 3 -type f -name \"submission*.csv\" | head -n 10 || true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8922ac",
   "metadata": {},
   "source": [
    "## 6) Inspect Predictions (preview & quick plot)\n",
    "This section attempts to open a CSV of predictions and visualize a spectrum for a quick sanity check (optional)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa0772d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def _find_pred_csv():\n",
    "    # prefer explicit preds directory, else any submission*.csv\n",
    "    cands = list(Path(\"outputs/preds_quick\").glob(\"**/*.csv\"))\n",
    "    if cands:\n",
    "        return cands[0]\n",
    "    cands = list(Path(\".\").glob(\"**/submission*.csv\"))\n",
    "    return cands[0] if cands else None\n",
    "\n",
    "csv_path = _find_pred_csv()\n",
    "if csv_path and csv_path.is_file():\n",
    "    print(\"Preview:\", csv_path)\n",
    "    df = pd.read_csv(csv_path)\n",
    "    display(df.head(10))\n",
    "    # Try naive plot: look for columns that look like wavelength bins\n",
    "    num_cols = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]\n",
    "    # If a single row contains a full spectrum across columns, plot the first row\n",
    "    if len(num_cols) > 10:\n",
    "        y = df[num_cols].iloc[0].values\n",
    "        x = list(range(len(y)))\n",
    "        plt.figure(figsize=(8,3))\n",
    "        plt.plot(x, y, lw=1)\n",
    "        plt.title(f\"Quick Spectrum Preview — {csv_path.name}\")\n",
    "        plt.xlabel(\"Bin index\")\n",
    "        plt.ylabel(\"Predicted μ\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"(Skipping plot — could not infer wide spectrum columns)\")\n",
    "else:\n",
    "    print(\"No prediction CSV found to preview.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b367d0d6",
   "metadata": {},
   "source": [
    "## 7) (Optional) Diagnostics Snapshot\n",
    "Run a minimal dashboard build to confirm plots render (UMAP/t-SNE disabled for speed). Outputs in `outputs/diagnostics_quick/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9613eeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "diag_cmd = \"diagnose dashboard --no-umap --no-tsne --outdir outputs/diagnostics_quick\"\n",
    "sm_print(diag_cmd)\n",
    "dg = sm(diag_cmd, capture=True)\n",
    "print(dg.stdout or \"(no output)\")\n",
    "\n",
    "!find outputs/diagnostics_quick -maxdepth 2 -type f | head -n 20 || true\n",
    "!ls -lah outputs/diagnostics_quick 2>/dev/null || true\n",
    "!find outputs -maxdepth 3 -type f -name \"*report*.html\" | head -n 10 || true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "062a5c90",
   "metadata": {},
   "source": [
    "## 8) Summary & Next Steps\n",
    "- You now have calibrated data, a quick-trained model, and a prediction artifact.\n",
    "- For real experiments, increase `--sample`, remove `--fast_dev_run`, and raise `--epochs`.\n",
    "- Consider running `submit` without `--dry-run` to package a full submission when ready.\n",
    "- Explore diagnostics HTML under `outputs/diagnostics_quick/`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db3cd736",
   "metadata": {},
   "source": [
    "## 02_diagnostics_explainability.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2878799a",
   "metadata": {},
   "source": [
    "# 🧭 SpectraMind V50 — 02 · Diagnostics & Explainability\n",
    "\n",
    "**NeurIPS 2025 Ariel Data Challenge** · *NASA‑grade reproducibility w/ Hydra + DVC + CLI*\n",
    "\n",
    "This notebook focuses on diagnostics and explainability artifacts produced by the SpectraMind V50 pipeline.\n",
    "It is *CLI‑first*, meaning: wherever possible, we **call the `spectramind` CLI** to generate artifacts,\n",
    "then **render them here**. Each section also includes a *graceful fallback* path that reads existing files\n",
    "(e.g., `diagnostic_summary.json`, `*.html`, `*.csv`, `*.npy`) directly if present.\n",
    "\n",
    "> Golden rule: **No hidden analytics**. The GUI/notebook **reflects** what the CLI generated to keep faithful,\n",
    "> reproducible results. Cells here log their actions and record run metadata for later auditing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "388d3f1d",
   "metadata": {},
   "source": [
    "## ✅ What you can do here\n",
    "\n",
    "1. Run *lightweight* CLI diagnostics (UMAP/t‑SNE, SHAP overlays, symbolic rule analysis, FFT/smoothness, calibration).\n",
    "2. Render previously generated HTML dashboards and plots.\n",
    "3. Inspect metrics from `diagnostic_summary.json` across planets and configs.\n",
    "4. Export a refreshed diagnostics dashboard and append metadata into `v50_debug_log.md`.\n",
    "\n",
    "> Tip: Start with the **Environment & Paths** cell below, then try **Quick CLI sanity** to verify your setup.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97ab6a9",
   "metadata": {},
   "source": [
    "## 🔧 Environment & Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d8c19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell sets up common paths and logs environment context.\n",
    "# Adjust ROOT to your repository root if needed.\n",
    "from pathlib import Path\n",
    "import json, os, sys, platform, shutil, textwrap\n",
    "from datetime import datetime\n",
    "\n",
    "# ---- Repository root deduction: prefer CWD; allow manual override ----\n",
    "ROOT = Path.cwd()\n",
    "ARTEFACTS = ROOT / \"artifacts\"\n",
    "DIAG_DIR = ARTEFACTS / \"diagnostics\"\n",
    "HTML_DIR = DIAG_DIR / \"html\"\n",
    "PLOTS_DIR = DIAG_DIR / \"plots\"\n",
    "\n",
    "# Common artifact inputs (created by CLI)\n",
    "DIAG_SUMMARY = DIAG_DIR / \"diagnostic_summary.json\"\n",
    "SHAP_JSON = DIAG_DIR / \"shap_symbolic_fusion_topk_bins.json\"\n",
    "SYMB_JSON = DIAG_DIR / \"symbolic_violation_summary.json\"\n",
    "COREL_JSON = DIAG_DIR / \"corel_calibration_summary.json\"\n",
    "UMAP_HTML = HTML_DIR / \"umap_v50.html\"\n",
    "TSNE_HTML = HTML_DIR / \"tsne_interactive.html\"\n",
    "DASHBOARD_HTML = HTML_DIR / \"diagnostic_report_v1.html\"\n",
    "LOG_MD = ROOT / \"v50_debug_log.md\"\n",
    "\n",
    "# Optional latent/label fallbacks\n",
    "LATENTS_NPY = DIAG_DIR / \"latents.npy\"\n",
    "LATENTS_CSV = DIAG_DIR / \"latents.csv\"\n",
    "LABELS_CSV = DIAG_DIR / \"labels.csv\"\n",
    "\n",
    "# Create folders if missing (no-op if existing)\n",
    "for d in [ARTEFACTS, DIAG_DIR, HTML_DIR, PLOTS_DIR]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "env = {\n",
    "    \"timestamp\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "    \"python\": sys.version.replace(\"\\n\", \" \"),\n",
    "    \"platform\": platform.platform(),\n",
    "    \"cwd\": str(ROOT),\n",
    "    \"paths\": {\n",
    "        \"ARTEFACTS\": str(ARTEFACTS),\n",
    "        \"DIAG_DIR\": str(DIAG_DIR),\n",
    "        \"HTML_DIR\": str(HTML_DIR),\n",
    "        \"PLOTS_DIR\": str(PLOTS_DIR),\n",
    "        \"DIAG_SUMMARY\": str(DIAG_SUMMARY),\n",
    "        \"SHAP_JSON\": str(SHAP_JSON),\n",
    "        \"SYMB_JSON\": str(SYMB_JSON),\n",
    "        \"COREL_JSON\": str(COREL_JSON),\n",
    "        \"UMAP_HTML\": str(UMAP_HTML),\n",
    "        \"TSNE_HTML\": str(TSNE_HTML),\n",
    "        \"DASHBOARD_HTML\": str(DASHBOARD_HTML),\n",
    "        \"LOG_MD\": str(LOG_MD),\n",
    "        \"LATENTS_NPY\": str(LATENTS_NPY),\n",
    "        \"LATENTS_CSV\": str(LATENTS_CSV),\n",
    "        \"LABELS_CSV\": str(LABELS_CSV),\n",
    "    },\n",
    "}\n",
    "print(json.dumps(env, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7313799b",
   "metadata": {},
   "source": [
    "## 🩺 Quick CLI sanity (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6d21ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell *optionally* checks CLI availability. It's safe to skip if your CLI isn't on PATH.\n",
    "# In many notebook runtimes, subprocess may not find your local CLI; that's okay.\n",
    "import shutil, subprocess\n",
    "\n",
    "def check_cli(cmd=\"spectramind\", args=[\"--version\"]):\n",
    "    exe = shutil.which(cmd)\n",
    "    if not exe:\n",
    "        print(\"⚠️ 'spectramind' CLI not found on PATH. Skipping CLI sanity check.\")\n",
    "        return {\"available\": False}\n",
    "    try:\n",
    "        out = subprocess.check_output([cmd] + args, stderr=subprocess.STDOUT, text=True, timeout=30)\n",
    "        print(out)\n",
    "        return {\"available\": True, \"output\": out}\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ CLI call failed: {e}\")\n",
    "        return {\"available\": True, \"error\": str(e)}\n",
    "\n",
    "cli_info = check_cli()\n",
    "cli_info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7298b5ba",
   "metadata": {},
   "source": [
    "## 🌈 UMAP Diagnostics — generate or load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab306dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt to call the CLI to generate UMAP HTML; otherwise load existing HTML if present.\n",
    "import subprocess, webbrowser\n",
    "\n",
    "def run_umap_cli():\n",
    "    try:\n",
    "        exe = shutil.which(\"spectramind\")\n",
    "        if not exe:\n",
    "            return False, \"CLI not found\"\n",
    "        # Minimal example — adjust flags to your configs\n",
    "        cmd = [\"spectramind\", \"diagnose\", \"umap\",\n",
    "               \"--html-out\", str(UMAP_HTML),\n",
    "               \"--log-file\", str(LOG_MD),\n",
    "               \"--open-browser\", \"false\"]\n",
    "        print(\"Running:\", \" \".join(cmd))\n",
    "        subprocess.check_call(cmd, timeout=600)\n",
    "        return True, \"OK\"\n",
    "    except Exception as e:\n",
    "        return False, str(e)\n",
    "\n",
    "ok, msg = run_umap_cli()\n",
    "if ok:\n",
    "    print(f\"✅ UMAP generated at: {UMAP_HTML}\")\n",
    "elif UMAP_HTML.exists():\n",
    "    print(\"⚠️ CLI skipped/failed; using existing:\", UMAP_HTML)\n",
    "else:\n",
    "    print(\"❌ UMAP not available; create latents or run CLI separately.\")\n",
    "\n",
    "# Inline display hint (cannot iframe automatically in all notebook environments)\n",
    "print(\"To open UMAP HTML manually if needed:\", str(UMAP_HTML))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb8a8576",
   "metadata": {},
   "source": [
    "## 🌀 t‑SNE Diagnostics — generate or load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce55a41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attempt to call CLI t‑SNE; otherwise load existing HTML if present.\n",
    "def run_tsne_cli():\n",
    "    try:\n",
    "        exe = shutil.which(\"spectramind\")\n",
    "        if not exe:\n",
    "            return False, \"CLI not found\"\n",
    "        cmd = [\"spectramind\", \"diagnose\", \"tsne-latents\",\n",
    "               \"--html-out\", str(TSNE_HTML),\n",
    "               \"--log-file\", str(LOG_MD),\n",
    "               \"--open-browser\", \"false\"]\n",
    "        print(\"Running:\", \" \".join(cmd))\n",
    "        subprocess.check_call(cmd, timeout=600)\n",
    "        return True, \"OK\"\n",
    "    except Exception as e:\n",
    "        return False, str(e)\n",
    "\n",
    "ok, msg = run_tsne_cli()\n",
    "if ok:\n",
    "    print(f\"✅ t‑SNE generated at: {TSNE_HTML}\")\n",
    "elif TSNE_HTML.exists():\n",
    "    print(\"⚠️ CLI skipped/failed; using existing:\", TSNE_HTML)\n",
    "else:\n",
    "    print(\"❌ t‑SNE not available; ensure latents exist or run CLI separately.\")\n",
    "\n",
    "print(\"To open t‑SNE HTML manually if needed:\", str(TSNE_HTML))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbcc7a9",
   "metadata": {},
   "source": [
    "### 🔁 Fallback: quickscatter for latents (matplotlib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca4ed76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If HTMLs are not present, try to render a simple 2D scatter from CSV/NPY latents.\n",
    "# Matplotlib only (no seaborn, and a single plot per chart per project constraints).\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_matrix(path):\n",
    "    if path.suffix == \".csv\":\n",
    "        import pandas as pd\n",
    "        return pd.read_csv(path, index_col=None).values\n",
    "    elif path.suffix == \".npy\":\n",
    "        return np.load(path)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported format for {path}\")\n",
    "\n",
    "latent_matrix = None\n",
    "for p in [LATENTS_CSV, LATENTS_NPY]:\n",
    "    if p.exists():\n",
    "        try:\n",
    "            latent_matrix = load_matrix(p)\n",
    "            print(f\"Loaded latents from {p} with shape {latent_matrix.shape}\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to load {p}: {e}\")\n",
    "\n",
    "if latent_matrix is not None:\n",
    "    # Use first 2 columns as a crude projection\n",
    "    x = latent_matrix[:, 0]\n",
    "    y = latent_matrix[:, 1] if latent_matrix.shape[1] > 1 else np.zeros_like(x)\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.scatter(x, y, s=10, alpha=0.7)\n",
    "    plt.title(\"Latent Quickscatter (fallback)\")\n",
    "    plt.xlabel(\"Dim 1\")\n",
    "    plt.ylabel(\"Dim 2\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No latents found for fallback quickscatter.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8888d158",
   "metadata": {},
   "source": [
    "## 🔍 SHAP × Symbolic Overlay Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038ddc66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SHAP/symbolic overlay JSON if present; show top-K bins per planet or aggregate stats.\n",
    "import json\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "def safe_load_json(path):\n",
    "    if not path.exists():\n",
    "        return None\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            return json.load(f)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to parse {path}: {e}\")\n",
    "        return None\n",
    "\n",
    "overlay = safe_load_json(SHAP_JSON)\n",
    "if overlay is None:\n",
    "    print(f\"⚠️ No overlay JSON at {SHAP_JSON}. Generate via CLI or scripts first.\")\n",
    "else:\n",
    "    # Expecting schema like: { \"planets\": { \"planet_id\": { \"top_bins\": [...], \"scores\": {...} } }, ...}\n",
    "    planets = overlay.get(\"planets\") or overlay  # tolerate flat schema\n",
    "    print(f\"Loaded overlay for {len(planets)} planets.\")\n",
    "    # Simple aggregate: most common bins across planets (if present)\n",
    "    bin_counts = Counter()\n",
    "    for pid, rec in planets.items():\n",
    "        top_bins = rec.get(\"top_bins\") or []\n",
    "        bin_counts.update(top_bins)\n",
    "    most_common = bin_counts.most_common(10)\n",
    "    print(\"Top 10 recurrent bins across planets:\", most_common)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35774b5",
   "metadata": {},
   "source": [
    "## 🧩 Symbolic Rule Violations — summary view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ddd2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect symbolic violation summaries to find dominant rules and hotspots.\n",
    "symb = safe_load_json(SYMB_JSON)\n",
    "if symb is None:\n",
    "    print(f\"⚠️ No symbolic violation summary at {SYMB_JSON}.\")\n",
    "else:\n",
    "    # Tolerate either list or dict formats\n",
    "    if isinstance(symb, dict) and \"rules\" in symb:\n",
    "        rules = symb[\"rules\"]\n",
    "    elif isinstance(symb, list):\n",
    "        rules = symb\n",
    "    else:\n",
    "        rules = symb\n",
    "\n",
    "    print(\"Symbolic rules summary keys:\", list(rules)[:10] if isinstance(rules, dict) else \"list\")\n",
    "    # Very simple aggregate if dict: sort rules by mean violation\n",
    "    if isinstance(rules, dict):\n",
    "        agg = []\n",
    "        for rname, vals in rules.items():\n",
    "            v = vals if isinstance(vals, (int, float)) else vals.get(\"mean\", None) if isinstance(vals, dict) else None\n",
    "            if v is not None:\n",
    "                agg.append((rname, float(v)))\n",
    "        agg.sort(key=lambda x: x[1], reverse=True)\n",
    "        print(\"Top 10 rules by mean violation:\")\n",
    "        for r, v in agg[:10]:\n",
    "            print(f\"  {r:40s}  {v:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655a4fce",
   "metadata": {},
   "source": [
    "## 📈 FFT & Smoothness — quick checks on μ spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c9120c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a lightweight smoothness/FFT visualization from diagnostic_summary.json if available.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "diag = safe_load_json(DIAG_SUMMARY)\n",
    "if not diag:\n",
    "    print(f\"⚠️ No diagnostic summary at {DIAG_SUMMARY}. Generate via CLI or scripts first.\")\n",
    "else:\n",
    "    # Heuristic: look for one planet entry with \"mu\" or \"spectrum\" field\n",
    "    # and draw its FFT magnitude and finite-difference smoothness.\n",
    "    # The exact schema varies; we attempt to be permissive.\n",
    "    candidates = []\n",
    "    if isinstance(diag, dict):\n",
    "        # Possible structures: {\"planets\": {...}}, or {\"items\": [...]}, or direct\n",
    "        root = diag.get(\"planets\") or diag.get(\"items\") or diag\n",
    "        if isinstance(root, dict):\n",
    "            candidates = list(root.values())\n",
    "        elif isinstance(root, list):\n",
    "            candidates = root\n",
    "    if not candidates:\n",
    "        print(\"Could not find planet entries in diagnostic summary.\")\n",
    "    else:\n",
    "        # Find first record with a 1D mu-like array\n",
    "        mu = None\n",
    "        for rec in candidates:\n",
    "            arr = rec.get(\"mu\") or rec.get(\"spectrum\") or rec.get(\"mu_mean\")\n",
    "            if isinstance(arr, list) and len(arr) >= 16:\n",
    "                mu = np.array(arr, dtype=float)\n",
    "                break\n",
    "        if mu is None:\n",
    "            print(\"No suitable μ array found in diagnostic summary.\")\n",
    "        else:\n",
    "            # FFT magnitude\n",
    "            fft_mag = np.abs(np.fft.rfft(mu))\n",
    "            plt.figure(figsize=(6,4))\n",
    "            plt.plot(fft_mag)\n",
    "            plt.title(\"FFT magnitude of μ (example planet)\")\n",
    "            plt.xlabel(\"Frequency bin\")\n",
    "            plt.ylabel(\"|FFT|\")\n",
    "            plt.show()\n",
    "\n",
    "            # Smoothness (finite differences)\n",
    "            grad = np.diff(mu)\n",
    "            curv = np.diff(mu, n=2)\n",
    "            plt.figure(figsize=(6,4))\n",
    "            plt.plot(np.abs(grad), label=\"|∂μ|\")\n",
    "            plt.plot(np.abs(curv), label=\"|∂²μ|\")\n",
    "            plt.title(\"Smoothness diagnostics of μ (example planet)\")\n",
    "            plt.xlabel(\"Spectral bin\")\n",
    "            plt.ylabel(\"Magnitude\")\n",
    "            plt.legend()\n",
    "            plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afbff4f",
   "metadata": {},
   "source": [
    "## 🎯 Calibration (σ) — COREL overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd98f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load calibration summary if available and show simple histograms.\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "corel = safe_load_json(COREL_JSON)\n",
    "if corel is None:\n",
    "    print(f\"⚠️ No COREL calibration summary at {COREL_JSON}.\")\n",
    "else:\n",
    "    # Heuristic: expect per-bin coverage or residuals arrays\n",
    "    cov = corel.get(\"coverage\") or corel.get(\"per_bin_coverage\")\n",
    "    res = corel.get(\"residuals\") or corel.get(\"per_bin_residual\")\n",
    "    if isinstance(cov, list) and len(cov) > 0:\n",
    "        cov = np.array(cov, dtype=float)\n",
    "        plt.figure(figsize=(6,4))\n",
    "        plt.hist(cov, bins=30, alpha=0.9)\n",
    "        plt.title(\"Coverage histogram (COREL)\")\n",
    "        plt.xlabel(\"Coverage\")\n",
    "        plt.ylabel(\"Count\")\n",
    "        plt.show()\n",
    "    if isinstance(res, list) and len(res) > 0:\n",
    "        res = np.array(res, dtype=float)\n",
    "        plt.figure(figsize=(6,4))\n",
    "        plt.hist(res, bins=30, alpha=0.9)\n",
    "        plt.title(\"Residual histogram (COREL)\")\n",
    "        plt.xlabel(\"Residual\")\n",
    "        plt.ylabel(\"Count\")\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce95f0d",
   "metadata": {},
   "source": [
    "## 🧪 Build Diagnostics Dashboard (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7ea214b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the CLI to generate the full HTML diagnostics dashboard, or load an existing one.\n",
    "def run_dashboard_cli():\n",
    "    try:\n",
    "        exe = shutil.which(\"spectramind\")\n",
    "        if not exe:\n",
    "            return False, \"CLI not found\"\n",
    "        cmd = [\"spectramind\", \"diagnose\", \"dashboard\",\n",
    "               \"--html-out\", str(DASHBOARD_HTML),\n",
    "               \"--log-file\", str(LOG_MD),\n",
    "               \"--open-browser\", \"false\"]\n",
    "        print(\"Running:\", \" \".join(cmd))\n",
    "        subprocess.check_call(cmd, timeout=1200)\n",
    "        return True, \"OK\"\n",
    "    except Exception as e:\n",
    "        return False, str(e)\n",
    "\n",
    "ok, msg = run_dashboard_cli()\n",
    "if ok:\n",
    "    print(f\"✅ Diagnostics dashboard generated at: {DASHBOARD_HTML}\")\n",
    "elif DASHBOARD_HTML.exists():\n",
    "    print(\"⚠️ CLI skipped/failed; using existing:\", DASHBOARD_HTML)\n",
    "else:\n",
    "    print(\"❌ Dashboard not available. Generate via CLI when ready.\")\n",
    "\n",
    "print(\"To open manually if needed:\", str(DASHBOARD_HTML))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c565a96",
   "metadata": {},
   "source": [
    "## 🧾 Append run metadata to `v50_debug_log.md`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae3ff13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append a structured entry to v50_debug_log.md for auditability.\n",
    "from datetime import datetime\n",
    "entry = f\"\"\"### Notebook: 02_diagnostics_explainability.ipynb\n",
    "- timestamp: {datetime.now().isoformat(timespec=\"seconds\")}\n",
    "- cwd: {ROOT}\n",
    "- python: {platform.python_version()}\n",
    "- actions:\n",
    "  - env_init\n",
    "  - umap_try_cli: {'exists' if UMAP_HTML.exists() else 'missing'}\n",
    "  - tsne_try_cli: {'exists' if TSNE_HTML.exists() else 'missing'}\n",
    "  - shap_overlay_loaded: {SHAP_JSON.exists()}\n",
    "  - symbolic_summary_loaded: {SYMB_JSON.exists()}\n",
    "  - corel_summary_loaded: {COREL_JSON.exists()}\n",
    "  - dashboard: {'exists' if DASHBOARD_HTML.exists() else 'missing'}\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    with open(LOG_MD, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(entry + \"\\n\")\n",
    "    print(f\"Appended notebook log entry to {LOG_MD}\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Could not append to {LOG_MD}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25db1e1",
   "metadata": {},
   "source": [
    "## 📚 References & Next Steps\n",
    "\n",
    "- `spectramind diagnose umap` · Generate UMAP HTML with symbolic overlays and links.\n",
    "- `spectramind diagnose tsne-latents` · Interactive t‑SNE with confidence/links.\n",
    "- `spectramind diagnose smoothness` · Produce smoothness maps and CSV summaries.\n",
    "- `spectramind diagnose dashboard` · Unified HTML diagnostics dashboard.\n",
    "\n",
    "**Pro tip:** Pair this notebook with `00_quickstart.ipynb` and `03_ablation_and_tuning.ipynb` for the full pipeline flow.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c3677d",
   "metadata": {},
   "source": [
    "## 03_ablation_and_tuning.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be9df8ff",
   "metadata": {},
   "source": [
    "# 🧪 SpectraMind V50 — 03 · Ablation & Tuning\n",
    "\n",
    "**NeurIPS 2025 Ariel Data Challenge** · *CLI‑first, Hydra‑safe, reproducible*\n",
    "\n",
    "This notebook drives **systematic ablations and hyperparameter tuning** over the SpectraMind V50 stack.\n",
    "It favors the **`spectramind` CLI** (Typer) for all heavy‑lifting, and **reads artifacts** to visualize\n",
    "and compare results. Every step is **deterministic where possible** and produces evidence (logs/JSON/HTML).\n",
    "\n",
    "> Golden rule: the notebook mirrors what you'd do from the CLI and never hides transformations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e261e6e2",
   "metadata": {},
   "source": [
    "## 🎯 Objectives\n",
    "\n",
    "- Run or resume **symbolic‑aware ablations** (smoothness, symbolic weights, entropy penalties, COREL/σ, etc.).\n",
    "- Inspect and plot **GLL, RMSE/MAE, calibration, violation scores** from ablation artifacts.\n",
    "- Generate **Markdown + HTML leaderboards**, and optionally a **Top‑N ZIP** bundle.\n",
    "- Append run metadata to `v50_debug_log.md` for auditability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68344e22",
   "metadata": {},
   "source": [
    "## 🔧 Environment & Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94988e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json, os, sys, platform\n",
    "from datetime import datetime\n",
    "\n",
    "ROOT = Path.cwd()\n",
    "ART = ROOT / \"artifacts\"\n",
    "ABLATE_DIR = ART / \"ablation\"\n",
    "ABLATE_RUNS = ABLATE_DIR / \"runs\"\n",
    "ABLATE_SUMMARY_JSON = ABLATE_DIR / \"leaderboard.json\"\n",
    "ABLATE_SUMMARY_CSV  = ABLATE_DIR / \"leaderboard.csv\"\n",
    "ABLATE_SUMMARY_MD   = ABLATE_DIR / \"leaderboard.md\"\n",
    "ABLATE_SUMMARY_HTML = ABLATE_DIR / \"leaderboard.html\"\n",
    "TOPN_ZIP            = ABLATE_DIR / \"topN_bundle.zip\"\n",
    "LOG_MD              = ROOT / \"v50_debug_log.md\"\n",
    "\n",
    "for d in [ART, ABLATE_DIR, ABLATE_RUNS]:\n",
    "    d.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "env = {\n",
    "    \"timestamp\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "    \"python\": sys.version.replace(\"\\n\", \" \"),\n",
    "    \"platform\": platform.platform(),\n",
    "    \"cwd\": str(ROOT),\n",
    "    \"paths\": {\n",
    "        \"ART\": str(ART),\n",
    "        \"ABLATE_DIR\": str(ABLATE_DIR),\n",
    "        \"ABLATE_RUNS\": str(ABLATE_RUNS),\n",
    "        \"ABLATE_SUMMARY_JSON\": str(ABLATE_SUMMARY_JSON),\n",
    "        \"ABLATE_SUMMARY_CSV\": str(ABLATE_SUMMARY_CSV),\n",
    "        \"ABLATE_SUMMARY_MD\": str(ABLATE_SUMMARY_MD),\n",
    "        \"ABLATE_SUMMARY_HTML\": str(ABLATE_SUMMARY_HTML),\n",
    "        \"TOPN_ZIP\": str(TOPN_ZIP),\n",
    "        \"LOG_MD\": str(LOG_MD),\n",
    "    }\n",
    "}\n",
    "print(json.dumps(env, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2318be",
   "metadata": {},
   "source": [
    "## 🩺 Quick CLI sanity (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f1d521",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil, subprocess\n",
    "\n",
    "def check_cli(cmd=\"spectramind\", args=[\"--version\"]):\n",
    "    exe = shutil.which(cmd)\n",
    "    if not exe:\n",
    "        print(\"⚠️ 'spectramind' CLI not found on PATH. Skipping CLI sanity check.\")\n",
    "        return {\"available\": False}\n",
    "    try:\n",
    "        out = subprocess.check_output([cmd] + args, stderr=subprocess.STDOUT, text=True, timeout=30)\n",
    "        print(out)\n",
    "        return {\"available\": True, \"output\": out}\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ CLI call failed: {e}\")\n",
    "        return {\"available\": True, \"error\": str(e)}\n",
    "\n",
    "cli_info = check_cli()\n",
    "cli_info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67115b38",
   "metadata": {},
   "source": [
    "## 🧰 Define Ablation Grids\n",
    "\n",
    "Below are **example grids** focused on symbolic and physics knobs. Tune to match your configs.\n",
    "\n",
    "- `loss.symbolic.smoothness.lambda`: smoothness penalty weight\n",
    "- `loss.symbolic.asymmetry.lambda`: asymmetry penalty weight\n",
    "- `loss.symbolic.nonneg.lambda`: non‑negativity hinge\n",
    "- `training.scheduler.max_lr`: learning rate peak\n",
    "- `training.augmentation.jitter`: FGS1 jitter augmentation on/off\n",
    "- `uncertainty.corel.enable`: COREL σ calibration toggle\n",
    "- `uncertainty.temperature.value`: temperature scaling\n",
    "- `decoder.sigma.attn_fusion`: enable attention×symbolic fusion in σ head\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ff947d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can edit these directly or build them programmatically.\n",
    "AB_GRID = {\n",
    "    \"one_at_a_time\": {\n",
    "        \"loss.symbolic.smoothness.lambda\": [0.0, 0.01, 0.05, 0.1],\n",
    "        \"loss.symbolic.asymmetry.lambda\":  [0.0, 0.01, 0.05],\n",
    "        \"loss.symbolic.nonneg.lambda\":     [0.0, 0.05],\n",
    "        \"training.scheduler.max_lr\":       [1e-4, 3e-4, 1e-3],\n",
    "        \"training.augmentation.jitter\":    [False, True],\n",
    "        \"uncertainty.corel.enable\":        [False, True],\n",
    "        \"uncertainty.temperature.value\":   [1.0, 1.25, 1.5],\n",
    "        \"decoder.sigma.attn_fusion\":       [False, True],\n",
    "    },\n",
    "    \"cartesian\": {\n",
    "        # Keep cartesian grids modest to respect runtime budgets.\n",
    "        \"loss.symbolic.smoothness.lambda\": [0.0, 0.05],\n",
    "        \"training.scheduler.max_lr\":       [1e-4, 3e-4],\n",
    "        \"uncertainty.corel.enable\":        [False, True],\n",
    "    }\n",
    "}\n",
    "print(json.dumps(AB_GRID, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc7b160b",
   "metadata": {},
   "source": [
    "## 🚀 Launch ablations via `spectramind ablate` (or load existing results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda77606",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, subprocess, shutil, os, time\n",
    "\n",
    "def run_ablate(mode=\"one-at-a-time\",\n",
    "               top_n=10,\n",
    "               retries=1,\n",
    "               parallel=2,\n",
    "               md_out=None,\n",
    "               html_out=None,\n",
    "               csv_out=None,\n",
    "               topn_zip=None):\n",
    "    exe = shutil.which(\"spectramind\")\n",
    "    if not exe:\n",
    "        print(\"⚠️ CLI not found; skipping ablation launch. You can run it in a terminal and re-run next cells.\")\n",
    "        return False\n",
    "    cmd = [\n",
    "        \"spectramind\", \"ablate\",\n",
    "        \"--mode\", mode,\n",
    "        \"--retries\", str(retries),\n",
    "        \"--parallel\", str(parallel),\n",
    "        \"--top_n\", str(top_n),\n",
    "    ]\n",
    "    if md_out:\n",
    "        cmd += [\"--md\", str(md_out)]\n",
    "    if html_out:\n",
    "        cmd += [\"--open_html\", \"false\", \"--html\", str(html_out)]\n",
    "    if csv_out:\n",
    "        cmd += [\"--csv\", str(csv_out)]\n",
    "    if topn_zip:\n",
    "        cmd += [\"--zip\", str(topn_zip)]\n",
    "    # Note: advanced flags (symbols, metrics, diagnostics) are provided by the CLI impl.\n",
    "    print(\"Running:\", \" \".join(cmd))\n",
    "    try:\n",
    "        subprocess.check_call(cmd, timeout=18000)  # 5 hours cap; adjust to your infra\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(\"Ablation run failed/aborted:\", e)\n",
    "        return False\n",
    "\n",
    "# Example: try a quick one-at-a-time sweep that writes leaderboard artifacts\n",
    "ok = run_ablate(\n",
    "    mode=\"one-at-a-time\",\n",
    "    top_n=10,\n",
    "    retries=1,\n",
    "    parallel=2,\n",
    "    md_out=ABLATE_SUMMARY_MD,\n",
    "    html_out=ABLATE_SUMMARY_HTML,\n",
    "    csv_out=ABLATE_SUMMARY_CSV,\n",
    "    topn_zip=TOPN_ZIP\n",
    ")\n",
    "\n",
    "if ok:\n",
    "    print(\"✅ Ablation CLI triggered. Artifacts should appear under:\", ABLATE_DIR)\n",
    "else:\n",
    "    print(\"⚠️ Skipped or failed to launch ablation via CLI.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aea35a3",
   "metadata": {},
   "source": [
    "## 📊 Parse leaderboard & visualize top runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20504493",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, csv, math\n",
    "from pathlib import Path\n",
    "\n",
    "def load_leaderboard(json_path=ABLATE_SUMMARY_JSON, csv_path=ABLATE_SUMMARY_CSV):\n",
    "    data = None\n",
    "    if json_path.exists():\n",
    "        try:\n",
    "            with open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "        except Exception as e:\n",
    "            print(\"Failed to parse JSON leaderboard:\", e)\n",
    "    rows = []\n",
    "    if csv_path.exists():\n",
    "        try:\n",
    "            import pandas as pd\n",
    "            df = pd.read_csv(csv_path)\n",
    "            print(\"CSV leaderboard loaded with shape:\", df.shape)\n",
    "            rows = df.to_dict(orient=\"records\")\n",
    "        except Exception as e:\n",
    "            print(\"Failed to parse CSV leaderboard via pandas:\", e)\n",
    "            # Fallback to Python csv\n",
    "            try:\n",
    "                with open(csv_path, newline=\"\", encoding=\"utf-8\") as f:\n",
    "                    reader = csv.DictReader(f)\n",
    "                    rows = list(reader)\n",
    "                print(\"CSV leaderboard loaded via csv module:\", len(rows), \"rows.\")\n",
    "            except Exception as e2:\n",
    "                print(\"CSV fallback failed:\", e2)\n",
    "    return data, rows\n",
    "\n",
    "lb_json, lb_rows = load_leaderboard()\n",
    "if lb_json is None and not lb_rows:\n",
    "    print(\"No leaderboard artifacts found yet. Once ablations complete, re-run this cell.\")\n",
    "else:\n",
    "    # Heuristic: find a numeric \"gll\" or \"score\" column for ranking\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    # Select rows and numeric metric\n",
    "    metric_key = None\n",
    "    if lb_rows:\n",
    "        keys = lb_rows[0].keys()\n",
    "        for k in [\"gll\", \"GLL\", \"score\", \"val_gll\", \"val_score\", \"mean_gll\"]:\n",
    "            if k in keys:\n",
    "                metric_key = k\n",
    "                break\n",
    "    # Plot top 15 by metric (lower GLL is better; adjust sign if needed)\n",
    "    if lb_rows and metric_key:\n",
    "        # Convert values to float where possible\n",
    "        def to_float(x):\n",
    "            try: return float(x)\n",
    "            except: return math.inf\n",
    "        sorted_rows = sorted(lb_rows, key=lambda r: to_float(r.get(metric_key, math.inf)))\n",
    "        top = sorted_rows[:15]\n",
    "        labels = [r.get(\"run_id\", r.get(\"short_id\", f\"r{i}\")) for i, r in enumerate(top)]\n",
    "        vals = [to_float(r.get(metric_key, math.inf)) for r in top]\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        plt.bar(range(len(vals)), vals)\n",
    "        plt.xticks(range(len(vals)), labels, rotation=45, ha=\"right\")\n",
    "        plt.ylabel(metric_key)\n",
    "        plt.title(\"Top runs by leaderboard metric\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"Leaderboard parsed, but could not identify a numeric metric column to plot.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae8ad2d",
   "metadata": {},
   "source": [
    "## 🧰 Optional: rebuild leaderboard & Top‑N ZIP via CLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9bff2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you ran custom ablations manually, you can regenerate summary artifacts here.\n",
    "import shutil, subprocess\n",
    "\n",
    "def rebuild_leaderboard(md_out=ABLATE_SUMMARY_MD, html_out=ABLATE_SUMMARY_HTML,\n",
    "                        csv_out=ABLATE_SUMMARY_CSV, topn_zip=TOPN_ZIP, top_n=10):\n",
    "    exe = shutil.which(\"spectramind\")\n",
    "    if not exe:\n",
    "        print(\"⚠️ CLI not found; cannot rebuild leaderboard via CLI.\")\n",
    "        return False\n",
    "    cmd = [\n",
    "        \"spectramind\", \"ablate\", \"--rebuild-only\",\n",
    "        \"--top_n\", str(top_n),\n",
    "        \"--md\", str(md_out),\n",
    "        \"--open_html\", \"false\", \"--html\", str(html_out),\n",
    "        \"--csv\", str(csv_out),\n",
    "        \"--zip\", str(topn_zip),\n",
    "    ]\n",
    "    print(\"Running:\", \" \".join(cmd))\n",
    "    try:\n",
    "        subprocess.check_call(cmd, timeout=3600)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(\"Rebuild failed:\", e)\n",
    "        return False\n",
    "\n",
    "# Example (safe to skip):\n",
    "# ok = rebuild_leaderboard()\n",
    "# print(\"Rebuild:\", ok)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e22946",
   "metadata": {},
   "source": [
    "## 🧾 Append run metadata to `v50_debug_log.md`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d54845",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "entry = f\"\"\"### Notebook: 03_ablation_and_tuning.ipynb\n",
    "- timestamp: {datetime.now().isoformat(timespec=\"seconds\")}\n",
    "- cwd: {ROOT}\n",
    "- python: {platform.python_version()}\n",
    "- actions:\n",
    "  - env_init\n",
    "  - ablate_cli_attempted: true\n",
    "  - leaderboard_json_exists: {ABLATE_SUMMARY_JSON.exists()}\n",
    "  - leaderboard_csv_exists: {ABLATE_SUMMARY_CSV.exists()}\n",
    "  - leaderboard_md_exists: {ABLATE_SUMMARY_MD.exists()}\n",
    "  - leaderboard_html_exists: {ABLATE_SUMMARY_HTML.exists()}\n",
    "  - topN_zip_exists: {TOPN_ZIP.exists()}\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    with open(LOG_MD, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(entry + \"\\n\")\n",
    "    print(f\"Appended notebook log entry to {LOG_MD}\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Could not append to {LOG_MD}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667d416f",
   "metadata": {},
   "source": [
    "## 04_leaderboard_and_submission_fixed.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d59c58d",
   "metadata": {},
   "source": [
    "# 🏁 SpectraMind V50 — 04 · Leaderboard & Submission\n",
    "\n",
    "This notebook closes the loop for the **NeurIPS 2025 Ariel Data Challenge**:\n",
    "\n",
    "- Aggregate **ablation/tuning** results and pick a best run\n",
    "- Run **self-test** to validate the pipeline\n",
    "- Generate a **submission** (μ, σ) with CLI\n",
    "- Perform basic **schema checks** on the submission\n",
    "- (Optional) **Upload** to Kaggle via API\n",
    "- Append an entry to `v50_debug_log.md` for reproducibility\n",
    "\n",
    "> CLI-first, Hydra-driven, DVC-aware. Cells auto-skip gracefully if CLI isn't on PATH.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d8954a",
   "metadata": {},
   "source": [
    "## 🔧 Environment & Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381be649",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json, os, sys, platform, shutil\n",
    "from datetime import datetime\n",
    "\n",
    "ROOT = Path.cwd()\n",
    "ART = ROOT / \"artifacts\"\n",
    "ABLATE_DIR = ART / \"ablation\"\n",
    "DIAG_DIR = ART / \"diagnostics\"\n",
    "SUBMIT_DIR = ART / \"submission\"\n",
    "SUBMIT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "LEADERBOARD_JSON = ABLATE_DIR / \"leaderboard.json\"\n",
    "LEADERBOARD_CSV  = ABLATE_DIR / \"leaderboard.csv\"\n",
    "BEST_RUN_JSON    = ABLATE_DIR / \"best_run.json\"   # optional\n",
    "SUBMISSION_CSV   = SUBMIT_DIR / \"submission.csv\"\n",
    "BUNDLE_ZIP       = SUBMIT_DIR / \"submission_bundle.zip\"\n",
    "LOG_MD           = ROOT / \"v50_debug_log.md\"\n",
    "\n",
    "env = {\n",
    "    \"timestamp\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "    \"python\": sys.version.replace(\"\\n\", \" \"),\n",
    "    \"platform\": platform.platform(),\n",
    "    \"cwd\": str(ROOT),\n",
    "    \"paths\": {\n",
    "        \"ART\": str(ART),\n",
    "        \"ABLATE_DIR\": str(ABLATE_DIR),\n",
    "        \"DIAG_DIR\": str(DIAG_DIR),\n",
    "        \"SUBMIT_DIR\": str(SUBMIT_DIR),\n",
    "        \"LEADERBOARD_JSON\": str(LEADERBOARD_JSON),\n",
    "        \"LEADERBOARD_CSV\": str(LEADERBOARD_CSV),\n",
    "        \"BEST_RUN_JSON\": str(BEST_RUN_JSON),\n",
    "        \"SUBMISSION_CSV\": str(SUBMISSION_CSV),\n",
    "        \"BUNDLE_ZIP\": str(BUNDLE_ZIP),\n",
    "        \"LOG_MD\": str(LOG_MD),\n",
    "    }\n",
    "}\n",
    "print(json.dumps(env, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7792d7e",
   "metadata": {},
   "source": [
    "## 🩺 CLI sanity (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd0d7cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil, subprocess\n",
    "\n",
    "def check_cli(cmd=\"spectramind\", args=[\"--version\"]):\n",
    "    exe = shutil.which(cmd)\n",
    "    if not exe:\n",
    "        print(\"⚠️ 'spectramind' CLI not found on PATH. Notebook will still run, but CLI calls will be skipped.\")\n",
    "        return {\"available\": False}\n",
    "    try:\n",
    "        out = subprocess.check_output([cmd] + args, stderr=subprocess.STDOUT, text=True, timeout=30)\n",
    "        print(out)\n",
    "        return {\"available\": True, \"output\": out}\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ CLI call failed: {e}\")\n",
    "        return {\"available\": True, \"error\": str(e)}\n",
    "\n",
    "cli_info = check_cli()\n",
    "cli_info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69767392",
   "metadata": {},
   "source": [
    "## 📊 Aggregate ablation & pick best run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0c184c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, math, json\n",
    "from pathlib import Path\n",
    "\n",
    "best = None\n",
    "rows = []\n",
    "\n",
    "# Try CSV first\n",
    "if LEADERBOARD_CSV.exists():\n",
    "    try:\n",
    "        import pandas as pd\n",
    "        df = pd.read_csv(LEADERBOARD_CSV)\n",
    "        print(\"Loaded ablation leaderboard CSV:\", LEADERBOARD_CSV, \"shape=\", df.shape)\n",
    "        # Try common metric keys, prefer lower-is-better GLL if present\n",
    "        metric_key = None\n",
    "        for k in [\"gll\", \"GLL\", \"score\", \"val_gll\", \"val_score\", \"mean_gll\"]:\n",
    "            if k in df.columns:\n",
    "                metric_key = k\n",
    "                break\n",
    "        if metric_key:\n",
    "            # lower GLL better; if score, assume higher is better and invert for sort flag\n",
    "            ascending = True if \"gll\" in metric_key.lower() else False\n",
    "            df_sorted = df.sort_values(metric_key, ascending=ascending)\n",
    "            best = df_sorted.iloc[0].to_dict()\n",
    "            display_cols = [c for c in df.columns if c in [\"run_id\", \"short_id\", \"config\", metric_key]]\n",
    "            print(\"Top 5 by\", metric_key)\n",
    "            display(df_sorted[display_cols].head(5))\n",
    "        else:\n",
    "            print(\"No known metric column found; showing head:\")\n",
    "            display(df.head())\n",
    "    except Exception as e:\n",
    "        print(\"CSV parse failed:\", e)\n",
    "\n",
    "# Fallback to JSON\n",
    "if best is None and LEADERBOARD_JSON.exists():\n",
    "    try:\n",
    "        with open(LEADERBOARD_JSON, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        # Expect list of dicts; pick by gll or score\n",
    "        metric_key = None\n",
    "        if isinstance(data, list) and data:\n",
    "            keys = list(data[0].keys())\n",
    "            for k in [\"gll\", \"GLL\", \"score\", \"val_gll\", \"val_score\", \"mean_gll\"]:\n",
    "                if k in keys:\n",
    "                    metric_key = k\n",
    "                    break\n",
    "        if metric_key:\n",
    "            def sort_key(d):\n",
    "                v = d.get(metric_key, math.inf)\n",
    "                try: return float(v)\n",
    "                except: return math.inf\n",
    "            # lower gll better; if score, higher better\n",
    "            reverse = False if \"gll\" in metric_key.lower() else True\n",
    "            data_sorted = sorted(data, key=sort_key, reverse=reverse)\n",
    "            best = data_sorted[0]\n",
    "            print(\"Top 3 candidates by\", metric_key)\n",
    "            for i, d in enumerate(data_sorted[:3]):\n",
    "                rid = d.get(\"run_id\") or d.get(\"short_id\") or f\"r{i}\"\n",
    "                print(i+1, rid, metric_key, d.get(metric_key))\n",
    "        else:\n",
    "            print(\"JSON leaderboard present but no known metric. Keys:\", list(data[0].keys()) if isinstance(data, list) and data else None)\n",
    "    except Exception as e:\n",
    "        print(\"JSON parse failed:\", e)\n",
    "\n",
    "# Persist best run selection (optional)\n",
    "if best:\n",
    "    with open(BEST_RUN_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(best, f, indent=2)\n",
    "    print(\"Best run saved to:\", BEST_RUN_JSON)\n",
    "else:\n",
    "    print(\"⚠️ No ablation leaderboard found; proceeding without an explicit best-run selection.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206e874e",
   "metadata": {},
   "source": [
    "## ✅ Run pipeline self-test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe51cdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, shutil\n",
    "\n",
    "def run_selftest():\n",
    "    exe = shutil.which(\"spectramind\")\n",
    "    if not exe:\n",
    "        print(\"⚠️ spectramind CLI not found; skipping self-test.\")\n",
    "        return False\n",
    "    # Try deep test first, fallback to fast\n",
    "    cmds = [\n",
    "        [\"spectramind\", \"test\", \"--deep\"],\n",
    "        [\"spectramind\", \"test\"]\n",
    "    ]\n",
    "    for cmd in cmds:\n",
    "        try:\n",
    "            print(\"Running:\", \" \".join(cmd))\n",
    "            subprocess.check_call(cmd, timeout=1800)\n",
    "            print(\"✅ Self-test passed:\", \" \".join(cmd))\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(\"Self-test variant failed:\", e)\n",
    "    print(\"⚠️ All self-test variants failed.\")\n",
    "    return False\n",
    "\n",
    "_ = run_selftest()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc717453",
   "metadata": {},
   "source": [
    "## 📦 Generate submission (μ/σ) via CLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3409694f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, shutil\n",
    "from pathlib import Path\n",
    "\n",
    "def generate_submission(out_csv: Path):\n",
    "    exe = shutil.which(\"spectramind\")\n",
    "    if not exe:\n",
    "        print(\"⚠️ spectramind CLI not found; skipping submission generation.\")\n",
    "        return False\n",
    "    out_csv.parent.mkdir(parents=True, exist_ok=True)\n",
    "    cmd = [\"spectramind\", \"submit\", \"--out\", str(out_csv), \"--include-diagnostics\", \"true\"]\n",
    "    # If BEST_RUN_JSON exists and includes a config or short_id, pass it through as override if supported\n",
    "    if BEST_RUN_JSON.exists():\n",
    "        try:\n",
    "            import json\n",
    "            best = json.loads(BEST_RUN_JSON.read_text())\n",
    "            # attempt a generic override key if available\n",
    "            if \"config\" in best and isinstance(best[\"config\"], str):\n",
    "                cmd += [\"--config\", best[\"config\"]]\n",
    "            elif \"short_id\" in best:\n",
    "                cmd += [\"--run-id\", str(best[\"short_id\"])]\n",
    "        except Exception as e:\n",
    "            print(\"Note: couldn't parse BEST_RUN_JSON for overrides:\", e)\n",
    "    print(\"Running:\", \" \".join(cmd))\n",
    "    try:\n",
    "        subprocess.check_call(cmd, timeout=6*3600)  # allow long run\n",
    "        print(\"✅ Submission written:\", out_csv)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(\"❌ Submission generation failed:\", e)\n",
    "        return False\n",
    "\n",
    "ok = generate_submission(SUBMISSION_CSV)\n",
    "ok\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d30f65",
   "metadata": {},
   "source": [
    "## 🔎 Basic submission schema check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0802b101",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def validate_submission(path: Path):\n",
    "    if not path.exists():\n",
    "        print(\"❌ No submission file found at\", path)\n",
    "        return False\n",
    "    try:\n",
    "        df = pd.read_csv(path)\n",
    "    except Exception as e:\n",
    "        print(\"❌ CSV read failed:\", e)\n",
    "        return False\n",
    "\n",
    "    print(\"Submission shape:\", df.shape)\n",
    "    # Very generic checks: non-empty, numeric columns present\n",
    "    if df.empty:\n",
    "        print(\"❌ Submission is empty.\")\n",
    "        return False\n",
    "\n",
    "    # Try to detect numeric prediction columns (μ/σ); require at least a few numeric columns\n",
    "    num_cols = [c for c in df.columns if np.issubdtype(df[c].dtype, np.number)]\n",
    "    if len(num_cols) < 10:\n",
    "        print(\"⚠️ Fewer than 10 numeric columns detected; ensure μ/σ columns are included.\")\n",
    "    else:\n",
    "        # Check no NaNs in prediction columns\n",
    "        nan_counts = df[num_cols].isna().sum().sum()\n",
    "        if nan_counts > 0:\n",
    "            print(f\"⚠️ Found {nan_counts} NaNs in numeric columns; please investigate.\")\n",
    "\n",
    "    # Optional: enforce monotonic ID or required column name heuristics if known\n",
    "    print(\"✅ Basic checks completed.\")\n",
    "    return True\n",
    "\n",
    "_ = validate_submission(SUBMISSION_CSV)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe1e9af",
   "metadata": {},
   "source": [
    "## 🗜️ Bundle submission + diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff810a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "def bundle_zip(bundle: Path, submission: Path, extras: list[Path] = None):\n",
    "    extras = extras or []\n",
    "    with zipfile.ZipFile(bundle, \"w\", compression=zipfile.ZIP_DEFLATED) as z:\n",
    "        if submission.exists():\n",
    "            z.write(submission, submission.name)\n",
    "        else:\n",
    "            print(\"⚠️ No submission to include.\")\n",
    "        for p in extras:\n",
    "            if p.exists():\n",
    "                if p.is_file():\n",
    "                    z.write(p, p.name)\n",
    "                else:\n",
    "                    # add directory contents flatly\n",
    "                    for child in p.rglob(\"*\"):\n",
    "                        if child.is_file():\n",
    "                            arc = child.relative_to(p.parent)\n",
    "                            z.write(child, str(arc))\n",
    "    print(\"Bundle written:\", bundle)\n",
    "\n",
    "extras = [DIAG_DIR] if DIAG_DIR.exists() else []\n",
    "bundle_zip(BUNDLE_ZIP, SUBMISSION_CSV, extras)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed24683a",
   "metadata": {},
   "source": [
    "## ⤴️ (Optional) Push to Kaggle via API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d105904a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell attempts a Kaggle upload if configured.\n",
    "# Prerequisites:\n",
    "#  - Install kaggle:  pip install kaggle\n",
    "#  - Place Kaggle credentials (~/.kaggle/kaggle.json) or set KAGGLE_USERNAME / KAGGLE_KEY env vars\n",
    "#  - Set COMPETITION to the exact competition slug\n",
    "\n",
    "import os, subprocess, shutil\n",
    "\n",
    "COMPETITION = os.environ.get(\"KAGGLE_COMPETITION\", \"ariel-data-challenge-2025\")\n",
    "\n",
    "def kaggle_submit(csv_path: Path, message: str = \"SpectraMind V50 submission\"):\n",
    "    if not csv_path.exists():\n",
    "        print(\"⚠️ No CSV to submit.\")\n",
    "        return False\n",
    "    # Try kaggle CLI\n",
    "    exe = shutil.which(\"kaggle\")\n",
    "    if not exe:\n",
    "        print(\"ℹ️ kaggle CLI not found; skipping Kaggle upload.\")\n",
    "        return False\n",
    "    try:\n",
    "        cmd = [\"kaggle\", \"competitions\", \"submit\", \"-c\", COMPETITION, \"-f\", str(csv_path), \"-m\", message]\n",
    "        print(\"Running:\", \" \".join(cmd))\n",
    "        subprocess.check_call(cmd, timeout=600)\n",
    "        print(\"✅ Kaggle submission attempted.\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(\"⚠️ Kaggle submission failed:\", e)\n",
    "        return False\n",
    "\n",
    "# Disabled by default; uncomment to attempt submission\n",
    "# _ = kaggle_submit(SUBMISSION_CSV, \"SpectraMind V50 auto-generated submission\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83514e33",
   "metadata": {},
   "source": [
    "## 🧾 Append run metadata to `v50_debug_log.md`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b918469",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "entry = f\"\"\"### Notebook: 04_leaderboard_and_submission.ipynb\n",
    "- timestamp: {datetime.now().isoformat(timespec=\"seconds\")}\n",
    "- cwd: {ROOT}\n",
    "- python: {platform.python_version()}\n",
    "- actions:\n",
    "  - ablation_leaderboard_csv: {LEADERBOARD_CSV.exists()}\n",
    "  - ablation_leaderboard_json: {LEADERBOARD_JSON.exists()}\n",
    "  - best_run_json: {BEST_RUN_JSON.exists()}\n",
    "  - submission_csv_exists: {SUBMISSION_CSV.exists()}\n",
    "  - bundle_zip_exists: {BUNDLE_ZIP.exists()}\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    with open(LOG_MD, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(entry + \"\\n\")\n",
    "    print(f\"Appended notebook log entry to {LOG_MD}\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Could not append to {LOG_MD}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e03e15",
   "metadata": {},
   "source": [
    "## 05_uncertainty_calibration_and_cycle_consistency_fixed.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355f8fa8",
   "metadata": {},
   "source": [
    "# 📐 SpectraMind V50 — 05 · Uncertainty Calibration & Cycle Consistency\n",
    "\n",
    "Goals:\n",
    "- Evaluate calibration quality (coverage vs. nominal for μ/σ)\n",
    "- Apply **temperature scaling** (global σ scale) and optional per-bin scaling\n",
    "- Emit a **calibrated submission CSV** with updated σ\n",
    "- (Optional) Run a **cycle-consistency** sanity check via forward sim\n",
    "- Log a **reproducibility entry** to `v50_debug_log.md`\n",
    "\n",
    "> CLI-first and reproducibility-friendly. Cells skip gracefully if inputs/CLI aren't present.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd38a4d",
   "metadata": {},
   "source": [
    "## 🔧 Environment & Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75bb1fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json, os, sys, platform, shutil\n",
    "from datetime import datetime\n",
    "\n",
    "ROOT = Path.cwd()\n",
    "ART = ROOT / \"artifacts\"\n",
    "DIAG_DIR = ART / \"diagnostics\"\n",
    "SUBMIT_DIR = ART / \"submission\"\n",
    "CAL_DIR = ART / \"calibration\"\n",
    "CAL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Typical inputs (if present)\n",
    "DIAG_SUMMARY = DIAG_DIR / \"diagnostic_summary.json\"  # expected to hold residual/sigma stats if generated\n",
    "SUBMISSION_CSV = SUBMIT_DIR / \"submission.csv\"\n",
    "\n",
    "# Outputs\n",
    "CAL_REPORT_JSON = CAL_DIR / \"calibration_report.json\"\n",
    "CAL_FACTORS_JSON = CAL_DIR / \"sigma_scale_factors.json\"\n",
    "CALIBRATED_SUBMISSION_CSV = SUBMIT_DIR / \"submission_calibrated.csv\"\n",
    "LOG_MD = ROOT / \"v50_debug_log.md\"\n",
    "\n",
    "env = {\n",
    "    \"timestamp\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "    \"python\": sys.version.replace(\"\\n\", \" \"),\n",
    "    \"platform\": platform.platform(),\n",
    "    \"cwd\": str(ROOT),\n",
    "    \"paths\": {\n",
    "        \"ART\": str(ART),\n",
    "        \"DIAG_DIR\": str(DIAG_DIR),\n",
    "        \"SUBMIT_DIR\": str(SUBMIT_DIR),\n",
    "        \"CAL_DIR\": str(CAL_DIR),\n",
    "        \"DIAG_SUMMARY\": str(DIAG_SUMMARY),\n",
    "        \"SUBMISSION_CSV\": str(SUBMISSION_CSV),\n",
    "        \"CAL_REPORT_JSON\": str(CAL_REPORT_JSON),\n",
    "        \"CAL_FACTORS_JSON\": str(CAL_FACTORS_JSON),\n",
    "        \"CALIBRATED_SUBMISSION_CSV\": str(CALIBRATED_SUBMISSION_CSV),\n",
    "        \"LOG_MD\": str(LOG_MD),\n",
    "    }\n",
    "}\n",
    "print(json.dumps(env, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7162c5c",
   "metadata": {},
   "source": [
    "## 🩺 CLI sanity (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8fce53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil, subprocess\n",
    "\n",
    "def check_cli(cmd=\"spectramind\", args=[\"--version\"]):\n",
    "    exe = shutil.which(cmd)\n",
    "    if not exe:\n",
    "        print(\"ℹ️ 'spectramind' CLI not found on PATH. CLI-dependent steps will be skipped.\")\n",
    "        return {\"available\": False}\n",
    "    try:\n",
    "        out = subprocess.check_output([cmd] + args, stderr=subprocess.STDOUT, text=True, timeout=30)\n",
    "        print(out)\n",
    "        return {\"available\": True, \"output\": out}\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ CLI call failed: {e}\")\n",
    "        return {\"available\": True, \"error\": str(e)}\n",
    "\n",
    "cli_info = check_cli()\n",
    "cli_info\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae0a8f4",
   "metadata": {},
   "source": [
    "## 📊 Load diagnostics & compute temperature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e30b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, math\n",
    "from statistics import median\n",
    "import numpy as np\n",
    "\n",
    "def load_diag_summary(path: Path):\n",
    "    if not path.exists():\n",
    "        print(\"⚠️ No diagnostic summary found:\", path)\n",
    "        return None\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(\"⚠️ Failed to parse diagnostic summary:\", e)\n",
    "        return None\n",
    "\n",
    "def extract_resid_sigma(diag_data):\n",
    "    \"\"\"Attempt to extract arrays of residuals and predicted sigmas from a generic diagnostic JSON.\n",
    "    Heuristics handle a few common schemas.\n",
    "    Returns arrays flattened across items/bins if possible.\n",
    "    \"\"\"\n",
    "    resids = []\n",
    "    sigmas = []\n",
    "    if diag_data is None:\n",
    "        return np.array([]), np.array([])\n",
    "\n",
    "    # Common shapes: {\"items\": [{\"residuals\":[...], \"sigma\":[...]} ...]}\n",
    "    # Or {\"planets\": {\"id\": {\"residuals\":[...], \"sigma\":[...]}}}\n",
    "    # Or flat: {\"residuals\":[...], \"sigma\":[...]}\n",
    "    if isinstance(diag_data, dict):\n",
    "        if \"items\" in diag_data and isinstance(diag_data[\"items\"], list):\n",
    "            for rec in diag_data[\"items\"]:\n",
    "                r = rec.get(\"residuals\") or rec.get(\"resid\") or rec.get(\"residual\")\n",
    "                s = rec.get(\"sigma\") or rec.get(\"sigmas\") or rec.get(\"pred_sigma\")\n",
    "                if isinstance(r, list) and isinstance(s, list):\n",
    "                    m = min(len(r), len(s))\n",
    "                    resids.extend(r[:m])\n",
    "                    sigmas.extend(s[:m])\n",
    "        elif \"planets\" in diag_data and isinstance(diag_data[\"planets\"], dict):\n",
    "            for _, rec in diag_data[\"planets\"].items():\n",
    "                r = rec.get(\"residuals\") or rec.get(\"resid\") or rec.get(\"residual\")\n",
    "                s = rec.get(\"sigma\") or rec.get(\"sigmas\") or rec.get(\"pred_sigma\")\n",
    "                if isinstance(r, list) and isinstance(s, list):\n",
    "                    m = min(len(r), len(s))\n",
    "                    resids.extend(r[:m])\n",
    "                    sigmas.extend(s[:m])\n",
    "        else:\n",
    "            r = diag_data.get(\"residuals\") or diag_data.get(\"resid\") or diag_data.get(\"residual\")\n",
    "            s = diag_data.get(\"sigma\") or diag_data.get(\"sigmas\") or diag_data.get(\"pred_sigma\")\n",
    "            if isinstance(r, list) and isinstance(s, list):\n",
    "                m = min(len(r), len(s))\n",
    "                resids.extend(r[:m])\n",
    "                sigmas.extend(s[:m])\n",
    "\n",
    "    res = np.array(resids, dtype=float) if resids else np.array([])\n",
    "    sg = np.array(sigmas, dtype=float) if sigmas else np.array([])\n",
    "    print(\"Extracted residuals:\", res.shape, \"sigmas:\", sg.shape)\n",
    "    return res, sg\n",
    "\n",
    "def compute_temperature_scaling(res, sg, eps=1e-12):\n",
    "    \"\"\"Compute a global scale alpha for sigma to improve calibration.\n",
    "    Two estimates:\n",
    "      - RMS-based: sqrt(mean(res^2) / mean(sigma^2))\n",
    "      - Median-abs-based: median(|res|/sigma) / median(|N(0,1)|) with median(|Z|)=0.674489...\n",
    "    Returns dict with both and a chosen alpha.\n",
    "    \"\"\"\n",
    "    if res.size == 0 or sg.size == 0 or res.size != sg.size:\n",
    "        return {\"alpha_rms\": 1.0, \"alpha_med\": 1.0, \"alpha\": 1.0, \"note\": \"insufficient data\"}\n",
    "\n",
    "    rms_res = np.sqrt(np.mean(res**2))\n",
    "    rms_sig = np.sqrt(np.mean((sg+eps)**2))\n",
    "    alpha_rms = (rms_res / (rms_sig + eps)) if rms_sig > 0 else 1.0\n",
    "\n",
    "    ratio = np.abs(res) / (sg + eps)\n",
    "    med_ratio = np.median(ratio)\n",
    "    med_abs_z = 0.6744897501960817  # median |N(0,1)|\n",
    "    alpha_med = (med_ratio / med_abs_z) if med_abs_z > 0 else 1.0\n",
    "\n",
    "    # Choose alpha preferring robust median, falling back to rms if extreme\n",
    "    alpha = alpha_med if 0.1 <= alpha_med <= 10 else alpha_rms\n",
    "    return {\"alpha_rms\": float(alpha_rms), \"alpha_med\": float(alpha_med), \"alpha\": float(alpha)}\n",
    "\n",
    "def nominal_coverage_checks(res, sg, alphas=(1.0,), eps=1e-12):\n",
    "    \"\"\"Compute empirical coverage for ±1σ (~68.27%) and ±1.96σ (~95%) under various alphas.\"\"\"\n",
    "    if res.size == 0 or sg.size == 0 or res.size != sg.size:\n",
    "        return {}\n",
    "    out = {}\n",
    "    for a in alphas:\n",
    "        z = np.abs(res) / (a*(sg+eps))\n",
    "        cov68 = float(np.mean(z <= 1.0))\n",
    "        cov95 = float(np.mean(z <= 1.96))\n",
    "        out[str(a)] = {\"cov_68\": cov68, \"cov_95\": cov95}\n",
    "    return out\n",
    "\n",
    "diag = load_diag_summary(DIAG_SUMMARY)\n",
    "res, sg = extract_resid_sigma(diag)\n",
    "scales = compute_temperature_scaling(res, sg)\n",
    "cov = nominal_coverage_checks(res, sg, alphas=(1.0, scales.get(\"alpha\", 1.0)))\n",
    "print(\"Proposed scaling:\", json.dumps(scales, indent=2))\n",
    "print(\"Coverage:\", json.dumps(cov, indent=2))\n",
    "\n",
    "# Persist a small calibration report\n",
    "report = {\n",
    "    \"timestamp\": datetime.now().isoformat(timespec=\"seconds\"),\n",
    "    \"counts\": int(min(res.size, sg.size)),\n",
    "    \"scales\": scales,\n",
    "    \"coverage\": cov\n",
    "}\n",
    "with open(CAL_REPORT_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(report, f, indent=2)\n",
    "\n",
    "with open(CAL_FACTORS_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"global_sigma_scale\": scales.get(\"alpha\", 1.0)}, f, indent=2)\n",
    "\n",
    "print(\"Saved:\", CAL_REPORT_JSON, \"and\", CAL_FACTORS_JSON)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c743eb13",
   "metadata": {},
   "source": [
    "## 📈 Visualize calibration (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e501664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build an empirical CDF of |res|/sigma and overlay nominal 68/95 thresholds\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_empirical_cdf(res, sg, alpha=1.0, title=\"Empirical |z| CDF\"):\n",
    "    if res.size == 0 or sg.size == 0 or res.size != sg.size:\n",
    "        print(\"No data to plot.\")\n",
    "        return\n",
    "    z = np.abs(res) / (alpha*(sg+1e-12))\n",
    "    z_sorted = np.sort(z)\n",
    "    y = np.linspace(0, 1, len(z_sorted), endpoint=False)\n",
    "\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(z_sorted, y)\n",
    "    plt.axvline(1.0, linestyle=\"--\")\n",
    "    plt.axvline(1.96, linestyle=\"--\")\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"|z| = |res| / (alpha * sigma)\")\n",
    "    plt.ylabel(\"CDF\")\n",
    "    plt.show()\n",
    "\n",
    "# Raw and scaled views (if data available)\n",
    "if res.size and sg.size and res.size == sg.size:\n",
    "    plot_empirical_cdf(res, sg, alpha=1.0, title=\"Empirical |z| CDF (alpha=1.0)\")\n",
    "    plot_empirical_cdf(res, sg, alpha=max(1e-6, float(scales.get(\"alpha\", 1.0))), title=f\"Empirical |z| CDF (alpha={scales.get('alpha', 1.0):.3f})\")\n",
    "else:\n",
    "    print(\"⚠️ Skipping plots — residual/sigma arrays unavailable.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b387f925",
   "metadata": {},
   "source": [
    "## 🛠️ Apply scaling to submission σ columns and save calibrated CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8be52c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def detect_sigma_columns(df: pd.DataFrame):\n",
    "    # Heuristics: columns containing \"sigma\" or ending with \"_sigma\"\n",
    "    cands = [c for c in df.columns if \"sigma\" in c.lower() or c.lower().endswith(\"_sigma\")]\n",
    "    return cands\n",
    "\n",
    "def scale_submission_sigmas(sub_csv: Path, out_csv: Path, alpha: float):\n",
    "    if not sub_csv.exists():\n",
    "        print(\"⚠️ Submission not found:\", sub_csv)\n",
    "        return False, []\n",
    "    try:\n",
    "        df = pd.read_csv(sub_csv)\n",
    "    except Exception as e:\n",
    "        print(\"❌ Failed to read submission CSV:\", e)\n",
    "        return False, []\n",
    "\n",
    "    sigma_cols = detect_sigma_columns(df)\n",
    "    if not sigma_cols:\n",
    "        # Fallback: if columns alternate mu/sigma per bin with patterns, user can adapt here\n",
    "        print(\"⚠️ No sigma-like columns detected; no scaling applied.\")\n",
    "        out_csv.write_text(df.to_csv(index=False))\n",
    "        return True, []\n",
    "\n",
    "    alpha = float(alpha) if np.isfinite(alpha) else 1.0\n",
    "    df[sigma_cols] = df[sigma_cols].astype(float) * alpha\n",
    "    out_csv.write_text(df.to_csv(index=False))\n",
    "    print(f\"✅ Wrote calibrated submission with alpha={alpha} to\", out_csv)\n",
    "    return True, sigma_cols\n",
    "\n",
    "alpha = float((scales or {}).get(\"alpha\", 1.0))\n",
    "ok, used_cols = scale_submission_sigmas(SUBMISSION_CSV, CALIBRATED_SUBMISSION_CSV, alpha)\n",
    "print(\"Sigma columns scaled:\", used_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6e23fb",
   "metadata": {},
   "source": [
    "## 🔬 (Optional) Per-bin scaling (if binwise stats available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad523b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If DIAG_SUMMARY contains per-bin RMSE and mean sigma per wavelength/bin,\n",
    "# compute per-bin alpha and (optionally) save a separate per-bin-calibrated submission.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "PERBIN_FACTORS_JSON = CAL_DIR / \"sigma_scale_perbin.json\"\n",
    "CALIBRATED_PERBIN_SUBMISSION_CSV = SUBMIT_DIR / \"submission_calibrated_perbin.csv\"\n",
    "\n",
    "def compute_perbin_alphas(diag_data):\n",
    "    # Heuristics: look for entries like {\"bin_index\": i, \"rmse\": ..., \"mean_sigma\": ...}\n",
    "    # or arrays diag_data[\"per_bin\"][\"rmse\"], diag_data[\"per_bin\"][\"mean_sigma\"]\n",
    "    if diag_data is None:\n",
    "        return None\n",
    "    rmse = None; msig = None\n",
    "    if isinstance(diag_data, dict) and \"per_bin\" in diag_data:\n",
    "        per = diag_data[\"per_bin\"]\n",
    "        if isinstance(per, dict):\n",
    "            rmse = per.get(\"rmse\")\n",
    "            msig = per.get(\"mean_sigma\") or per.get(\"sigma_mean\")\n",
    "    if isinstance(rmse, list) and isinstance(msig, list) and len(rmse)==len(msig):\n",
    "        rmse = np.array(rmse, dtype=float)\n",
    "        msig = np.array(msig, dtype=float)\n",
    "        with np.errstate(divide='ignore', invalid='ignore'):\n",
    "            alphas = np.where(msig>0, np.sqrt(rmse**2 / (msig**2 + 1e-12)), 1.0)\n",
    "        return alphas.tolist()\n",
    "    return None\n",
    "\n",
    "perbin_alphas = compute_perbin_alphas(diag)\n",
    "if perbin_alphas:\n",
    "    with open(PERBIN_FACTORS_JSON, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump({\"per_bin_alphas\": perbin_alphas}, f, indent=2)\n",
    "    print(\"Saved per-bin scale factors:\", PERBIN_FACTORS_JSON)\n",
    "\n",
    "    # Attempt to apply to submission if it has distinct sigma columns per-bin\n",
    "    try:\n",
    "        df = pd.read_csv(SUBMISSION_CSV)\n",
    "        sigma_cols = [c for c in df.columns if \"sigma\" in c.lower() or c.lower().endswith(\"_sigma\")]\n",
    "        # If sigma columns equal in number to perbin_alphas, map directly\n",
    "        if sigma_cols and len(sigma_cols) == len(perbin_alphas):\n",
    "            df[sigma_cols] = df[sigma_cols].astype(float) * np.array(perbin_alphas, dtype=float)\n",
    "            CALIBRATED_PERBIN_SUBMISSION_CSV.write_text(df.to_csv(index=False))\n",
    "            print(\"✅ Wrote per-bin calibrated submission:\", CALIBRATED_PERBIN_SUBMISSION_CSV)\n",
    "        else:\n",
    "            print(\"ℹ️ Per-bin scaling not applied: mismatch between sigma columns and per-bin factors.\")\n",
    "    except Exception as e:\n",
    "        print(\"ℹ️ Per-bin scaling skipped due to error:\", e)\n",
    "else:\n",
    "    print(\"ℹ️ No per-bin stats detected; skipping per-bin scaling.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8731591e",
   "metadata": {},
   "source": [
    "## 🔁 (Optional) Cycle-consistency via forward simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7932a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil, subprocess\n",
    "\n",
    "def run_cycle_consistency(sub_csv: Path):\n",
    "    exe = shutil.which(\"spectramind\")\n",
    "    if not exe:\n",
    "        print(\"ℹ️ spectramind CLI not found; skipping forward-sim cycle test.\")\n",
    "        return False\n",
    "    # Hypothetical CLI signature; adjust to your repo's `simulate` subcommand if present\n",
    "    cmd = [\"spectramind\", \"simulate\", \"--from-spectra\", str(sub_csv), \"--out\", str(CAL_DIR / \"simulated_observations\")]\n",
    "    print(\"Running:\", \" \".join(cmd))\n",
    "    try:\n",
    "        subprocess.check_call(cmd, timeout=3600)\n",
    "        print(\"✅ Forward simulation produced artifacts in:\", CAL_DIR / \"simulated_observations\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(\"ℹ️ Forward-sim step failed or not implemented:\", e)\n",
    "        return False\n",
    "\n",
    "# Try on calibrated submission if present; fallback to original\n",
    "target_csv = CALIBRATED_SUBMISSION_CSV if CALIBRATED_SUBMISSION_CSV.exists() else SUBMISSION_CSV\n",
    "_ = run_cycle_consistency(target_csv) if target_csv.exists() else print(\"ℹ️ No submission CSV available for cycle check.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d53c57",
   "metadata": {},
   "source": [
    "## 🧾 Append run metadata to `v50_debug_log.md`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9342ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "entry = f\"\"\"### Notebook: 05_uncertainty_calibration_and_cycle_consistency.ipynb\n",
    "- timestamp: {datetime.now().isoformat(timespec=\"seconds\")}\n",
    "- cwd: {ROOT}\n",
    "- python: {platform.python_version()}\n",
    "- actions:\n",
    "  - diag_summary_present: {DIAG_SUMMARY.exists()}\n",
    "  - submission_csv_present: {SUBMISSION_CSV.exists()}\n",
    "  - alpha_used: {(scales or {}).get(\"alpha\", 1.0)}\n",
    "  - calibrated_submission_csv_exists: {CALIBRATED_SUBMISSION_CSV.exists()}\n",
    "\"\"\"\n",
    "try:\n",
    "    with open(LOG_MD, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(entry + \"\\n\")\n",
    "    print(f\"Appended notebook log entry to {LOG_MD}\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Could not append to {LOG_MD}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff29166",
   "metadata": {},
   "source": [
    "## 06_physics_informed_modeling_and_symbolic_constraints.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7155c2",
   "metadata": {},
   "source": [
    "# 🛰️ SpectraMind V50 — Physics‑Informed Modeling & Symbolic Constraints (Notebook 06)\n",
    "\n",
    "**Purpose.** Add *physics‑informed, symbolic constraints* to the SpectraMind V50 pipeline and run diagnostics for\n",
    "constraint violations and cycle‑consistency. The notebook adheres to the CLI‑first, Hydra‑safe workflow used in 00–05.\n",
    "\n",
    "**Sections**\n",
    "1. Pre‑flight & environment capture\n",
    "2. Compose Hydra overrides for symbolic losses\n",
    "3. Train with symbolic constraints\n",
    "4. Symbolic diagnostics (rule ranking/overlays)\n",
    "5. Cycle‑consistency (simulate μ → validate)\n",
    "6. Artifacts & next steps\n",
    "\n",
    "> Degrades gracefully: if the `spectramind` CLI is not available, the notebook switches to **DRY‑RUN** and still produces configs/logs/placeholder artifacts to keep the workflow reproducible.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d87a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ░░ Pre‑flight: environment, run IDs, paths, CLI detection ░░\n",
    "import os, sys, json, platform, shutil, subprocess, datetime, pathlib\n",
    "\n",
    "RUN_TS = datetime.datetime.utcnow().strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "RUN_ID = f\"physics_informed_{RUN_TS}\"\n",
    "ROOT_OUT = \"/mnt/data/physics_informed\"\n",
    "ARTIFACTS = os.path.join(ROOT_OUT, RUN_ID)\n",
    "LOGS = os.path.join(ARTIFACTS, \"logs\")\n",
    "CFG_OUT = os.path.join(ARTIFACTS, \"configs\")\n",
    "DIAG_OUT = os.path.join(ARTIFACTS, \"diagnostics\")\n",
    "SIM_OUT = os.path.join(ARTIFACTS, \"simulation\")\n",
    "for p in (ROOT_OUT, ARTIFACTS, LOGS, CFG_OUT, DIAG_OUT, SIM_OUT):\n",
    "    os.makedirs(p, exist_ok=True)\n",
    "\n",
    "def which(cmd: str):\n",
    "    return shutil.which(cmd) is not None\n",
    "\n",
    "CLI_PRESENT = which(\"spectramind\")\n",
    "DRY_RUN = not CLI_PRESENT\n",
    "\n",
    "def git_cmd(args):\n",
    "    try:\n",
    "        out = subprocess.check_output([\"git\", *args], stderr=subprocess.STDOUT, timeout=5).decode().strip()\n",
    "        return out\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "env = {\n",
    "    \"python\": sys.version.replace(\"\\n\",\" \"),\n",
    "    \"platform\": platform.platform(),\n",
    "    \"cli_present\": CLI_PRESENT,\n",
    "    \"dry_run\": DRY_RUN,\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"paths\": {\"artifacts\": ARTIFACTS, \"logs\": LOGS, \"configs\": CFG_OUT, \"diagnostics\": DIAG_OUT, \"simulation\": SIM_OUT},\n",
    "    \"git\": {\n",
    "        \"commit\": git_cmd([\"rev-parse\", \"HEAD\"]),\n",
    "        \"branch\": git_cmd([\"rev-parse\", \"--abbrev-ref\", \"HEAD\"]),\n",
    "        \"status\": git_cmd([\"status\", \"--porcelain\"]),\n",
    "    },\n",
    "}\n",
    "with open(os.path.join(ARTIFACTS, \"env.json\"), \"w\") as f:\n",
    "    json.dump(env, f, indent=2)\n",
    "\n",
    "print(\"=== SpectraMind V50 — Notebook 06 ===\")\n",
    "print(json.dumps(env, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfaff362",
   "metadata": {},
   "source": [
    "## Configuration knobs (Hydra overrides)\n",
    "\n",
    "**Symbolic losses** enabled here:\n",
    "- `nonnegativity` — penalize negative flux/μ\n",
    "- `smoothness` — L2 gradient/curvature prior on μ\n",
    "- `fft_coherence` — spectral structure coherence\n",
    "- `molecular_priors` — optional rule pack (H₂O/CO₂/CH₄ bands)\n",
    "\n",
    "> Start with small weights and increase gradually while monitoring GLL and violation dashboards.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b6be1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os\n",
    "\n",
    "overrides = {\n",
    "    \"loss.symbolic.enable\": \"true\",\n",
    "    \"loss.symbolic.weights.nonnegativity\": \"1.0\",\n",
    "    \"loss.symbolic.weights.smoothness\": \"0.05\",\n",
    "    \"loss.symbolic.weights.fft_coherence\": \"0.10\",\n",
    "    \"loss.symbolic.molecular_priors.enable\": \"true\",\n",
    "    \"loss.symbolic.molecular_priors.pack\": \"default_v1\",\n",
    "    \"diagnostics.symbolic.top_k\": \"12\",\n",
    "    \"training.max_epochs\": \"12\",\n",
    "    \"training.batch_size\": \"16\",\n",
    "    \"data\": \"ariel_nominal\",\n",
    "    \"model\": \"v50\",\n",
    "    \"training.seed\": \"1337\",\n",
    "}\n",
    "\n",
    "cfg_file = os.path.join(CFG_OUT, \"symbolic_overrides.json\")\n",
    "with open(cfg_file, \"w\") as f:\n",
    "    json.dump(overrides, f, indent=2)\n",
    "\n",
    "print(\"Saved overrides ->\", cfg_file)\n",
    "print(json.dumps(overrides, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57828cc",
   "metadata": {},
   "source": [
    "## Helper: robust CLI runner (uses DRY‑RUN when CLI not present)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62873361",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shlex, subprocess, time\n",
    "\n",
    "def run_cli(cmd_list, log_name=\"run\"):\n",
    "    log_path = os.path.join(LOGS, f\"{log_name}.log\")\n",
    "    err_path = os.path.join(LOGS, f\"{log_name}.err\")\n",
    "    start = time.time()\n",
    "    result = {\"cmd\": cmd_list, \"dry_run\": DRY_RUN, \"returncode\": 0, \"stdout\": \"\", \"stderr\": \"\"}\n",
    "    if DRY_RUN:\n",
    "        msg = f\"[DRY-RUN] Would execute: {' '.join(shlex.quote(c) for c in cmd_list)}\\n\"\n",
    "        result[\"stdout\"] = msg\n",
    "        with open(log_path, \"w\") as f: f.write(msg)\n",
    "        with open(err_path, \"w\") as f: f.write(\"\")\n",
    "        placeholder = os.path.join(ARTIFACTS, \"dry_run_placeholder.txt\")\n",
    "        with open(placeholder, \"a\") as f: f.write(msg)\n",
    "        return result\n",
    "\n",
    "    with open(log_path, \"wb\") as out, open(err_path, \"wb\") as err:\n",
    "        try:\n",
    "            proc = subprocess.Popen(cmd_list, stdout=out, stderr=err, env=os.environ.copy())\n",
    "            proc.wait()\n",
    "            result[\"returncode\"] = proc.returncode\n",
    "        except Exception as e:\n",
    "            result[\"returncode\"] = 99\n",
    "            with open(err_path, \"ab\") as errf:\n",
    "                errf.write(str(e).encode())\n",
    "\n",
    "    try:\n",
    "        result[\"stdout\"] = open(log_path, \"r\").read()\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        result[\"stderr\"] = open(err_path, \"r\").read()\n",
    "    except Exception:\n",
    "        pass\n",
    "    result[\"elapsed_sec\"] = round(time.time() - start, 3)\n",
    "    print(f\"[rc={result['returncode']}] logs: {log_path}\")\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dace0d4d",
   "metadata": {},
   "source": [
    "## Train with physics‑informed symbolic constraints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e82dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = [\n",
    "    \"spectramind\", \"train\",\n",
    "    \"--config-name\", \"config_v50.yaml\",\n",
    "    \"+outputs.root_dir=\" + ARTIFACTS,\n",
    "]\n",
    "for k, v in overrides.items():\n",
    "    cmd.append(f\"+{k}={v}\")\n",
    "cmd += [\"+training.fast_mode=true\"]  # if supported\n",
    "\n",
    "res_train = run_cli(cmd, log_name=\"01_train_symbolic\")\n",
    "print(res_train[\"stdout\"][:500])\n",
    "if res_train[\"returncode\"] not in (0, None):\n",
    "    print(\"Training non-zero return code:\", res_train[\"returncode\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66fba25f",
   "metadata": {},
   "source": [
    "## Symbolic diagnostics & rule ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5d2688",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd_diag = [\n",
    "    \"spectramind\", \"diagnose\", \"symbolic-rank\",\n",
    "    \"--top-k\", overrides.get(\"diagnostics.symbolic.top_k\", \"12\"),\n",
    "    \"--export\", DIAG_OUT,\n",
    "]\n",
    "res_diag = run_cli(cmd_diag, log_name=\"02_diagnose_symbolic_rank\")\n",
    "print(res_diag[\"stdout\"][:500])\n",
    "\n",
    "cmd_dash = [\n",
    "    \"spectramind\", \"diagnose\", \"dashboard\",\n",
    "    \"--out\", os.path.join(DIAG_OUT, \"diagnostic_report_v1.html\"),\n",
    "    \"--show-logic-graph\",\n",
    "]\n",
    "res_dash = run_cli(cmd_dash, log_name=\"03_diagnose_dashboard\")\n",
    "print(res_dash[\"stdout\"][:500])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b02999",
   "metadata": {},
   "source": [
    "## Cycle‑consistency: simulate → validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115d1bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare a tiny placeholder μ CSV if none exists (DRY-RUN friendly)\n",
    "mu_csv = os.path.join(ARTIFACTS, \"pred_mu.csv\")\n",
    "if not os.path.exists(mu_csv):\n",
    "    import csv\n",
    "    with open(mu_csv, \"w\", newline=\"\") as f:\n",
    "        w = csv.writer(f)\n",
    "        w.writerow([\"planet_id\"] + [f\"mu_{i:03d}\" for i in range(283)])\n",
    "        for pid in [\"P0001\",\"P0002\",\"P0003\"]:\n",
    "            row = [pid] + [0.0]*283\n",
    "            w.writerow(row)\n",
    "\n",
    "cmd_sim = [\"spectramind\", \"simulate-lightcurve-from-mu\", \"--mu-csv\", mu_csv, \"--out\", SIM_OUT]\n",
    "res_sim = run_cli(cmd_sim, log_name=\"04_simulate_from_mu\")\n",
    "print(res_sim[\"stdout\"][:500])\n",
    "\n",
    "cmd_cc = [\"spectramind\", \"validate\", \"cycle-consistency\",\n",
    "          \"--sim-dir\", SIM_OUT, \"--mu-ref\", mu_csv,\n",
    "          \"--out\", os.path.join(DIAG_OUT, \"cycle_consistency.json\")]\n",
    "res_cc = run_cli(cmd_cc, log_name=\"05_cycle_consistency\")\n",
    "print(res_cc[\"stdout\"][:500])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ac76b5",
   "metadata": {},
   "source": [
    "## Browse produced artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d663ff34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def tree(path, prefix=\"\"):\n",
    "    items = sorted(os.listdir(path))\n",
    "    lines = []\n",
    "    for i, name in enumerate(items):\n",
    "        full = os.path.join(path, name)\n",
    "        connector = \"└── \" if i == len(items)-1 else \"├── \"\n",
    "        lines.append(prefix + connector + name)\n",
    "        if os.path.isdir(full):\n",
    "            extension = \"    \" if i == len(items)-1 else \"│   \"\n",
    "            lines.extend(tree(full, prefix + extension))\n",
    "    return lines\n",
    "\n",
    "print(\"ARTIFACTS TREE:\", ARTIFACTS)\n",
    "print(\"\\n\".join(tree(ARTIFACTS)))\n",
    "dash_path = os.path.join(DIAG_OUT, \"diagnostic_report_v1.html\")\n",
    "print(\"\\nDashboard:\", dash_path if os.path.exists(dash_path) else \"(not found)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41012613",
   "metadata": {},
   "source": [
    "## Pipeline sketch (Mermaid)\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "  A[Calibrated data] --> B[Train with symbolic losses]\n",
    "  B --> C[Predict μ, σ]\n",
    "  C --> D[Symbolic diagnostics<br/>rule ranking & overlays]\n",
    "  C --> E[Simulate lightcurves from μ]\n",
    "  E --> F[Cycle‑consistency validation]\n",
    "  D --> G[Dashboard / Reports]\n",
    "  F --> G\n",
    "```\n",
    "\n",
    "## Next steps\n",
    "- Sweep symbolic weights via `spectramind ablate` and compare GLL vs. violation score.\n",
    "- Enable molecule‑specific prior packs where available; monitor per‑band improvements.\n",
    "- Integrate outputs into your unified HTML report and CI for regression checks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec098122",
   "metadata": {},
   "source": [
    "## 07_huggingface_integration_and_transfer_learning.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca0aa9b",
   "metadata": {},
   "source": [
    "# 🤝 SpectraMind V50 — Hugging Face Integration & Transfer Learning (Notebook 07)\n",
    "\n",
    "**Goal.** Integrate **Hugging Face** models into the SpectraMind V50 pipeline and demonstrate **parameter‑efficient fine‑tuning (PEFT/LoRA)** in a CLI‑first, Hydra‑safe flow. This notebook follows the physics‑informed work in 06 and extends the pipeline with pretrained backbones and transfer learning.\n",
    "\n",
    "**What you’ll do**\n",
    "1. Pre‑flight & environment capture (CLI presence, run IDs)\n",
    "2. HF environment check (Transformers / Accelerate / PEFT) and graceful fallbacks\n",
    "3. Compose Hydra overrides to select a **HF model** (e.g., ViT/TimeSformer) and **LoRA** knobs\n",
    "4. Train via `spectramind train` with HF + PEFT overrides\n",
    "5. Diagnose & compare against the custom SSM+GNN baseline\n",
    "6. Artifact tree, Mermaid sketch, and next steps\n",
    "\n",
    "> The notebook **degrades gracefully**: if `spectramind` or HF libs are not installed, it runs **DRY‑RUN** and still produces configs/logs/placeholder artifacts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db24c30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ░░ Pre-flight: env, run IDs, paths, CLI detection ░░\n",
    "import os, sys, json, platform, shutil, subprocess, datetime, pathlib\n",
    "\n",
    "RUN_TS = datetime.datetime.utcnow().strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "RUN_ID = f\"huggingface_transfer_{RUN_TS}\"\n",
    "ROOT_OUT = \"/mnt/data/hf_transfer\"\n",
    "ARTIFACTS = os.path.join(ROOT_OUT, RUN_ID)\n",
    "LOGS = os.path.join(ARTIFACTS, \"logs\")\n",
    "CFG_OUT = os.path.join(ARTIFACTS, \"configs\")\n",
    "DIAG_OUT = os.path.join(ARTIFACTS, \"diagnostics\")\n",
    "for p in (ROOT_OUT, ARTIFACTS, LOGS, CFG_OUT, DIAG_OUT):\n",
    "    os.makedirs(p, exist_ok=True)\n",
    "\n",
    "def which(cmd: str) -> bool:\n",
    "    return shutil.which(cmd) is not None\n",
    "\n",
    "CLI_PRESENT = which(\"spectramind\")\n",
    "DRY_RUN = not CLI_PRESENT\n",
    "\n",
    "def git_cmd(args):\n",
    "    try:\n",
    "        out = subprocess.check_output([\"git\", *args], stderr=subprocess.STDOUT, timeout=5).decode().strip()\n",
    "        return out\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "env = {\n",
    "    \"python\": sys.version.replace(\"\\n\", \" \"),\n",
    "    \"platform\": platform.platform(),\n",
    "    \"cli_present\": CLI_PRESENT,\n",
    "    \"dry_run\": DRY_RUN,\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"paths\": {\"artifacts\": ARTIFACTS, \"logs\": LOGS, \"configs\": CFG_OUT, \"diagnostics\": DIAG_OUT},\n",
    "    \"git\": {\n",
    "        \"commit\": git_cmd([\"rev-parse\", \"HEAD\"]),\n",
    "        \"branch\": git_cmd([\"rev-parse\", \"--abbrev-ref\", \"HEAD\"]),\n",
    "        \"status\": git_cmd([\"status\", \"--porcelain\"]),\n",
    "    },\n",
    "}\n",
    "with open(os.path.join(ARTIFACTS, \"env.json\"), \"w\") as f:\n",
    "    json.dump(env, f, indent=2)\n",
    "\n",
    "print(\"=== SpectraMind V50 — Notebook 07 ===\")\n",
    "print(json.dumps(env, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d676a5c5",
   "metadata": {},
   "source": [
    "## Hugging Face environment check (Transformers / Accelerate / PEFT)\n",
    "\n",
    "We try importing `transformers`, `accelerate`, and `peft`. If any are missing, we **don’t install** them here (to keep the notebook reproducible/air‑gapped) but proceed in **DRY‑RUN**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828adbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing = []\n",
    "try:\n",
    "    import transformers  # type: ignore\n",
    "    tr_ok = True\n",
    "except Exception:\n",
    "    tr_ok = False\n",
    "    missing.append(\"transformers\")\n",
    "try:\n",
    "    import accelerate  # type: ignore\n",
    "    acc_ok = True\n",
    "except Exception:\n",
    "    acc_ok = False\n",
    "    missing.append(\"accelerate\")\n",
    "try:\n",
    "    import peft  # type: ignore\n",
    "    peft_ok = True\n",
    "except Exception:\n",
    "    peft_ok = False\n",
    "    missing.append(\"peft\")\n",
    "\n",
    "print(\"HF libs — transformers:\", tr_ok, \"| accelerate:\", acc_ok, \"| peft:\", peft_ok)\n",
    "if missing:\n",
    "    print(\"[NOTE] Missing libs:\", \", \".join(missing), \" — continuing with DRY‑RUN semantics where required.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbd9eba",
   "metadata": {},
   "source": [
    "## Compose Hydra overrides: select HF backbone + LoRA\n",
    "\n",
    "Common toggles:\n",
    "- `model=hf_vit` or `model=hf_timesformer` (assumes your repo has these config groups)\n",
    "- `peft.lora.enable=true`, with rank/alpha/dropout as tunables\n",
    "- Batch/epochs kept conservative by default; use ablations later for sweeps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c1d778",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os\n",
    "\n",
    "# Sensible defaults (adjust to your config structure)\n",
    "overrides = {\n",
    "    # Switch to an HF model group (example names; match to your repo’s configs/)\n",
    "    \"model\": \"hf_vit\",                 # or \"hf_timesformer\"\n",
    "    # Enable PEFT/LoRA adapters\n",
    "    \"peft.lora.enable\": \"true\",\n",
    "    \"peft.lora.r\": \"16\",\n",
    "    \"peft.lora.alpha\": \"32\",\n",
    "    \"peft.lora.dropout\": \"0.05\",\n",
    "    # Data and training\n",
    "    \"data\": \"ariel_nominal\",\n",
    "    \"training.max_epochs\": \"6\",\n",
    "    \"training.batch_size\": \"16\",\n",
    "    \"training.seed\": \"1337\",\n",
    "    # Optional: mixed precision / accelerate flags if your code supports these hydra keys\n",
    "    \"training.mixed_precision\": \"fp16\",\n",
    "}\n",
    "\n",
    "cfg_file = os.path.join(CFG_OUT, \"hf_peft_overrides.json\")\n",
    "with open(cfg_file, \"w\") as f:\n",
    "    json.dump(overrides, f, indent=2)\n",
    "print(\"Saved overrides ->\", cfg_file)\n",
    "print(json.dumps(overrides, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f4ff85",
   "metadata": {},
   "source": [
    "## Helper: robust CLI runner (DRY‑RUN when CLI not present)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864e7c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shlex, time\n",
    "\n",
    "def run_cli(cmd_list, log_name=\"run\"):\n",
    "    log_path = os.path.join(LOGS, f\"{log_name}.log\")\n",
    "    err_path = os.path.join(LOGS, f\"{log_name}.err\")\n",
    "    start = time.time()\n",
    "    result = {\"cmd\": cmd_list, \"dry_run\": DRY_RUN, \"returncode\": 0, \"stdout\": \"\", \"stderr\": \"\"}\n",
    "    if DRY_RUN:\n",
    "        msg = f\"[DRY‑RUN] Would execute: {' '.join(shlex.quote(c) for c in cmd_list)}\\n\"\n",
    "        result[\"stdout\"] = msg\n",
    "        with open(log_path, \"w\") as f: f.write(msg)\n",
    "        with open(err_path, \"w\") as f: f.write(\"\")\n",
    "        place = os.path.join(ARTIFACTS, \"dry_run_placeholder.txt\")\n",
    "        with open(place, \"a\") as f: f.write(msg)\n",
    "        return result\n",
    "\n",
    "    with open(log_path, \"wb\") as out, open(err_path, \"wb\") as err:\n",
    "        try:\n",
    "            proc = subprocess.Popen(cmd_list, stdout=out, stderr=err, env=os.environ.copy())\n",
    "            proc.wait()\n",
    "            result[\"returncode\"] = proc.returncode\n",
    "        except Exception as e:\n",
    "            result[\"returncode\"] = 99\n",
    "            with open(err_path, \"ab\") as errf:\n",
    "                errf.write(str(e).encode())\n",
    "\n",
    "    try:\n",
    "        result[\"stdout\"] = open(log_path, \"r\").read()\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        result[\"stderr\"] = open(err_path, \"r\").read()\n",
    "    except Exception:\n",
    "        pass\n",
    "    result[\"elapsed_sec\"] = round(time.time() - start, 3)\n",
    "    print(f\"[rc={result['returncode']}] logs: {log_path}\")\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d2d6eb",
   "metadata": {},
   "source": [
    "## Train with Hugging Face + PEFT/LoRA (CLI‑first)\n",
    "\n",
    "We pass the overrides to `spectramind train` and let Hydra compose the full config. This aligns with the CLI‑first/hydra architecture and reproducibility practices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010eb65f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = [\n",
    "    \"spectramind\", \"train\",\n",
    "    \"--config-name\", \"config_v50.yaml\",\n",
    "    \"+outputs.root_dir=\" + ARTIFACTS,\n",
    "]\n",
    "for k, v in overrides.items():\n",
    "    cmd.append(f\"+{k}={v}\")\n",
    "# Optional fast mode, if supported by your code:\n",
    "cmd += [\"+training.fast_mode=true\"]\n",
    "\n",
    "res_train = run_cli(cmd, log_name=\"01_train_hf_peft\")\n",
    "print(res_train[\"stdout\"][:500])\n",
    "if res_train[\"returncode\"] not in (0, None):\n",
    "    print(\"Training non-zero return code:\", res_train[\"returncode\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50dcabc0",
   "metadata": {},
   "source": [
    "## Diagnostics & comparison vs custom SSM+GNN\n",
    "\n",
    "We run diagnostics and (optionally) a comparison routine (e.g., validation metrics table or overlay plots) to quantify transfer benefits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea11d1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Symbolic / general diagnostics (adjust to your CLI):\n",
    "cmd_diag = [\"spectramind\", \"diagnose\", \"dashboard\",\n",
    "            \"--out\", os.path.join(DIAG_OUT, \"diagnostic_report_hf_v1.html\")]\n",
    "res_diag = run_cli(cmd_diag, log_name=\"02_diagnose_dashboard\")\n",
    "print(res_diag[\"stdout\"][:500])\n",
    "\n",
    "# (Optional) If you have a comparison subcommand:\n",
    "cmd_cmp = [\"spectramind\", \"diagnose\", \"compare\",\n",
    "           \"--models\", \"baseline_v50,hf_vit\",\n",
    "           \"--out\", os.path.join(DIAG_OUT, \"compare_baseline_vs_hf.json\")]\n",
    "res_cmp = run_cli(cmd_cmp, log_name=\"03_diagnose_compare\")\n",
    "print(res_cmp[\"stdout\"][:500])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adf51933",
   "metadata": {},
   "source": [
    "## Browse produced artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c7bcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def tree(path, prefix=\"\"):\n",
    "    items = sorted(os.listdir(path))\n",
    "    lines = []\n",
    "    for i, name in enumerate(items):\n",
    "        full = os.path.join(path, name)\n",
    "        connector = \"└── \" if i == len(items)-1 else \"├── \"\n",
    "        lines.append(prefix + connector + name)\n",
    "        if os.path.isdir(full):\n",
    "            extension = \"    \" if i == len(items)-1 else \"│   \"\n",
    "            lines.extend(tree(full, prefix + extension))\n",
    "    return lines\n",
    "\n",
    "print(\"ARTIFACTS TREE:\", ARTIFACTS)\n",
    "print(\"\\n\".join(tree(ARTIFACTS)))\n",
    "dash_path = os.path.join(DIAG_OUT, \"diagnostic_report_hf_v1.html\")\n",
    "print(\"\\nDashboard:\", dash_path if os.path.exists(dash_path) else \"(not found)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc6eb70",
   "metadata": {},
   "source": [
    "## Pipeline sketch (Mermaid)\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "  A[Pretrained HF backbone] --> B[PEFT/LoRA fine‑tune (Hydra)]\n",
    "  B --> C[Predict μ, σ]\n",
    "  C --> D[Diagnostics & overlays]\n",
    "  D --> E[Compare vs SSM+GNN baseline]\n",
    "  E --> F[Report / CI]\n",
    "```\n",
    "\n",
    "## Next steps\n",
    "- Try **TimeSformer** or additional HF backbones; tune LoRA rank/alpha/dropout via `spectramind ablate`.\n",
    "- Enable **Accelerate** multi‑GPU or mixed precision for faster runs if supported by your config.\n",
    "- Publish the fine‑tuned model to a private registry (or Hugging Face Hub) for controlled reuse.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b511da9c",
   "metadata": {},
   "source": [
    "## 08_ensembles_mc_dropout_and_corel_conformal.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a8bab0",
   "metadata": {},
   "source": [
    "# 📈 SpectraMind V50 — Ensembles, MC Dropout & COREL Conformal (Notebook 08)\n",
    "\n",
    "**Goal.** Add **epistemic uncertainty** and **coverage-guaranteed intervals** on top of per-bin σ by:\n",
    "- Training **ensembles** (multi-seed or multi-arch)\n",
    "- Enabling **MC Dropout** at inference\n",
    "- Running **post-hoc calibration** (temperature scaling; optional per-wavelength)\n",
    "- Applying **COREL conformal prediction** for graph-aware coverage\n",
    "\n",
    "**Workflow**\n",
    "1. Pre-flight & environment capture\n",
    "2. Hydra overrides for ensembles, MC dropout, calibration, and COREL\n",
    "3. Train K ensemble members (looped CLI calls)\n",
    "4. Predict per member; aggregate mean/variance\n",
    "5. Temperature scaling (post-hoc)\n",
    "6. COREL conformal calibration & coverage evaluation\n",
    "7. Diagnostics dashboard + artifact tree\n",
    "\n",
    "> As with earlier notebooks, this is **CLI-first & Hydra-safe** with **DRY-RUN** fallbacks when `spectramind` is unavailable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903f09bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ░░ Pre-flight: env, run IDs, paths, CLI detection ░░\n",
    "import os, sys, json, platform, shutil, subprocess, datetime, pathlib\n",
    "\n",
    "RUN_TS = datetime.datetime.utcnow().strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "RUN_ID = f\"uq_conformal_{RUN_TS}\"\n",
    "ROOT_OUT = \"/mnt/data/uq_conformal\"\n",
    "ARTIFACTS = os.path.join(ROOT_OUT, RUN_ID)\n",
    "LOGS = os.path.join(ARTIFACTS, \"logs\")\n",
    "CFG_OUT = os.path.join(ARTIFACTS, \"configs\")\n",
    "PRED_OUT = os.path.join(ARTIFACTS, \"predictions\")\n",
    "DIAG_OUT = os.path.join(ARTIFACTS, \"diagnostics\")\n",
    "for p in (ROOT_OUT, ARTIFACTS, LOGS, CFG_OUT, PRED_OUT, DIAG_OUT):\n",
    "    os.makedirs(p, exist_ok=True)\n",
    "\n",
    "def which(cmd: str) -> bool:\n",
    "    return shutil.which(cmd) is not None\n",
    "\n",
    "CLI_PRESENT = which(\"spectramind\")\n",
    "DRY_RUN = not CLI_PRESENT\n",
    "\n",
    "def git_cmd(args):\n",
    "    try:\n",
    "        out = subprocess.check_output([\"git\", *args], stderr=subprocess.STDOUT, timeout=5).decode().strip()\n",
    "        return out\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "env = {\n",
    "    \"python\": sys.version.replace(\"\\n\", \" \"),\n",
    "    \"platform\": platform.platform(),\n",
    "    \"cli_present\": CLI_PRESENT,\n",
    "    \"dry_run\": DRY_RUN,\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"paths\": {\"artifacts\": ARTIFACTS, \"logs\": LOGS, \"configs\": CFG_OUT, \"predictions\": PRED_OUT, \"diagnostics\": DIAG_OUT},\n",
    "    \"git\": {\n",
    "        \"commit\": git_cmd([\"rev-parse\", \"HEAD\"]),\n",
    "        \"branch\": git_cmd([\"rev-parse\", \"--abbrev-ref\", \"HEAD\"]),\n",
    "        \"status\": git_cmd([\"status\", \"--porcelain\"]),\n",
    "    },\n",
    "}\n",
    "with open(os.path.join(ARTIFACTS, \"env.json\"), \"w\") as f:\n",
    "    json.dump(env, f, indent=2)\n",
    "\n",
    "print(\"=== SpectraMind V50 — Notebook 08 ===\")\n",
    "print(json.dumps(env, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8532f66d",
   "metadata": {},
   "source": [
    "## Compose Hydra overrides for Ensembles, MC Dropout, Calibration & COREL\n",
    "\n",
    "**Toggles (adjust to your repo’s `configs/`):**\n",
    "- `uq.ensemble.enable=true`, `uq.ensemble.size=5`\n",
    "- `uq.mc_dropout.enable=true`, with `keep_prob` or `p`, and `mc_samples=30`\n",
    "- `calibration.temperature.enable=true` (optionally per-wavelength)\n",
    "- `conformal.corel.enable=true`, e.g., `coverage=0.9`, graph spec for AIRS bins\n",
    "\n",
    "We keep epochs small for demo; run ablations later for full sweeps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4683ecdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os\n",
    "\n",
    "overrides = {\n",
    "    # Base runtime\n",
    "    \"data\": \"ariel_nominal\",\n",
    "    \"model\": \"v50\",                 # swap with 'hf_vit' or others if desired\n",
    "    \"training.max_epochs\": \"6\",\n",
    "    \"training.batch_size\": \"16\",\n",
    "    \"training.seed\": \"1337\",\n",
    "    # Ensemble\n",
    "    \"uq.ensemble.enable\": \"true\",\n",
    "    \"uq.ensemble.size\": \"5\",\n",
    "    # MC Dropout\n",
    "    \"uq.mc_dropout.enable\": \"true\",\n",
    "    \"uq.mc_dropout.p\": \"0.1\",\n",
    "    \"uq.mc_dropout.mc_samples\": \"30\",\n",
    "    # Temperature scaling\n",
    "    \"calibration.temperature.enable\": \"true\",\n",
    "    \"calibration.temperature.per_wavelength\": \"false\",\n",
    "    # COREL conformal\n",
    "    \"conformal.corel.enable\": \"true\",\n",
    "    \"conformal.corel.coverage\": \"0.90\",\n",
    "    \"conformal.corel.graph\": \"airs_default\",\n",
    "    # Mixed precision (optional)\n",
    "    \"training.mixed_precision\": \"fp16\",\n",
    "}\n",
    "\n",
    "cfg_file = os.path.join(CFG_OUT, \"uq_corel_overrides.json\")\n",
    "with open(cfg_file, \"w\") as f:\n",
    "    json.dump(overrides, f, indent=2)\n",
    "print(\"Saved overrides ->\", cfg_file)\n",
    "print(json.dumps(overrides, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a83cc8f",
   "metadata": {},
   "source": [
    "## Helper: robust CLI runner (DRY-RUN when CLI not present)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28cc04f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shlex, time\n",
    "\n",
    "def run_cli(cmd_list, log_name=\"run\"):\n",
    "    log_path = os.path.join(LOGS, f\"{log_name}.log\")\n",
    "    err_path = os.path.join(LOGS, f\"{log_name}.err\")\n",
    "    start = time.time()\n",
    "    result = {\"cmd\": cmd_list, \"dry_run\": DRY_RUN, \"returncode\": 0, \"stdout\": \"\", \"stderr\": \"\"}\n",
    "    if DRY_RUN:\n",
    "        msg = f\"[DRY-RUN] Would execute: {' '.join(shlex.quote(c) for c in cmd_list)}\\n\"\n",
    "        result[\"stdout\"] = msg\n",
    "        with open(log_path, \"w\") as f: f.write(msg)\n",
    "        with open(err_path, \"w\") as f: f.write(\"\")\n",
    "        placeholder = os.path.join(ARTIFACTS, \"dry_run_placeholder.txt\")\n",
    "        with open(placeholder, \"a\") as f: f.write(msg)\n",
    "        return result\n",
    "\n",
    "    with open(log_path, \"wb\") as out, open(err_path, \"wb\") as err:\n",
    "        try:\n",
    "            proc = subprocess.Popen(cmd_list, stdout=out, stderr=err, env=os.environ.copy())\n",
    "            proc.wait()\n",
    "            result[\"returncode\"] = proc.returncode\n",
    "        except Exception as e:\n",
    "            result[\"returncode\"] = 99\n",
    "            with open(err_path, \"ab\") as errf:\n",
    "                errf.write(str(e).encode())\n",
    "\n",
    "    try:\n",
    "        result[\"stdout\"] = open(log_path, \"r\").read()\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        result[\"stderr\"] = open(err_path, \"r\").read()\n",
    "    except Exception:\n",
    "        pass\n",
    "    result[\"elapsed_sec\"] = round(time.time() - start, 3)\n",
    "    print(f\"[rc={result['returncode']}] logs: {log_path}\")\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a85608",
   "metadata": {},
   "source": [
    "## Train **K** ensemble members"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529ff884",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = int(overrides.get(\"uq.ensemble.size\", \"5\"))\n",
    "base_seed = int(overrides.get(\"training.seed\", \"1337\"))\n",
    "run_dirs = []\n",
    "\n",
    "for k in range(K):\n",
    "    seed = base_seed + k\n",
    "    run_name = f\"member_{k:02d}_seed_{seed}\"\n",
    "    out_dir = os.path.join(PRED_OUT, run_name)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    run_dirs.append(out_dir)\n",
    "\n",
    "    cmd = [\"spectramind\", \"train\",\n",
    "           \"--config-name\", \"config_v50.yaml\",\n",
    "           \"+outputs.root_dir=\" + out_dir,\n",
    "           f\"+training.seed={seed}\"]\n",
    "    for key, val in overrides.items():\n",
    "        cmd.append(f\"+{key}={val}\")\n",
    "    cmd += [\"+training.fast_mode=true\"]\n",
    "    res = run_cli(cmd, log_name=f\"01_train_{run_name}\")\n",
    "    print(res[\"stdout\"][:300])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d0aecb",
   "metadata": {},
   "source": [
    "## Predict for each member and **aggregate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d703d00c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dirs = []\n",
    "for out_dir in run_dirs:\n",
    "    pred_dir = os.path.join(out_dir, \"pred\")\n",
    "    os.makedirs(pred_dir, exist_ok=True)\n",
    "    pred_dirs.append(pred_dir)\n",
    "    cmd_pred = [\"spectramind\", \"predict\",\n",
    "                \"--config-name\", \"config_v50.yaml\",\n",
    "                \"+load.from_checkpoint=true\",\n",
    "                \"+outputs.root_dir=\" + pred_dir]\n",
    "    for key, val in overrides.items():\n",
    "        cmd_pred.append(f\"+{key}={val}\")\n",
    "    cmd_pred += [\"+inference.save_mu_sigma=true\"]\n",
    "    res_p = run_cli(cmd_pred, log_name=f\"02_predict_{os.path.basename(out_dir)}\")\n",
    "    print(res_p[\"stdout\"][:300])\n",
    "\n",
    "agg_dir = os.path.join(PRED_OUT, \"ensemble_aggregate\")\n",
    "os.makedirs(agg_dir, exist_ok=True)\n",
    "cmd_agg = [\"spectramind\", \"diagnose\", \"ensemble-aggregate\",\n",
    "           \"--inputs\"] + pred_dirs + [\"--out\", agg_dir]\n",
    "res_agg = run_cli(cmd_agg, log_name=\"03_ensemble_aggregate\")\n",
    "print(res_agg[\"stdout\"][:300])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60858634",
   "metadata": {},
   "source": [
    "## Temperature scaling (post-hoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c68a1f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_dir = os.path.join(DIAG_OUT, \"temperature_scaling\")\n",
    "os.makedirs(cal_dir, exist_ok=True)\n",
    "cmd_ts = [\"spectramind\", \"calibrate\", \"temperature\",\n",
    "          \"--pred\", os.path.join(PRED_OUT, \"ensemble_aggregate\"),\n",
    "          \"--out\", cal_dir,\n",
    "          \"--mode\", \"global\"]\n",
    "res_ts = run_cli(cmd_ts, log_name=\"04_temperature_scaling\")\n",
    "print(res_ts[\"stdout\"][:300])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05feef6c",
   "metadata": {},
   "source": [
    "## COREL conformal calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aafd40f",
   "metadata": {},
   "outputs": [],
   "source": [
    "corel_dir = os.path.join(DIAG_OUT, \"corel\")\n",
    "os.makedirs(corel_dir, exist_ok=True)\n",
    "cmd_corel_fit = [\"spectramind\", \"conformal\", \"corel-fit\",\n",
    "                 \"--pred\", os.path.join(PRED_OUT, \"ensemble_aggregate\"),\n",
    "                 \"--out\", corel_dir,\n",
    "                 \"--coverage\", overrides.get(\"conformal.corel.coverage\", \"0.90\"),\n",
    "                 \"--graph\", overrides.get(\"conformal.corel.graph\", \"airs_default\")]\n",
    "res_corel_fit = run_cli(cmd_corel_fit, log_name=\"05_corel_fit\")\n",
    "print(res_corel_fit[\"stdout\"][:300])\n",
    "\n",
    "cmd_corel_eval = [\"spectramind\", \"conformal\", \"corel-eval\",\n",
    "                  \"--model\", corel_dir, \"--out\", os.path.join(corel_dir, \"eval.json\")]\n",
    "res_corel_eval = run_cli(cmd_corel_eval, log_name=\"06_corel_eval\")\n",
    "print(res_corel_eval[\"stdout\"][:300])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5c922fe",
   "metadata": {},
   "source": [
    "## Diagnostics dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2890121e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dash_path = os.path.join(DIAG_OUT, \"diagnostic_report_uq_corel_v1.html\")\n",
    "cmd_dash = [\"spectramind\", \"diagnose\", \"dashboard\",\n",
    "            \"--out\", dash_path,\n",
    "            \"--include-coverage\", \"true\"]\n",
    "res_dash = run_cli(cmd_dash, log_name=\"07_dashboard\")\n",
    "print(res_dash[\"stdout\"][:300])\n",
    "print(\"Dashboard:\", dash_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7695c681",
   "metadata": {},
   "source": [
    "## Browse produced artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7adb8ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def tree(path, prefix=\"\"):\n",
    "    items = sorted(os.listdir(path))\n",
    "    lines = []\n",
    "    for i, name in enumerate(items):\n",
    "        full = os.path.join(path, name)\n",
    "        connector = \"└── \" if i == len(items)-1 else \"├── \"\n",
    "        lines.append(prefix + connector + name)\n",
    "        if os.path.isdir(full):\n",
    "            extension = \"    \" if i == len(items)-1 else \"│   \"\n",
    "            lines.extend(tree(full, prefix + extension))\n",
    "    return lines\n",
    "\n",
    "print(\"ARTIFACTS TREE:\", ARTIFACTS)\n",
    "print(\"\\n\".join(tree(ARTIFACTS)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92468af1",
   "metadata": {},
   "source": [
    "## Pipeline sketch (Mermaid)\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "  A[Train K ensemble members] --> B[Predict each + MC Dropout]\n",
    "  B --> C[Aggregate μ,σ; compute epistemic variance]\n",
    "  C --> D[Temp scaling (post-hoc)]\n",
    "  D --> E[COREL conformal fit + eval (coverage)]\n",
    "  E --> F[Diagnostics dashboard / CI]\n",
    "```\n",
    "\n",
    "## Next steps\n",
    "- Sweep **ensemble size, MC samples, dropout p**, and **coverage** via `spectramind ablate`.\n",
    "- Try **per-wavelength** temperature scaling (if supported) and compare GLL/coverage.\n",
    "- Export JSON summaries for CI gating and archive HTML in versioned reports.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c634a5",
   "metadata": {},
   "source": [
    "## 09_experiment_tracking_dvc_ci.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ab5b41",
   "metadata": {},
   "source": [
    "# 🧪 SpectraMind V50 — Experiment Tracking, DVC DAG & CI (Notebook 09)\n",
    "\n",
    "**Purpose.** Close the reproducibility loop by wiring **experiment tracking**, **DVC pipeline/DAG**, and **CI artifacts**:\n",
    "- Track runs (config → metrics → artifacts) with **MLflow** (or structured JSON fallback).\n",
    "- Define/validate a **DVC DAG** for calibration → train → predict → diagnostics stages.\n",
    "- Emit CI-friendly artifacts (JSON summaries, HTML reports) and run **self-test** hooks.\n",
    "\n",
    "This continues the sequence after 08 (ensembles/MC Dropout/COREL) and focuses on **engineering rigor**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600067cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ░░ Pre-flight: env, run IDs, paths, CLI detection ░░\n",
    "import os, sys, json, platform, shutil, subprocess, datetime, pathlib\n",
    "\n",
    "RUN_TS = datetime.datetime.utcnow().strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "RUN_ID = f\"tracking_dvc_ci_{RUN_TS}\"\n",
    "ROOT_OUT = \"/mnt/data/tracking_dvc_ci\"\n",
    "ARTIFACTS = os.path.join(ROOT_OUT, RUN_ID)\n",
    "LOGS = os.path.join(ARTIFACTS, \"logs\")\n",
    "CFG_OUT = os.path.join(ARTIFACTS, \"configs\")\n",
    "CI_OUT = os.path.join(ARTIFACTS, \"ci_artifacts\")\n",
    "MLF_OUT = os.path.join(ARTIFACTS, \"mlruns\")  # default local store if mlflow used\n",
    "for p in (ROOT_OUT, ARTIFACTS, LOGS, CFG_OUT, CI_OUT, MLF_OUT):\n",
    "    os.makedirs(p, exist_ok=True)\n",
    "\n",
    "def which(cmd: str) -> bool:\n",
    "    return shutil.which(cmd) is not None\n",
    "\n",
    "CLI_PRESENT = which(\"spectramind\")\n",
    "DRY_RUN = not CLI_PRESENT\n",
    "\n",
    "def git_cmd(args):\n",
    "    try:\n",
    "        out = subprocess.check_output([\"git\", *args], stderr=subprocess.STDOUT, timeout=5).decode().strip()\n",
    "        return out\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "env = {\n",
    "    \"python\": sys.version.replace(\"\\n\", \" \"),\n",
    "    \"platform\": platform.platform(),\n",
    "    \"cli_present\": CLI_PRESENT,\n",
    "    \"dry_run\": DRY_RUN,\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"paths\": {\"artifacts\": ARTIFACTS, \"logs\": LOGS, \"configs\": CFG_OUT, \"ci\": CI_OUT, \"mlruns\": MLF_OUT},\n",
    "    \"git\": {\n",
    "        \"commit\": git_cmd([\"rev-parse\", \"HEAD\"]),\n",
    "        \"branch\": git_cmd([\"rev-parse\", \"--abbrev-ref\", \"HEAD\"]),\n",
    "        \"status\": git_cmd([\"status\", \"--porcelain\"]),\n",
    "    },\n",
    "}\n",
    "with open(os.path.join(ARTIFACTS, \"env.json\"), \"w\") as f:\n",
    "    json.dump(env, f, indent=2)\n",
    "\n",
    "print(\"=== SpectraMind V50 — Notebook 09 ===\")\n",
    "print(json.dumps(env, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddf2cf8",
   "metadata": {},
   "source": [
    "## Experiment Tracking: MLflow (with structured JSON fallback)\n",
    "\n",
    "We attempt to use **MLflow**; if unavailable, we record runs in `events.jsonl` + `summary.json` so CI can ingest metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d89463a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "MLFLOW_OK = False\n",
    "try:\n",
    "    import mlflow  # type: ignore\n",
    "    MLFLOW_OK = True\n",
    "except Exception:\n",
    "    MLFLOW_OK = False\n",
    "\n",
    "print(\"MLflow available:\", MLFLOW_OK)\n",
    "EVENTS = os.path.join(ARTIFACTS, \"events.jsonl\")\n",
    "SUMMARY = os.path.join(ARTIFACTS, \"summary.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b5dcc8",
   "metadata": {},
   "source": [
    "## Compose Hydra overrides for a small demo run\n",
    "\n",
    "We keep it light here; use your ablation tooling for full sweeps. The goal is to demonstrate **tracked runs**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428a200e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os\n",
    "\n",
    "overrides = {\n",
    "    \"data\": \"ariel_nominal\",\n",
    "    \"model\": \"v50\",\n",
    "    \"training.max_epochs\": \"3\",\n",
    "    \"training.batch_size\": \"16\",\n",
    "    \"training.seed\": \"2025\",\n",
    "    # Diagnostics toggles for predictable outputs\n",
    "    \"diagnostics.export_json\": \"true\",\n",
    "}\n",
    "cfg_file = os.path.join(CFG_OUT, \"tracking_overrides.json\")\n",
    "with open(cfg_file, \"w\") as f:\n",
    "    json.dump(overrides, f, indent=2)\n",
    "print(\"Saved overrides ->\", cfg_file)\n",
    "print(json.dumps(overrides, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47bda63d",
   "metadata": {},
   "source": [
    "## Helper: robust CLI runner (DRY-RUN if CLI not present)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f2209d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shlex, time\n",
    "\n",
    "def run_cli(cmd_list, log_name=\"run\"):\n",
    "    log_path = os.path.join(LOGS, f\"{log_name}.log\")\n",
    "    err_path = os.path.join(LOGS, f\"{log_name}.err\")\n",
    "    start = time.time()\n",
    "    result = {\"cmd\": cmd_list, \"dry_run\": DRY_RUN, \"returncode\": 0, \"stdout\": \"\", \"stderr\": \"\"}\n",
    "    if DRY_RUN:\n",
    "        msg = f\"[DRY-RUN] Would execute: {' '.join(shlex.quote(c) for c in cmd_list)}\\n\"\n",
    "        result[\"stdout\"] = msg\n",
    "        with open(log_path, \"w\") as f: f.write(msg)\n",
    "        with open(err_path, \"w\") as f: f.write(\"\")\n",
    "        with open(os.path.join(ARTIFACTS, \"events.jsonl\"), \"a\") as ev:\n",
    "            ev.write(json.dumps({\"ts\": RUN_TS, \"event\": \"dry_run\", \"cmd\": result[\"cmd\"]}) + \"\\n\")\n",
    "        return result\n",
    "\n",
    "    with open(log_path, \"wb\") as out, open(err_path, \"wb\") as err:\n",
    "        try:\n",
    "            proc = subprocess.Popen(cmd_list, stdout=out, stderr=err, env=os.environ.copy())\n",
    "            proc.wait()\n",
    "            result[\"returncode\"] = proc.returncode\n",
    "        except Exception as e:\n",
    "            result[\"returncode\"] = 99\n",
    "            with open(err_path, \"ab\") as errf:\n",
    "                errf.write(str(e).encode())\n",
    "\n",
    "    try: result[\"stdout\"] = open(log_path, \"r\").read()\n",
    "    except Exception: pass\n",
    "    try: result[\"stderr\"] = open(err_path, \"r\").read()\n",
    "    except Exception: pass\n",
    "    result[\"elapsed_sec\"] = round(time.time() - start, 3)\n",
    "    print(f\"[rc={result['returncode']}] logs: {log_path}\")\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18487aff",
   "metadata": {},
   "source": [
    "## Tracked demo run: `train → predict → diagnose`\n",
    "\n",
    "- If MLflow is present, we start a run, log params/metrics/artifacts.\n",
    "- Otherwise, we append to `events.jsonl` and assemble `summary.json` for CI.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870066c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid, time, json, os, glob\n",
    "\n",
    "run_uid = str(uuid.uuid4())\n",
    "\n",
    "def log_fallback(record: dict):\n",
    "    with open(EVENTS, \"a\") as ev:\n",
    "        ev.write(json.dumps(record) + \"\\n\")\n",
    "\n",
    "# Start run\n",
    "if MLFLOW_OK:\n",
    "    mlflow.set_tracking_uri(\"file://\" + os.path.abspath(MLF_OUT))\n",
    "    mlflow.set_experiment(\"spectramind_v50\")\n",
    "    mlflow.start_run(run_name=f\"demo_{run_uid}\")\n",
    "    mlflow.log_params(overrides)\n",
    "else:\n",
    "    log_fallback({\"event\": \"start_run\", \"run_id\": run_uid, \"params\": overrides, \"ts\": time.time()})\n",
    "\n",
    "# train\n",
    "cmd_train = [\"spectramind\", \"train\", \"--config-name\", \"config_v50.yaml\", \"+outputs.root_dir=\" + ARTIFACTS]\n",
    "for k, v in overrides.items():\n",
    "    cmd_train.append(f\"+{k}={v}\")\n",
    "res_tr = run_cli(cmd_train, log_name=\"01_train\")\n",
    "if MLFLOW_OK:\n",
    "    mlflow.log_text(res_tr[\"stdout\"][:1000], \"logs/train_head.txt\")\n",
    "else:\n",
    "    log_fallback({\"event\": \"train_done\", \"rc\": res_tr[\"returncode\"], \"ts\": time.time()})\n",
    "\n",
    "# predict\n",
    "pred_dir = os.path.join(ARTIFACTS, \"pred\")\n",
    "os.makedirs(pred_dir, exist_ok=True)\n",
    "cmd_pred = [\"spectramind\", \"predict\", \"--config-name\", \"config_v50.yaml\", \"+load.from_checkpoint=true\", \"+outputs.root_dir=\" + pred_dir]\n",
    "for k, v in overrides.items():\n",
    "    cmd_pred.append(f\"+{k}={v}\")\n",
    "cmd_pred += [\"+inference.save_mu_sigma=true\"]\n",
    "res_pr = run_cli(cmd_pred, log_name=\"02_predict\")\n",
    "if MLFLOW_OK:\n",
    "    mlflow.log_text(res_pr[\"stdout\"][:1000], \"logs/predict_head.txt\")\n",
    "else:\n",
    "    log_fallback({\"event\": \"predict_done\", \"rc\": res_pr[\"returncode\"], \"ts\": time.time()})\n",
    "\n",
    "# diagnose dashboard\n",
    "diag_dir = os.path.join(ARTIFACTS, \"diagnostics\")\n",
    "os.makedirs(diag_dir, exist_ok=True)\n",
    "dashboard = os.path.join(diag_dir, \"diagnostic_report_v1.html\")\n",
    "cmd_diag = [\"spectramind\", \"diagnose\", \"dashboard\", \"--out\", dashboard]\n",
    "res_dg = run_cli(cmd_diag, log_name=\"03_diagnose\")\n",
    "if MLFLOW_OK:\n",
    "    if os.path.exists(dashboard):\n",
    "        mlflow.log_artifact(dashboard, artifact_path=\"reports\")\n",
    "    mlflow.log_text(res_dg[\"stdout\"][:1000], \"logs/diagnose_head.txt\")\n",
    "else:\n",
    "    log_fallback({\"event\": \"diagnose_done\", \"rc\": res_dg[\"returncode\"], \"dashboard\": dashboard, \"ts\": time.time()})\n",
    "\n",
    "# Finish run\n",
    "if MLFLOW_OK:\n",
    "    # Example metric emit (replace with real metrics from diagnostics JSON if available)\n",
    "    mlflow.log_metric(\"demo_metric_gll\", 0.0)\n",
    "    mlflow.end_run()\n",
    "else:\n",
    "    with open(SUMMARY, \"w\") as f:\n",
    "        json.dump({\"run_id\": run_uid, \"metrics\": {\"demo_metric_gll\": 0.0}}, f, indent=2)\n",
    "print(\"Tracking complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3865415e",
   "metadata": {},
   "source": [
    "## DVC DAG: emit stage templates\n",
    "\n",
    "We write **DVC stage YAML fragments** for `calibrate → train → predict → diagnose` so you can paste or integrate into `dvc.yaml`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c5bf5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dvc_frag = os.path.join(ARTIFACTS, \"dvc_stages.yaml\")\n",
    "frag = f\"\"\"\n",
    "stages:\n",
    "  calibrate:\n",
    "    cmd: spectramind calibrate --config-name config_v50.yaml\n",
    "    deps:\n",
    "    - configs\n",
    "    outs:\n",
    "    - data/calibrated\n",
    "  train:\n",
    "    cmd: spectramind train --config-name config_v50.yaml\n",
    "    deps:\n",
    "    - data/calibrated\n",
    "    - configs\n",
    "    outs:\n",
    "    - models/v50_checkpoint\n",
    "  predict:\n",
    "    cmd: spectramind predict --config-name config_v50.yaml +load.from_checkpoint=true\n",
    "    deps:\n",
    "    - models/v50_checkpoint\n",
    "    outs:\n",
    "    - outputs/predictions\n",
    "  diagnose:\n",
    "    cmd: spectramind diagnose dashboard --out outputs/diagnostic_report_v1.html\n",
    "    deps:\n",
    "    - outputs/predictions\n",
    "    outs:\n",
    "    - outputs/diagnostic_report_v1.html\n",
    "\"\"\"\n",
    "open(dvc_frag, \"w\").write(frag)\n",
    "print(\"Wrote DVC stage template ->\", dvc_frag)\n",
    "print(open(dvc_frag).read())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabda2d9",
   "metadata": {},
   "source": [
    "## CI artifacts: package JSON summaries & HTML dashboard\n",
    "\n",
    "We copy key files to the `ci_artifacts/` folder so a CI job can always upload them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c45e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil, os, glob\n",
    "\n",
    "# Collect likely artifacts if present; tolerate absence in DRY-RUN\n",
    "maybe = []\n",
    "for path in [\n",
    "    os.path.join(ARTIFACTS, \"diagnostics\", \"diagnostic_report_v1.html\"),\n",
    "    os.path.join(ARTIFACTS, \"summary.json\"),\n",
    "    os.path.join(ARTIFACTS, \"events.jsonl\"),\n",
    "]:\n",
    "    if os.path.exists(path):\n",
    "        maybe.append(path)\n",
    "\n",
    "for pth in maybe:\n",
    "    shutil.copy2(pth, CI_OUT)\n",
    "\n",
    "print(\"CI bundle contains:\", os.listdir(CI_OUT))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b267ee",
   "metadata": {},
   "source": [
    "## Browse produced artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461aa85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def tree(path, prefix=\"\"):\n",
    "    items = sorted(os.listdir(path))\n",
    "    lines = []\n",
    "    for i, name in enumerate(items):\n",
    "        full = os.path.join(path, name)\n",
    "        connector = \"└── \" if i == len(items)-1 else \"├── \"\n",
    "        lines.append(prefix + connector + name)\n",
    "        if os.path.isdir(full):\n",
    "            extension = \"    \" if i == len(items)-1 else \"│   \"\n",
    "            lines.extend(tree(full, prefix + extension))\n",
    "    return lines\n",
    "\n",
    "print(\"ARTIFACTS TREE:\", ARTIFACTS)\n",
    "print(\"\\n\".join(tree(ARTIFACTS)))\n",
    "print(\"CI artifacts:\", os.listdir(os.path.join(ARTIFACTS, \"ci_artifacts\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0125484b",
   "metadata": {},
   "source": [
    "## Pipeline sketch (Mermaid)\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "  A[Hydra config] --> B[Calibrate]\n",
    "  B --> C[Train (tracked)]\n",
    "  C --> D[Predict]\n",
    "  D --> E[Diagnose → HTML]\n",
    "  C --> F[Metrics JSON/MLflow]\n",
    "  E --> G[CI Upload]\n",
    "  F --> G\n",
    "```\n",
    "\n",
    "## Next steps\n",
    "- Wire **real metrics** from diagnostics JSON into MLflow/summary outputs.\n",
    "- Paste `dvc_stages.yaml` content into `dvc.yaml` and `dvc repro` to validate the DAG on small data.\n",
    "- Hook this notebook (or the CLI) into **CI** to auto-run `selftest` + produce a **CI bundle** on every PR.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36529038",
   "metadata": {},
   "source": [
    "---\n",
    "# 🔗 Additional Stitched Modules\n",
    "Imported after your latest upload."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e852fb55",
   "metadata": {},
   "source": [
    "## 10_full_pipeline_reproducibility_and_ci.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999ffe13",
   "metadata": {},
   "source": [
    "# 🔁 SpectraMind V50 — Full Pipeline Reproducibility & CI (Notebook 10)\n",
    "\n",
    "**Goal.** Execute the **entire pipeline end‑to‑end** in a controlled, CLI‑first flow; capture **exact config + data + code provenance**; and emit a **reproducibility manifest** compatible with CI.\n",
    "\n",
    "**What this notebook does**\n",
    "1. Pre‑flight & environment capture (CLI/DVC detection, git info, run ID)\n",
    "2. DVC quick status (cache/stage checks) with graceful fallback\n",
    "3. End‑to‑end pipeline run (calibrate → train → diagnose → submit) with **DRY‑RUN fallback** if `spectramind` is not available\n",
    "4. Create a **reproducibility manifest** (commit hash, config snapshot if available, artifact hashes)\n",
    "5. CI smoke‑test hooks (idempotent rerun check; basic integrity assertions)\n",
    "6. Artifact tree + next steps\n",
    "\n",
    "> As in earlier notebooks, everything is **CLI‑first, Hydra‑safe**, and will **degrade gracefully** if local tools are not present.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0906e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ░░ Pre-flight: detect tools, set paths, capture git/env ░░\n",
    "import os, sys, json, shutil, subprocess, datetime, pathlib\n",
    "\n",
    "RUN_TS = datetime.datetime.utcnow().strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "RUN_ID = f\"full_pipeline_ci_{RUN_TS}\"\n",
    "ROOT_OUT = \"/mnt/data/full_pipeline_ci\"\n",
    "ARTIFACTS = os.path.join(ROOT_OUT, RUN_ID)\n",
    "LOGS = os.path.join(ARTIFACTS, \"logs\")\n",
    "CFG_OUT = os.path.join(ARTIFACTS, \"configs\")\n",
    "DIAG_OUT = os.path.join(ARTIFACTS, \"diagnostics\")\n",
    "SUBMIT_OUT = os.path.join(ARTIFACTS, \"submission\")\n",
    "for p in (ROOT_OUT, ARTIFACTS, LOGS, CFG_OUT, DIAG_OUT, SUBMIT_OUT):\n",
    "    os.makedirs(p, exist_ok=True)\n",
    "\n",
    "def which(cmd:str)->bool: return shutil.which(cmd) is not None\n",
    "CLI_PRESENT = which(\"spectramind\")\n",
    "DVC_PRESENT = which(\"dvc\")\n",
    "\n",
    "def git_cmd(args):\n",
    "    try:\n",
    "        out = subprocess.check_output([\"git\", *args], stderr=subprocess.STDOUT, timeout=5).decode().strip()\n",
    "        return out\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "env = {\n",
    "    \"python\": sys.version.replace(\"\\n\",\" \"),\n",
    "    \"platform\": sys.platform,\n",
    "    \"cli_present\": CLI_PRESENT,\n",
    "    \"dvc_present\": DVC_PRESENT,\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"paths\": {\"artifacts\": ARTIFACTS, \"logs\": LOGS, \"configs\": CFG_OUT, \"diagnostics\": DIAG_OUT, \"submission\": SUBMIT_OUT},\n",
    "    \"git\": {\n",
    "        \"commit\": git_cmd([\"rev-parse\", \"HEAD\"]),\n",
    "        \"branch\": git_cmd([\"rev-parse\", \"--abbrev-ref\", \"HEAD\"]),\n",
    "        \"status\": git_cmd([\"status\", \"--porcelain\"]),\n",
    "        \"remote\": git_cmd([\"remote\", \"-v\"]),\n",
    "    },\n",
    "}\n",
    "with open(os.path.join(ARTIFACTS, \"env.json\"), \"w\") as f:\n",
    "    json.dump(env, f, indent=2)\n",
    "\n",
    "print(\"=== Pre-flight ===\")\n",
    "print(json.dumps(env, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ede81d",
   "metadata": {},
   "source": [
    "## DVC quick status (graceful fallback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1223b9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, pathlib, json, os\n",
    "\n",
    "dvc_info = {\"present\": DVC_PRESENT, \"version\": None, \"status\": None}\n",
    "if DVC_PRESENT:\n",
    "    try:\n",
    "        ver = subprocess.check_output([\"dvc\", \"--version\"], timeout=10).decode().strip()\n",
    "        dvc_info[\"version\"] = ver\n",
    "    except Exception as e:\n",
    "        dvc_info[\"version\"] = f\"error: {e}\"\n",
    "    try:\n",
    "        # Check for cached stage status; if no DVC repo, this will fail\n",
    "        out = subprocess.check_output([\"dvc\", \"status\", \"-c\"], stderr=subprocess.STDOUT, timeout=15).decode()\n",
    "        dvc_info[\"status\"] = out\n",
    "    except Exception as e:\n",
    "        dvc_info[\"status\"] = f\"(no DVC repo or error) {e}\"\n",
    "else:\n",
    "    dvc_info[\"status\"] = \"(dvc not installed)\"\n",
    "with open(os.path.join(ARTIFACTS, \"dvc_status.json\"), \"w\") as f:\n",
    "    json.dump(dvc_info, f, indent=2)\n",
    "print(json.dumps(dvc_info, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e0ac007",
   "metadata": {},
   "source": [
    "## Helper: robust CLI runner (DRY‑RUN when CLI missing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3556ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shlex, time\n",
    "\n",
    "def run_cli(cmd_list, log_name=\"run\"):\n",
    "    log_path = os.path.join(LOGS, f\"{log_name}.log\")\n",
    "    err_path = os.path.join(LOGS, f\"{log_name}.err\")\n",
    "    start = time.time()\n",
    "    result = {\"cmd\": cmd_list, \"dry_run\": not CLI_PRESENT, \"returncode\": 0, \"stdout\": \"\", \"stderr\": \"\"}\n",
    "    if not CLI_PRESENT:\n",
    "        msg = f\"[DRY-RUN] Would execute: {' '.join(shlex.quote(c) for c in cmd_list)}\\n\"\n",
    "        result[\"stdout\"] = msg\n",
    "        with open(log_path, \"w\") as f: f.write(msg)\n",
    "        with open(err_path, \"w\") as f: f.write(\"\")\n",
    "        return result\n",
    "\n",
    "    with open(log_path, \"wb\") as out, open(err_path, \"wb\") as err:\n",
    "        try:\n",
    "            proc = subprocess.Popen(cmd_list, stdout=out, stderr=err, env=os.environ.copy())\n",
    "            proc.wait()\n",
    "            result[\"returncode\"] = proc.returncode\n",
    "        except Exception as e:\n",
    "            result[\"returncode\"] = 99\n",
    "            with open(err_path, \"ab\") as errf:\n",
    "                errf.write(str(e).encode())\n",
    "\n",
    "    try: result[\"stdout\"] = open(log_path, \"r\").read()\n",
    "    except Exception: pass\n",
    "    try: result[\"stderr\"] = open(err_path, \"r\").read()\n",
    "    except Exception: pass\n",
    "    result[\"elapsed_sec\"] = round(time.time() - start, 3)\n",
    "    print(f\"[rc={result['returncode']}] logs: {log_path}\")\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ee16d8",
   "metadata": {},
   "source": [
    "## End‑to‑end pipeline run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3956a034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calibrate (optionally subset for a fast CI smoke run)\n",
    "cal_cmd = [\n",
    "    \"spectramind\",\"calibrate\",\n",
    "    \"+outputs.root_dir=\"+ARTIFACTS,\n",
    "    # Optional flags if supported by your CLI:\n",
    "    # \"--sample\",\"8\"\n",
    "]\n",
    "res_cal = run_cli(cal_cmd, log_name=\"01_calibrate\")\n",
    "print(res_cal[\"stdout\"][:400])\n",
    "\n",
    "# Train (fast mode for CI; bump epochs in full runs)\n",
    "train_cmd = [\n",
    "    \"spectramind\",\"train\",\n",
    "    \"--config-name\",\"config_v50.yaml\",\n",
    "    \"+outputs.root_dir=\"+ARTIFACTS,\n",
    "    \"+training.max_epochs=1\",\n",
    "    \"+training.fast_mode=true\"\n",
    "]\n",
    "res_tr = run_cli(train_cmd, log_name=\"02_train\")\n",
    "print(res_tr[\"stdout\"][:400])\n",
    "\n",
    "# Diagnose (HTML dashboard)\n",
    "dash_html = os.path.join(DIAG_OUT, \"diagnostic_report_ci_v1.html\")\n",
    "diag_cmd = [\"spectramind\",\"diagnose\",\"dashboard\",\"--out\", dash_html]\n",
    "res_dg = run_cli(diag_cmd, log_name=\"03_diagnose_dashboard\")\n",
    "print(res_dg[\"stdout\"][:400])\n",
    "\n",
    "# Submit bundle (pack outputs)\n",
    "submit_zip = os.path.join(SUBMIT_OUT, \"submission_bundle.zip\")\n",
    "sub_cmd = [\"spectramind\",\"submit\",\"--out\", submit_zip]\n",
    "res_sb = run_cli(sub_cmd, log_name=\"04_submit\")\n",
    "print(res_sb[\"stdout\"][:400])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57c014c",
   "metadata": {},
   "source": [
    "## Build a reproducibility manifest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc36fab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib, json, glob, os\n",
    "\n",
    "def sha256_of_file(path, chunk=1024*1024):\n",
    "    try:\n",
    "        h = hashlib.sha256()\n",
    "        with open(path, \"rb\") as f:\n",
    "            while True:\n",
    "                b = f.read(chunk)\n",
    "                if not b: break\n",
    "                h.update(b)\n",
    "        return h.hexdigest()\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# Collect artifacts and hashes\n",
    "artifact_files = []\n",
    "for root, dirs, files in os.walk(ARTIFACTS):\n",
    "    for fn in files:\n",
    "        full = os.path.join(root, fn)\n",
    "        rel = os.path.relpath(full, ARTIFACTS)\n",
    "        artifact_files.append({\"path\": rel, \"sha256\": sha256_of_file(full)})\n",
    "\n",
    "manifest = {\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"timestamp_utc\": RUN_TS,\n",
    "    \"git\": env.get(\"git\"),\n",
    "    \"cli_present\": CLI_PRESENT,\n",
    "    \"dvc\": json.load(open(os.path.join(ARTIFACTS, \"dvc_status.json\"))),\n",
    "    \"commands\": {\n",
    "        \"calibrate\": res_cal[\"cmd\"],\n",
    "        \"train\": res_tr[\"cmd\"],\n",
    "        \"diagnose_dashboard\": res_dg[\"cmd\"],\n",
    "        \"submit\": res_sb[\"cmd\"],\n",
    "    },\n",
    "    \"returncodes\": {\n",
    "        \"calibrate\": res_cal[\"returncode\"],\n",
    "        \"train\": res_tr[\"returncode\"],\n",
    "        \"diagnose_dashboard\": res_dg[\"returncode\"],\n",
    "        \"submit\": res_sb[\"returncode\"],\n",
    "    },\n",
    "    \"artifacts\": artifact_files,\n",
    "    \"notes\": \"Config snapshots will be included if the CLI dumps composed Hydra configs into outputs. DRY-RUN indicates missing CLI.\"\n",
    "}\n",
    "with open(os.path.join(ARTIFACTS, \"repro_manifest.json\"), \"w\") as f:\n",
    "    json.dump(manifest, f, indent=2)\n",
    "\n",
    "print(f\"Manifest written to: {os.path.join(ARTIFACTS, 'repro_manifest.json')}\")\n",
    "print(f\"Artifacts counted: {len(artifact_files)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474db11e",
   "metadata": {},
   "source": [
    "## CI smoke‑test checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77c7e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple checks that help CI verify integrity\n",
    "import json, os\n",
    "\n",
    "rc_ok = all(x == 0 for x in [\n",
    "    res_cal[\"returncode\"],\n",
    "    res_tr[\"returncode\"],\n",
    "    res_dg[\"returncode\"],\n",
    "    res_sb[\"returncode\"],\n",
    "]) if CLI_PRESENT else True  # In DRY-RUN, allow pass\n",
    "\n",
    "manifest_path = os.path.join(ARTIFACTS, \"repro_manifest.json\")\n",
    "has_manifest = os.path.exists(manifest_path)\n",
    "\n",
    "print(\"Return codes OK (or DRY-RUN):\", rc_ok)\n",
    "print(\"Manifest exists:\", has_manifest)\n",
    "\n",
    "# (Optional) Idempotent rerun of a tiny step, e.g., diagnose-only\n",
    "# This is skipped in DRY-RUN to keep the notebook fast & deterministic.\n",
    "if CLI_PRESENT:\n",
    "    res_dg2 = run_cli([\"spectramind\",\"diagnose\",\"dashboard\",\"--out\", os.path.join(DIAG_OUT,\"diagnostic_report_ci_v2.html\")], log_name=\"05_diagnose_dashboard_rerun\")\n",
    "    print(\"Second dashboard rc:\", res_dg2[\"returncode\"])\n",
    "else:\n",
    "    print(\"[DRY-RUN] Skipping idempotent re-run.\")\n",
    "\n",
    "assert has_manifest, \"Reproducibility manifest missing.\"\n",
    "print(\"CI smoke-test checks complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64de1f88",
   "metadata": {},
   "source": [
    "## Browse produced artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ba542a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def tree(path, prefix=\"\"):\n",
    "    items = sorted(os.listdir(path))\n",
    "    lines = []\n",
    "    for i, name in enumerate(items):\n",
    "        full = os.path.join(path, name)\n",
    "        connector = \"└── \" if i == len(items)-1 else \"├── \"\n",
    "        lines.append(prefix + connector + name)\n",
    "        if os.path.isdir(full):\n",
    "            extension = \"    \" if i == len(items)-1 else \"│   \"\n",
    "            lines.extend(tree(full, prefix + extension))\n",
    "    return lines\n",
    "\n",
    "print(\"ARTIFACTS TREE:\", ARTIFACTS)\n",
    "print(\"\\n\".join(tree(ARTIFACTS)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10109ebc",
   "metadata": {},
   "source": [
    "## Pipeline sketch (Mermaid)\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "  A[Calibrate] --> B[Train]\n",
    "  B --> C[Diagnose → HTML]\n",
    "  C --> D[Submit → ZIP]\n",
    "  A -.->|DVC stages| B\n",
    "  B -.->|Hydra config| C\n",
    "  C -.->|Manifest hashes| D\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e77557",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "- Commit the generated `repro_manifest.json` and artifacts (or track large ones via **DVC**).\n",
    "- Add a **CI job** (GitHub Actions) that runs this notebook or its equivalent CLI sequence on a schedule and on PRs.\n",
    "- Ensure the **CLI dumps composed Hydra configs** into `configs/` per run (the manifest will capture them automatically).\n",
    "- Consider adding **MLflow** run logging alongside DVC for a web UI comparison of runs.\n",
    "\n",
    "If you want, I can also prepare a **GitHub Actions** CI YAML that runs the same smoke‑test using the CLI only.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1dacef2",
   "metadata": {},
   "source": [
    "## 11_kaggle_submission_and_leaderboard_playbook.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247e217d",
   "metadata": {},
   "source": [
    "# 🏁 SpectraMind V50 — Kaggle Submission & Leaderboard Playbook (Notebook 11)\n",
    "\n",
    "**Goal.** Package, validate, and (optionally) upload a **Kaggle submission** for the NeurIPS Ariel Data Challenge.  \n",
    "This notebook is **CLI-first** and provides **DRY-RUN** safety if the `kaggle` CLI isn't available.\n",
    "\n",
    "**What this notebook does**\n",
    "1. Pre-flight: detect Kaggle CLI, capture env/git info, set run paths  \n",
    "2. Locate/validate the submission artifacts (CSV/ZIP) produced by `spectramind submit`  \n",
    "3. Create a **submission bundle** + README/model card and a **manifest** with hashes  \n",
    "4. (Optional) **Kaggle upload** via CLI/API — with **DRY-RUN fallback**  \n",
    "5. Record **leaderboard metadata** and a submission log usable in CI and postmortems  \n",
    "6. Mermaid sketch of the submission workflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2b8a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ░░ Pre-flight ░░\n",
    "import os, sys, json, shutil, subprocess, datetime, pathlib, hashlib\n",
    "\n",
    "RUN_TS = datetime.datetime.utcnow().strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "RUN_ID = f\"kaggle_submit_{RUN_TS}\"\n",
    "ROOT_OUT = \"/mnt/data/kaggle_submission\"\n",
    "ARTIFACTS = os.path.join(ROOT_OUT, RUN_ID)\n",
    "LOGS = os.path.join(ARTIFACTS, \"logs\")\n",
    "PKG = os.path.join(ARTIFACTS, \"package\")\n",
    "for p in (ROOT_OUT, ARTIFACTS, LOGS, PKG):\n",
    "    os.makedirs(p, exist_ok=True)\n",
    "\n",
    "def which(cmd:str)->bool: return shutil.which(cmd) is not None\n",
    "KAGGLE_PRESENT = which(\"kaggle\")\n",
    "CLI_PRESENT = which(\"spectramind\")\n",
    "\n",
    "def git_cmd(args):\n",
    "    try:\n",
    "        out = subprocess.check_output([\"git\", *args], stderr=subprocess.STDOUT, timeout=5).decode().strip()\n",
    "        return out\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "env = {\n",
    "    \"python\": sys.version.replace(\"\\n\",\" \"),\n",
    "    \"platform\": sys.platform,\n",
    "    \"kaggle_present\": KAGGLE_PRESENT,\n",
    "    \"spectramind_present\": CLI_PRESENT,\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"paths\": {\"artifacts\": ARTIFACTS, \"logs\": LOGS, \"package\": PKG},\n",
    "    \"git\": {\n",
    "        \"commit\": git_cmd([\"rev-parse\", \"HEAD\"]),\n",
    "        \"branch\": git_cmd([\"rev-parse\", \"--abbrev-ref\", \"HEAD\"]),\n",
    "        \"status\": git_cmd([\"status\", \"--porcelain\"]),\n",
    "    },\n",
    "}\n",
    "with open(os.path.join(ARTIFACTS, \"env.json\"), \"w\") as f:\n",
    "    json.dump(env, f, indent=2)\n",
    "\n",
    "print(\"=== Pre-flight ===\")\n",
    "print(json.dumps(env, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe0e59b",
   "metadata": {},
   "source": [
    "## Locate submission artifact (CSV/ZIP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db2847f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os, json, pathlib\n",
    "\n",
    "# Heuristics: look for a submission file produced earlier (e.g., by `spectramind submit`)\n",
    "candidate_globs = [\n",
    "    \"/mnt/data/**/submission*.csv\",\n",
    "    \"/mnt/data/**/submission*.zip\",\n",
    "    \"/mnt/data/**/submission_bundle*.zip\",\n",
    "]\n",
    "found = []\n",
    "for pattern in candidate_globs:\n",
    "    for path in glob.glob(pattern, recursive=True):\n",
    "        if os.path.isfile(path):\n",
    "            found.append(path)\n",
    "\n",
    "found = sorted(set(found), key=lambda p: (os.path.getmtime(p), p), reverse=True)\n",
    "print(\"Found candidate submissions:\", json.dumps(found[:10], indent=2))\n",
    "\n",
    "SUBMISSION_FILE = found[0] if found else None\n",
    "print(\"Chosen submission file:\", SUBMISSION_FILE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9418a330",
   "metadata": {},
   "source": [
    "## Validate & stage submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e9ee10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil, os, csv, zipfile, json, hashlib\n",
    "\n",
    "def sha256_of_file(path, chunk=1024*1024):\n",
    "    try:\n",
    "        h = hashlib.sha256()\n",
    "        with open(path, \"rb\") as f:\n",
    "            while True:\n",
    "                b = f.read(chunk)\n",
    "                if not b: break\n",
    "                h.update(b)\n",
    "        return h.hexdigest()\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "valid = False\n",
    "msg = \"\"\n",
    "\n",
    "if SUBMISSION_FILE and os.path.isfile(SUBMISSION_FILE):\n",
    "    # Basic checks: csv or zip\n",
    "    ext = os.path.splitext(SUBMISSION_FILE)[1].lower()\n",
    "    if ext == \".csv\":\n",
    "        # Minimal CSV sanity: header present, at least one row\n",
    "        try:\n",
    "            with open(SUBMISSION_FILE, newline=\"\") as f:\n",
    "                reader = csv.reader(f)\n",
    "                header = next(reader, None)\n",
    "                row = next(reader, None)\n",
    "                valid = header is not None and row is not None\n",
    "                msg = f\"CSV header={header} first_row={row[:3] if row else None}\"\n",
    "        except Exception as e:\n",
    "            msg = f\"CSV read error: {e}\"\n",
    "    elif ext == \".zip\":\n",
    "        try:\n",
    "            with zipfile.ZipFile(SUBMISSION_FILE, \"r\") as zf:\n",
    "                namelist = zf.namelist()\n",
    "                valid = len(namelist) > 0\n",
    "                msg = f\"ZIP contains: {namelist[:5]}...\"\n",
    "        except Exception as e:\n",
    "            msg = f\"ZIP read error: {e}\"\n",
    "    else:\n",
    "        msg = f\"Unsupported extension: {ext}\"\n",
    "else:\n",
    "    msg = \"No submission file found.\"\n",
    "\n",
    "print(\"Valid?\", valid, \"|\", msg)\n",
    "\n",
    "STAGED = None\n",
    "if valid:\n",
    "    STAGED = os.path.join(PKG, os.path.basename(SUBMISSION_FILE))\n",
    "    shutil.copy2(SUBMISSION_FILE, STAGED)\n",
    "    print(\"Staged:\", STAGED, \"SHA256:\", sha256_of_file(STAGED))\n",
    "else:\n",
    "    print(\"Skipping stage; invalid or missing submission file.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77075ae8",
   "metadata": {},
   "source": [
    "## Write README/model card & manifest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbcc881",
   "metadata": {},
   "outputs": [],
   "source": [
    "readme = f\"\"\"# SpectraMind V50 — Kaggle Submission Package\n",
    "\n",
    "**Run ID:** {RUN_ID}  \n",
    "**Timestamp (UTC):** {RUN_TS}\n",
    "\n",
    "This package was generated by *Notebook 11 — Kaggle Submission & Leaderboard Playbook*.\n",
    "\n",
    "## Contents\n",
    "- `{os.path.basename(STAGED) if STAGED else 'MISSING'}` — submission artifact\n",
    "- `manifest.json` — provenance (git, hashes)\n",
    "- `notes.md` — optional notes\n",
    "\n",
    "## Reproducibility\n",
    "- Code commit: {env['git']['commit']}\n",
    "- Branch: {env['git']['branch']}\n",
    "- Python: {env['python']}\n",
    "\n",
    "This package is designed to be CI-friendly and traceable.\n",
    "\"\"\"\n",
    "\n",
    "with open(os.path.join(PKG, \"README.md\"), \"w\") as f:\n",
    "    f.write(readme)\n",
    "\n",
    "manifest = {\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"timestamp_utc\": RUN_TS,\n",
    "    \"git\": env.get(\"git\"),\n",
    "    \"submission_file\": STAGED,\n",
    "    \"submission_sha256\": sha256_of_file(STAGED) if STAGED else None,\n",
    "    \"kaggle_cli_present\": KAGGLE_PRESENT,\n",
    "}\n",
    "with open(os.path.join(PKG, \"manifest.json\"), \"w\") as f:\n",
    "    json.dump(manifest, f, indent=2)\n",
    "\n",
    "with open(os.path.join(PKG, \"notes.md\"), \"w\") as f:\n",
    "    f.write(\"Add experiment notes or leaderboard observations here.\\n\")\n",
    "    \n",
    "print(\"Wrote README, manifest, notes into\", PKG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ba19de",
   "metadata": {},
   "source": [
    "## (Optional) Upload to Kaggle — DRY-RUN safe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74028015",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, shlex, os, json\n",
    "\n",
    "def run_cmd(cmd_list, log_name):\n",
    "    log_path = os.path.join(LOGS, f\"{log_name}.log\")\n",
    "    err_path = os.path.join(LOGS, f\"{log_name}.err\")\n",
    "    if not KAGGLE_PRESENT:\n",
    "        msg = f\"[DRY-RUN] Would execute: {' '.join(shlex.quote(c) for c in cmd_list)}\\n\"\n",
    "        open(log_path, \"w\").write(msg); open(err_path, \"w\").write(\"\")\n",
    "        return 0, msg, \"\"\n",
    "    try:\n",
    "        proc = subprocess.Popen(cmd_list, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        out, err = proc.communicate()\n",
    "        open(log_path, \"wb\").write(out or b\"\"); open(err_path, \"wb\").write(err or b\"\")\n",
    "        return proc.returncode, (out or b\"\").decode(), (err or b\"\").decode()\n",
    "    except Exception as e:\n",
    "        return 99, \"\", str(e)\n",
    "\n",
    "# NOTE: adjust competition slug if needed\n",
    "COMPETITION = \"ariel-data-challenge-2025\"\n",
    "\n",
    "rc, out, err = (0, \"\", \"\")\n",
    "if STAGED and os.path.isfile(STAGED):\n",
    "    # Kaggle expects: kaggle competitions submit -c <comp> -f <file> -m \"<message>\"\n",
    "    msg = f\"SpectraMind V50 auto-submit {RUN_ID}\"\n",
    "    cmd = [\"kaggle\", \"competitions\", \"submit\", \"-c\", COMPETITION, \"-f\", STAGED, \"-m\", msg]\n",
    "    rc, out, err = run_cmd(cmd, log_name=\"kaggle_submit\")\n",
    "    print(\"Submit rc:\", rc)\n",
    "    print(\"stdout (truncated):\", out[:300])\n",
    "    print(\"stderr (truncated):\", err[:300])\n",
    "else:\n",
    "    print(\"[Skip] No staged submission file to upload.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c2969e",
   "metadata": {},
   "source": [
    "## Record submission log & leaderboard metadata stub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d902bd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "log = {\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"ts_utc\": RUN_TS,\n",
    "    \"kaggle_present\": KAGGLE_PRESENT,\n",
    "    \"submitted_file\": os.path.basename(STAGED) if STAGED else None,\n",
    "    \"submit_rc\": rc if 'rc' in locals() else None,\n",
    "}\n",
    "with open(os.path.join(ARTIFACTS, \"submission_log.json\"), \"w\") as f:\n",
    "    json.dump(log, f, indent=2)\n",
    "print(\"Saved submission log:\", os.path.join(ARTIFACTS, \"submission_log.json\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa941e4",
   "metadata": {},
   "source": [
    "## Browse produced artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0966924d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def tree(path, prefix=\"\"):\n",
    "    items = sorted(os.listdir(path))\n",
    "    lines = []\n",
    "    for i, name in enumerate(items):\n",
    "        full = os.path.join(path, name)\n",
    "        connector = \"└── \" if i == len(items)-1 else \"├── \"\n",
    "        lines.append(prefix + connector + name)\n",
    "        if os.path.isdir(full):\n",
    "            extension = \"    \" if i == len(items)-1 else \"│   \"\n",
    "            lines.extend(tree(full, prefix + extension))\n",
    "    return lines\n",
    "\n",
    "print(\"PKG TREE:\", PKG)\n",
    "print(\"\\n\".join(tree(PKG)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20db91cf",
   "metadata": {},
   "source": [
    "## Submission flow (Mermaid)\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "  A[Find submission CSV/ZIP] --> B[Validate structure]\n",
    "  B --> C[Stage into /package]\n",
    "  C --> D[Write README + manifest]\n",
    "  D --> E{Kaggle CLI available?}\n",
    "  E -- Yes --> F[Upload via kaggle competitions submit]\n",
    "  E -- No --> G[DRY-RUN: log command]\n",
    "  F --> H[Submission log + LB notes]\n",
    "  G --> H\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e66ac94",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "- Ensure your **Kaggle API token** is configured (`~/.kaggle/kaggle.json`) with proper permissions.\n",
    "- Verify the **competition slug** (default: `ariel-data-challenge-2025`) before uploading.\n",
    "- Use this notebook in **CI** after `10_full_pipeline_reproducibility_and_ci.ipynb` to automate packaging and submission.\n",
    "- Track submissions & scores in a lightweight CSV or use MLflow/Sheets for team visibility.\n",
    "\n",
    "> Tip: read Kaggle’s platform guide for notebook/CLI usage and submission rules.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58e50c0e",
   "metadata": {},
   "source": [
    "## 12_post_submission_analysis_and_leaderboard_tracking.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b361c0d",
   "metadata": {},
   "source": [
    "# 📊 SpectraMind V50 — Post-Submission Analysis & Leaderboard Tracking (Notebook 12)\n",
    "\n",
    "**Goal.** Centralize post-submission metadata, track public leaderboard results, and maintain a lightweight, CI-friendly log of submissions and scores.\n",
    "\n",
    "**What this notebook does**\n",
    "1. Pre-flight (detect Kaggle CLI, set paths, read repo/git info)\n",
    "2. Gather **local submission logs** (from prior notebooks) and consolidate into a historical CSV/JSON\n",
    "3. (Optional) **Query Kaggle submissions** via CLI/API — DRY-RUN safe\n",
    "4. Merge local logs with Kaggle metadata; compute **deltas** between runs and annotate best scoring submissions\n",
    "5. Emit artifacts: `submissions_history.csv`, `best_submission.json`, and a **Mermaid** trend sketch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f14200e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ░░ Pre-flight ░░\n",
    "import os, sys, json, shutil, subprocess, datetime, pathlib, csv\n",
    "\n",
    "RUN_TS = datetime.datetime.utcnow().strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "RUN_ID = f\"post_submit_{RUN_TS}\"\n",
    "ROOT_OUT = \"/mnt/data/leaderboard_tracking\"\n",
    "ARTIFACTS = os.path.join(ROOT_OUT, RUN_ID)\n",
    "LOGS = os.path.join(ARTIFACTS, \"logs\")\n",
    "os.makedirs(ARTIFACTS, exist_ok=True); os.makedirs(LOGS, exist_ok=True)\n",
    "\n",
    "def which(cmd:str)->bool: return shutil.which(cmd) is not None\n",
    "KAGGLE_PRESENT = which(\"kaggle\")\n",
    "\n",
    "def git_cmd(args):\n",
    "    try:\n",
    "        out = subprocess.check_output([\"git\", *args], stderr=subprocess.STDOUT, timeout=5).decode().strip()\n",
    "        return out\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "env = {\n",
    "    \"python\": sys.version.replace(\"\\n\",\" \"),\n",
    "    \"platform\": sys.platform,\n",
    "    \"kaggle_present\": KAGGLE_PRESENT,\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"paths\": {\"artifacts\": ARTIFACTS, \"logs\": LOGS},\n",
    "    \"git\": {\n",
    "        \"commit\": git_cmd([\"rev-parse\", \"HEAD\"]),\n",
    "        \"branch\": git_cmd([\"rev-parse\", \"--abbrev-ref\", \"HEAD\"]),\n",
    "        \"status\": git_cmd([\"status\", \"--porcelain\"]),\n",
    "    },\n",
    "}\n",
    "with open(os.path.join(ARTIFACTS, \"env.json\"), \"w\") as f:\n",
    "    json.dump(env, f, indent=2)\n",
    "\n",
    "print(\"=== Pre-flight ===\")\n",
    "print(json.dumps(env, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f59c9f9",
   "metadata": {},
   "source": [
    "## Consolidate local submission logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929a4c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os, json, csv, hashlib\n",
    "\n",
    "# Find prior local logs (from Notebook 11 or pipeline)\n",
    "CANDIDATES = sorted(set(glob.glob(\"/mnt/data/**/submission_log.json\", recursive=True)))\n",
    "print(\"Found local logs:\", len(CANDIDATES))\n",
    "\n",
    "records = []\n",
    "for path in CANDIDATES:\n",
    "    try:\n",
    "        data = json.load(open(path))\n",
    "        # Normalize fields\n",
    "        rec = {\n",
    "            \"source_path\": path,\n",
    "            \"ts_utc\": data.get(\"ts_utc\") or data.get(\"timestamp_utc\"),\n",
    "            \"run_id\": data.get(\"run_id\"),\n",
    "            \"submitted_file\": data.get(\"submitted_file\"),\n",
    "            \"kaggle_present\": data.get(\"kaggle_present\"),\n",
    "            \"submit_rc\": data.get(\"submit_rc\"),\n",
    "        }\n",
    "        # Attach file hash if available\n",
    "        if rec[\"submitted_file\"]:\n",
    "            # try to resolve absolute path\n",
    "            abs_guess = os.path.join(os.path.dirname(path), \"..\", \"package\", rec[\"submitted_file\"])\n",
    "            abs_guess = os.path.abspath(abs_guess)\n",
    "            sha256 = None\n",
    "            if os.path.exists(abs_guess):\n",
    "                h = hashlib.sha256(); \n",
    "                with open(abs_guess, \"rb\") as f:\n",
    "                    h.update(f.read())\n",
    "                sha256 = h.hexdigest()\n",
    "            rec[\"file_sha256\"] = sha256\n",
    "        records.append(rec)\n",
    "    except Exception as e:\n",
    "        print(\"Skip unreadable log:\", path, e)\n",
    "\n",
    "# Write a consolidated CSV snapshot for this run\n",
    "snapshot_csv = os.path.join(ARTIFACTS, \"submissions_snapshot.csv\")\n",
    "with open(snapshot_csv, \"w\", newline=\"\") as f:\n",
    "    w = csv.DictWriter(f, fieldnames=sorted({k for r in records for k in r.keys()}))\n",
    "    w.writeheader()\n",
    "    for r in records: w.writerow(r)\n",
    "\n",
    "print(\"Snapshot rows:\", len(records), \"->\", snapshot_csv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b71b932",
   "metadata": {},
   "source": [
    "## (Optional) Pull live Kaggle submission metadata — DRY-RUN safe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3664945c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess, json, os, shlex\n",
    "\n",
    "COMPETITION = \"ariel-data-challenge-2025\"  # adjust if needed\n",
    "def run_cmd(cmd_list, log_name):\n",
    "    log_path = os.path.join(LOGS, f\"{log_name}.log\")\n",
    "    err_path = os.path.join(LOGS, f\"{log_name}.err\")\n",
    "    if not KAGGLE_PRESENT:\n",
    "        msg = f\"[DRY-RUN] Would execute: {' '.join(shlex.quote(c) for c in cmd_list)}\\n\"\n",
    "        open(log_path, \"w\").write(msg); open(err_path, \"w\").write(\"\")\n",
    "        return 0, msg, \"\"\n",
    "    try:\n",
    "        proc = subprocess.Popen(cmd_list, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        out, err = proc.communicate()\n",
    "        open(log_path, \"wb\").write(out or b\"\"); open(err_path, \"wb\").write(err or b\"\")\n",
    "        return proc.returncode, (out or b\"\").decode(), (err or b\"\").decode()\n",
    "    except Exception as e:\n",
    "        return 99, \"\", str(e)\n",
    "\n",
    "# The Kaggle CLI can list submissions for a competition:\n",
    "# kaggle competitions submissions -c <comp>\n",
    "rc, out, err = run_cmd([\"kaggle\",\"competitions\",\"submissions\",\"-c\", COMPETITION], \"kaggle_submissions_list\")\n",
    "print(\"Submissions rc:\", rc)\n",
    "print(\"stdout (truncated):\", out[:500])\n",
    "print(\"stderr (truncated):\", err[:500])\n",
    "\n",
    "# Save raw listing for audit\n",
    "open(os.path.join(ARTIFACTS, \"kaggle_submissions_raw.txt\"), \"w\").write(out if out else \"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c9c57b",
   "metadata": {},
   "source": [
    "## Parse Kaggle CLI table & merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0789a1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, csv, io, json, os\n",
    "\n",
    "kaggle_rows = []\n",
    "table = open(os.path.join(ARTIFACTS, \"kaggle_submissions_raw.txt\")).read() if os.path.exists(os.path.join(ARTIFACTS, \"kaggle_submissions_raw.txt\")) else \"\"\n",
    "\n",
    "# The Kaggle CLI prints a table; extract rows by splitting lines and using simple heuristics.\n",
    "lines = [ln for ln in table.splitlines() if ln.strip()]\n",
    "# Try to detect header separator line (----)\n",
    "sep_idx = None\n",
    "for i, ln in enumerate(lines):\n",
    "    if set(ln.strip()) <= set(\"-|+ \"):\n",
    "        sep_idx = i\n",
    "        break\n",
    "\n",
    "header = []\n",
    "if sep_idx is not None and sep_idx > 0:\n",
    "    header_line = re.sub(r\"\\s+\", \" \", lines[sep_idx-1]).strip()\n",
    "    header = header_line.split(\" \")\n",
    "    data_lines = lines[sep_idx+1:]\n",
    "    for ln in data_lines:\n",
    "        # Normalize whitespace columns (simple heuristic)\n",
    "        parts = re.sub(r\"\\s{2,}\", \" | \", ln).split(\" | \")\n",
    "        if len(parts) >= len(header):\n",
    "            row = dict(zip(header, parts[:len(header)]))\n",
    "            kaggle_rows.append(row)\n",
    "\n",
    "print(\"Parsed Kaggle rows:\", len(kaggle_rows))\n",
    "\n",
    "# Merge strategy: write a history CSV with both local logs and Kaggle columns (where available)\n",
    "history_csv = os.path.join(ARTIFACTS, \"submissions_history.csv\")\n",
    "all_fields = set()\n",
    "for r in kaggle_rows: all_fields.update(r.keys())\n",
    "for rec in records: all_fields.update(rec.keys())\n",
    "all_fields = sorted(all_fields)\n",
    "\n",
    "with open(history_csv, \"w\", newline=\"\") as f:\n",
    "    w = csv.DictWriter(f, fieldnames=all_fields)\n",
    "    w.writeheader()\n",
    "    # Prefer Kaggle rows (live) first, then append local records (with different fields)\n",
    "    for r in kaggle_rows:\n",
    "        w.writerow(r)\n",
    "    for rec in records:\n",
    "        w.writerow(rec)\n",
    "\n",
    "print(\"Wrote consolidated history:\", history_csv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20c9496a",
   "metadata": {},
   "source": [
    "## Compute best submission & annotate deltas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010d1530",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv, os, json\n",
    "\n",
    "history_csv = os.path.join(ARTIFACTS, \"submissions_history.csv\")\n",
    "best = None\n",
    "rows = []\n",
    "if os.path.exists(history_csv):\n",
    "    with open(history_csv, newline=\"\") as f:\n",
    "        rdr = csv.DictReader(f)\n",
    "        for r in rdr:\n",
    "            rows.append(r)\n",
    "\n",
    "    # Try to identify a PublicScore-like field (Kaggle CLI prints \"PublicScore\" for many comps)\n",
    "    score_field = None\n",
    "    for cand in [\"PublicScore\",\"Score\",\"PublicScore*\",\"Public_Score\",\"publicScore\"]:\n",
    "        if rows and cand in rows[0]:\n",
    "            score_field = cand; break\n",
    "\n",
    "    # Convert scores to float if possible and sort desc\n",
    "    def to_float(x):\n",
    "        try: return float(x)\n",
    "        except: return None\n",
    "\n",
    "    scored = [(r, to_float(r.get(score_field))) for r in rows] if score_field else []\n",
    "    scored = [(r, s) for (r, s) in scored if s is not None]\n",
    "    if scored:\n",
    "        scored.sort(key=lambda t: t[1], reverse=True)\n",
    "        best = {\"score_field\": score_field, \"row\": scored[0][0], \"score\": scored[0][1]}\n",
    "\n",
    "best_path = os.path.join(ARTIFACTS, \"best_submission.json\")\n",
    "json.dump(best or {\"note\":\"no scores found\"}, open(best_path, \"w\"), indent=2)\n",
    "print(\"Best submission summary ->\", best_path)\n",
    "print(json.dumps(best or {}, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3631a8",
   "metadata": {},
   "source": [
    "## Trend sketch (Mermaid)\n",
    "\n",
    "> You can paste the following into your README to visualize simple submission flow.\n",
    "\n",
    "```mermaid\n",
    "flowchart TB\n",
    "  A[Local submission logs] --> B[Consolidate snapshot]\n",
    "  B --> C[Pull Kaggle submissions list]\n",
    "  C --> D[Merge to history CSV]\n",
    "  D --> E{Best score?}\n",
    "  E -- yes --> F[Write best_submission.json]\n",
    "  E -- no --> G[No score available]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d372325",
   "metadata": {},
   "source": [
    "## Browse produced artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a5180c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def tree(path, prefix=\"\"):\n",
    "    items = sorted(os.listdir(path))\n",
    "    lines = []\n",
    "    for i, name in enumerate(items):\n",
    "        full = os.path.join(path, name)\n",
    "        connector = \"└── \" if i == len(items)-1 else \"├── \"\n",
    "        lines.append(prefix + connector + name)\n",
    "        if os.path.isdir(full):\n",
    "            extension = \"    \" if i == len(items)-1 else \"│   \"\n",
    "            lines.extend(tree(full, prefix + extension))\n",
    "    return lines\n",
    "\n",
    "print(\"ARTIFACTS TREE:\", ARTIFACTS)\n",
    "print(\"\\n\".join(tree(ARTIFACTS)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab955a9",
   "metadata": {},
   "source": [
    "## Next steps\n",
    "- Keep this notebook in CI after Notebook 11 to **log submissions automatically**.\n",
    "- If the competition exposes a public submissions API/CSV, swap the CLI table parser for a JSON/CSV endpoint for better reliability.\n",
    "- Extend the merge to include **config hash**, **data version** (from DVC), and Git tag for precise provenance.\n",
    "- Consider a tiny dashboard (static HTML) that renders `submissions_history.csv` and highlights the best score per day.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bee886c",
   "metadata": {},
   "source": [
    "## 13_gui_dashboard_demo.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b692cc70",
   "metadata": {},
   "source": [
    "# 13 · GUI Dashboard Demo (SpectraMind V50)\n",
    "\n",
    "Thin, **CLI‑first** viewer that launches/embeds the generated diagnostics dashboard (HTML) and provides a light local server for browsing artifacts under `outputs/`.\n",
    "\n",
    "**What this notebook does**\n",
    "1) Locates a previously generated diagnostics HTML (e.g., `outputs/diagnostics/report.html` or a run‑scoped diagnostics file).\n",
    "2) (Optional) Calls the CLI to generate the dashboard if missing.\n",
    "3) Displays the dashboard **in‑notebook** via an iframe.\n",
    "4) (Optional) Starts a simple local HTTP server to browse `outputs/` (for local dev; disabled on Kaggle).\n",
    "5) Writes a small viewer **manifest** under `outputs/notebooks/13_gui_dashboard_demo/`.\n",
    "\n",
    "> Contract: This is a **thin viewer**. No pipeline logic here — we only call the official CLI and render the resulting HTML.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defe9ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, json, shutil, subprocess, platform, socket, contextlib\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "ROOT = Path.cwd().resolve()\n",
    "NB_OUT = ROOT / 'outputs' / 'notebooks' / '13_gui_dashboard_demo'\n",
    "NB_OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "IS_KAGGLE = Path('/kaggle/working').exists()\n",
    "CLI = shutil.which('spectramind') or (f\"{sys.executable} {ROOT/'spectramind.py'}\" if (ROOT/'spectramind.py').exists() else f\"{sys.executable} -m spectramind\")\n",
    "\n",
    "env = {\n",
    "    'python': platform.python_version(),\n",
    "    'platform': platform.platform(),\n",
    "    'is_kaggle': IS_KAGGLE,\n",
    "    'cli': CLI\n",
    "}\n",
    "(NB_OUT/'env_snapshot.json').write_text(json.dumps(env, indent=2))\n",
    "print(json.dumps(env, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f3391d",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "Edit if you need to point at a specific run or force dashboard generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd07d806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to look for an existing diagnostics HTML\n",
    "REPORT_HINTS = [\n",
    "    ROOT/'outputs'/'diagnostics'/'report.html',\n",
    "    ROOT/'outputs',       # scan recursively for *.html with 'diagnostic' in name\n",
    "]\n",
    "\n",
    "# If True, call the CLI to (re)generate the dashboard when not found\n",
    "AUTO_GENERATE_DASHBOARD = False  # toggle on if you want the notebook to call the CLI\n",
    "\n",
    "# If generating via CLI, write to this path (adjust per your repo):\n",
    "CLI_DASHBOARD_OUT = ROOT/'outputs'/'diagnostics'/'report.html'\n",
    "\n",
    "print('REPORT_HINTS:', [str(p) for p in REPORT_HINTS])\n",
    "print('AUTO_GENERATE_DASHBOARD:', AUTO_GENERATE_DASHBOARD)\n",
    "print('CLI_DASHBOARD_OUT:', CLI_DASHBOARD_OUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4fa066",
   "metadata": {},
   "source": [
    "## Locate existing dashboard report\n",
    "We try direct paths first, then recursively search `outputs/` for files that look like diagnostics dashboards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ebae53",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "def newest_dashboard(hints) -> Optional[Path]:\n",
    "    cands = []\n",
    "    for h in hints:\n",
    "        if not h.exists():\n",
    "            continue\n",
    "        if h.is_file() and h.suffix.lower() == '.html':\n",
    "            cands.append(h)\n",
    "        elif h.is_dir():\n",
    "            for p in h.rglob('*.html'):\n",
    "                name = p.name.lower()\n",
    "                if any(tok in name for tok in ('diagnostic','dashboard','report')):\n",
    "                    cands.append(p)\n",
    "    if not cands:\n",
    "        return None\n",
    "    return sorted(cands, key=lambda p: p.stat().st_mtime)[-1]\n",
    "\n",
    "REPORT = newest_dashboard(REPORT_HINTS)\n",
    "print('Found dashboard:', REPORT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5371a3",
   "metadata": {},
   "source": [
    "## (Optional) Generate the dashboard via CLI\n",
    "This only runs when `AUTO_GENERATE_DASHBOARD=True`. Adjust the command to match your repository (e.g., `spectramind diagnose dashboard`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6d3d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "if AUTO_GENERATE_DASHBOARD:\n",
    "    try:\n",
    "        cmd = [\n",
    "            *CLI.split(), 'diagnose', 'dashboard',\n",
    "            f'--out', str(CLI_DASHBOARD_OUT)\n",
    "        ]\n",
    "        print('Running:', ' '.join(cmd))\n",
    "        subprocess.run(cmd, check=True)\n",
    "        REPORT = CLI_DASHBOARD_OUT if CLI_DASHBOARD_OUT.exists() else newest_dashboard([ROOT/'outputs'])\n",
    "        print('Generated report ->', REPORT)\n",
    "    except Exception as e:\n",
    "        print('CLI dashboard generation failed (non‑blocking):', e)\n",
    "else:\n",
    "    print('CLI generation disabled; using existing artifacts only.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6097e5",
   "metadata": {},
   "source": [
    "## In‑notebook viewer\n",
    "Renders the dashboard using an iframe. On Kaggle, this is the preferred way (no external ports)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7b089c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import IFrame, display, HTML\n",
    "\n",
    "if REPORT and REPORT.exists():\n",
    "    # Use relative path when possible so the iframe can resolve asset links\n",
    "    try:\n",
    "        rel = REPORT.relative_to(ROOT)\n",
    "    except Exception:\n",
    "        rel = REPORT\n",
    "    print('Displaying:', rel)\n",
    "    display(IFrame(src=str(rel), width='100%', height=700))\n",
    "else:\n",
    "    print('No diagnostics HTML found. Enable AUTO_GENERATE_DASHBOARD or populate outputs/diagnostics/.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac854cd",
   "metadata": {},
   "source": [
    "## (Optional) Lightweight local server (for local dev only)\n",
    "Starts a simple `http.server` to browse `outputs/`. **Not** recommended on Kaggle (no external ports). Set `START_SERVER=True` only on your workstation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4462d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "START_SERVER = False  # toggle True for local dev only\n",
    "SERVER_ROOT = ROOT/'outputs'\n",
    "\n",
    "def free_port(start=8000, end=8999):\n",
    "    for port in range(start, end+1):\n",
    "        with contextlib.closing(socket.socket(socket.AF_INET, socket.SOCK_STREAM)) as s:\n",
    "            if s.connect_ex(('127.0.0.1', port)) != 0:\n",
    "                return port\n",
    "    return None\n",
    "\n",
    "if START_SERVER and not IS_KAGGLE:\n",
    "    port = free_port()\n",
    "    if port is None:\n",
    "        print('No free port found.');\n",
    "    else:\n",
    "        print(f'Launching local server at http://127.0.0.1:{port}/ (root={SERVER_ROOT})')\n",
    "        print('Stop the server by interrupting the cell (Kernel -> Interrupt).')\n",
    "        os.chdir(SERVER_ROOT)\n",
    "        try:\n",
    "            from http.server import ThreadingHTTPServer, SimpleHTTPRequestHandler\n",
    "        except Exception:\n",
    "            from http.server import HTTPServer as ThreadingHTTPServer, SimpleHTTPRequestHandler\n",
    "        handler = SimpleHTTPRequestHandler\n",
    "        httpd = ThreadingHTTPServer(('127.0.0.1', port), handler)\n",
    "        try:\n",
    "            httpd.serve_forever()\n",
    "        except KeyboardInterrupt:\n",
    "            print('Shutting down server...')\n",
    "            httpd.server_close()\n",
    "else:\n",
    "    print('Local server disabled (or running on Kaggle).')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851ad9fd",
   "metadata": {},
   "source": [
    "## (Optional) Streamlit/Gradio mini‑app\n",
    "For a richer demo, you can spin up Streamlit/Gradio to serve your plots. This is **off** by default to avoid extra dependencies and network binding issues in shared environments.\n",
    "\n",
    "Uncomment and adapt the code below if you need it locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae47fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_STREAMLIT = False\n",
    "if USE_STREAMLIT and not IS_KAGGLE:\n",
    "    try:\n",
    "        import streamlit as st  # ensure installed in your env\n",
    "    except Exception:\n",
    "        print('Install streamlit first (pip install streamlit)');\n",
    "        USE_STREAMLIT = False\n",
    "\n",
    "if USE_STREAMLIT:\n",
    "    # Example: write a simple app file and run it\n",
    "    app_py = NB_OUT/'dashboard_app.py'\n",
    "    app_py.write_text(\n",
    "        \"\"\"\n",
    "import streamlit as st\n",
    "from pathlib import Path\n",
    "st.set_page_config(layout='wide')\n",
    "st.title('SpectraMind V50 — Diagnostics Dashboard')\n",
    "report = Path('outputs/diagnostics/report.html')\n",
    "if report.exists():\n",
    "    st.components.v1.html(report.read_text(encoding='utf-8'), height=900, scrolling=True)\n",
    "else:\n",
    "    st.warning('No diagnostics HTML found under outputs/diagnostics/.')\n",
    "        \"\"\".strip()\n",
    "    )\n",
    "    cmd = f\"streamlit run {app_py} --server.headless=true\"\n",
    "    print('Launching:', cmd)\n",
    "    subprocess.run(cmd, shell=True, check=False)\n",
    "else:\n",
    "    print('Streamlit mini‑app disabled (or not supported in this environment).')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80fedf3",
   "metadata": {},
   "source": [
    "## Manifest & (optional) DVC add\n",
    "We save a simple JSON manifest for this viewer session, and optionally `dvc add` the notebook outputs for full reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea84211",
   "metadata": {},
   "outputs": [],
   "source": [
    "manifest = {\n",
    "    'timestamp_utc': datetime.utcnow().isoformat(timespec='seconds')+'Z',\n",
    "    'report_path': str(REPORT) if REPORT else None,\n",
    "    'cli': CLI,\n",
    "    'is_kaggle': IS_KAGGLE\n",
    "}\n",
    "(NB_OUT/'viewer_manifest.json').write_text(json.dumps(manifest, indent=2))\n",
    "print('Wrote:', NB_OUT/'viewer_manifest.json')\n",
    "\n",
    "if shutil.which('dvc'):\n",
    "    try:\n",
    "        subprocess.run(['dvc','add', str(NB_OUT)], check=False)\n",
    "        subprocess.run(['git','add', f'{NB_OUT}.dvc', '.gitignore'], check=False)\n",
    "        subprocess.run(['dvc','status'], check=False)\n",
    "        print('DVC add done (non‑blocking).')\n",
    "    except Exception as e:\n",
    "        print('DVC step failed (non‑blocking):', e)\n",
    "else:\n",
    "    print('DVC not found; skipping.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580bb6ec",
   "metadata": {},
   "source": [
    "## 14_radiation_and_noise_modeling.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215cf9f1",
   "metadata": {},
   "source": [
    "# 14 · Radiation & Noise Modeling (SpectraMind V50)\n",
    "\n",
    "Educational, **mission‑grade** notebook to illustrate how radiation environments and detector/system noise influence transit spectra and downstream diagnostics. This notebook is **pipeline‑safe**: it *reads* existing artifacts (calibrated spectra or predictions) and writes educational diagnostics under `outputs/notebooks/14_radiation_noise_modeling/`.\n",
    "\n",
    "### Objectives\n",
    "1. Summarize noise sources relevant to spaceborne spectroscopy (shot noise, read noise, dark current, cosmic rays / radiation hits, background).\n",
    "2. Load one or more spectra from `outputs/` and inject controllable noise models (Poisson, Gaussian, 1/f) and **cosmic‑ray transients**.\n",
    "3. Visualize impact on FFT/autocorr structure, per‑bin variance, and symbolic bands.\n",
    "4. Export a compact diagnostics bundle (JSON + CSV + PNGs) for teaching and QA.\n",
    "\n",
    "> Contract: **Thin orchestration** over CLI/outputs; no ad‑hoc calibration or model training here. For production calibration and prediction, use the dedicated notebooks and CLI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6411c046",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, json, shutil, subprocess, platform, textwrap\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "try:\n",
    "    import seaborn as sns\n",
    "    sns.set_context('notebook'); sns.set_style('whitegrid')\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "ROOT = Path.cwd().resolve()\n",
    "NB_OUT = ROOT / 'outputs' / 'notebooks' / '14_radiation_noise_modeling'\n",
    "NB_OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "ENV = {\n",
    "    'python': platform.python_version(),\n",
    "    'platform': platform.platform(),\n",
    "    'time': datetime.utcnow().isoformat()+'Z',\n",
    "}\n",
    "(NB_OUT/'env_snapshot.json').write_text(json.dumps(ENV, indent=2))\n",
    "print('ROOT:', ROOT) ; print('NB_OUT:', NB_OUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf04c77",
   "metadata": {},
   "source": [
    "## 0) Background: radiation & detector/system noise (recap)\n",
    "**Noise categories (simplified):**\n",
    "- **Shot noise (photon counting)**: Poisson with variance $\\sigma^2 \\approx N$ photons. Dominant at high flux, fundamental.\n",
    "- **Read noise**: electronics/ADC noise per read; Gaussian with fixed variance per exposure/read.\n",
    "- **Dark current**: thermally generated electrons; behaves like additional Poisson process with rate depending on temperature.\n",
    "- **Background (zodiacal/thermal)**: adds counts and variance; often Poisson‑like per pixel/extraction window.\n",
    "- **1/f (pink) noise)**: low‑frequency drift; can imprint long‑scale structure in spectra/time series.\n",
    "- **Cosmic rays / radiation hits**: transient, often impulsive events (spikes/glitches) that must be detected/flagged.\n",
    "\n",
    "We’ll illustrate how each affects a clean spectrum (or an existing prediction) by perturbing it with controlled levels and visualizing FFT/autocorr & per‑bin variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3827a7",
   "metadata": {},
   "source": [
    "## 1) Load a base spectrum from outputs/\n",
    "We try `outputs/` for a predictions CSV or NPY and reduce to a single `(wavelength_index, mu)` spectrum for demonstration.\n",
    "\n",
    "If none are found, we synthesize a plausible spectrum with two absorption features for educational use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e8f066",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_candidates():\n",
    "    roots = [ROOT/'outputs']\n",
    "    pats = ['**/predictions.csv','**/mu.csv','**/spectra.npy','**/mu.npy']\n",
    "    cands = []\n",
    "    for r in roots:\n",
    "        if not r.exists():\n",
    "            continue\n",
    "        for pat in pats:\n",
    "            cands += list(r.glob(pat))\n",
    "    return sorted(set(cands), key=lambda p: p.stat().st_mtime) if cands else []\n",
    "\n",
    "def load_one_spectrum(path: Path) -> pd.DataFrame:\n",
    "    if path.suffix=='.npy':\n",
    "        arr = np.load(path)\n",
    "        if arr.ndim==1: arr = arr[None,:]\n",
    "        mu = arr[0]\n",
    "        return pd.DataFrame({'wavelength_index': np.arange(len(mu)), 'mu': mu})\n",
    "    df = pd.read_csv(path)\n",
    "    cols = {str(c).lower(): c for c in df.columns}\n",
    "    if {'planet_id','wavelength_index','mu'}.issubset(cols):\n",
    "        one = df[df[cols['planet_id']]==df[cols['planet_id']].iloc[0]].copy()\n",
    "        one = one.rename(columns={cols['wavelength_index']:'wavelength_index', cols['mu']:'mu'})\n",
    "        return one[['wavelength_index','mu']].sort_values('wavelength_index').reset_index(drop=True)\n",
    "    mu_cols = [c for c in df.columns if str(c).startswith('mu_')]\n",
    "    if mu_cols:\n",
    "        mu = df[mu_cols].iloc[0].to_numpy(float)\n",
    "        return pd.DataFrame({'wavelength_index': np.arange(len(mu)), 'mu': mu})\n",
    "    raise ValueError(f'Unsupported schema for {path}')\n",
    "\n",
    "CANDS = find_candidates()\n",
    "if not CANDS:\n",
    "    # synthesize a smooth demo spectrum\n",
    "    x = np.linspace(0, 1, 283)\n",
    "    mu = 0.01 + 0.002*np.exp(-0.5*((x-0.35)/0.08)**2) + 0.0015*np.exp(-0.5*((x-0.75)/0.05)**2)\n",
    "    base = pd.DataFrame({'wavelength_index': np.arange(283), 'mu': mu})\n",
    "    base_source = 'synthetic'\n",
    "    print('No outputs found; using synthetic spectrum.')\n",
    "else:\n",
    "    base = load_one_spectrum(CANDS[-1])\n",
    "    base_source = CANDS[-1].relative_to(ROOT).as_posix()\n",
    "    print('Loaded from:', base_source)\n",
    "\n",
    "base.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727d1c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,3))\n",
    "plt.plot(base['wavelength_index'], base['mu'], lw=1.5)\n",
    "plt.title('Base spectrum (μ)')\n",
    "plt.xlabel('wavelength index'); plt.ylabel('μ (arb)')\n",
    "plt.tight_layout(); plt.savefig(NB_OUT/'base_spectrum.png', dpi=150); plt.close()\n",
    "print('Saved base_spectrum.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14b3e0d0",
   "metadata": {},
   "source": [
    "## 2) Noise models\n",
    "We implement simple, composable perturbations:\n",
    "- **Poisson shot noise**: `y ~ Poisson(λ=S·μ) / S` with scaling `S` to get counts domain.\n",
    "- **Gaussian read noise**: `N(0, σ_read)`.\n",
    "- **1/f noise**: generated via colored‑noise frequency shaping.\n",
    "- **Cosmic ray hits**: sparse spikes at random indices; optionally spread with small kernels.\n",
    "\n",
    "All random draws are controlled by a fixed seed for reproducibility. Adjust parameters as needed for teaching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1910c10b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(1234)\n",
    "\n",
    "def add_shot_noise(mu, scale_counts=5e5):\n",
    "    lam = np.clip(scale_counts*np.maximum(mu, 0), 0, None)\n",
    "    y_counts = rng.poisson(lam)\n",
    "    return y_counts/scale_counts\n",
    "\n",
    "def add_read_noise(mu, sigma_read=2e-4):\n",
    "    return mu + rng.normal(0.0, sigma_read, size=mu.shape)\n",
    "\n",
    "def add_1f_noise(mu, alpha=1.0, amp=2e-4):\n",
    "    n = len(mu)\n",
    "    white = rng.normal(0,1,n)\n",
    "    f = np.fft.rfftfreq(n)\n",
    "    spec = np.fft.rfft(white)\n",
    "    with np.errstate(divide='ignore', invalid='ignore'):\n",
    "        shaping = 1.0/np.maximum(f, 1e-6)**alpha\n",
    "    shaped = spec*shaping\n",
    "    x = np.fft.irfft(shaped, n)\n",
    "    x = amp*x/np.std(x)\n",
    "    return mu + x\n",
    "\n",
    "def add_cosmic_rays(mu, n_hits=3, spike_amp=0.01, kernel=[1.0, 0.5]):\n",
    "    y = mu.copy()\n",
    "    W = len(mu)\n",
    "    if n_hits <= 0:\n",
    "        return y, np.array([], dtype=int)\n",
    "    hits = rng.choice(W, size=min(n_hits,W), replace=False)\n",
    "    for h in hits:\n",
    "        for k,a in enumerate(kernel):\n",
    "            idx = h+k\n",
    "            if idx < W:\n",
    "                y[idx] += a*spike_amp\n",
    "    return y, hits\n",
    "\n",
    "mu = base['mu'].to_numpy(float)\n",
    "noisy_shot   = add_shot_noise(mu, scale_counts=3e5)\n",
    "noisy_read   = add_read_noise(mu, sigma_read=2e-4)\n",
    "noisy_1f     = add_1f_noise(mu, alpha=1.0, amp=2e-4)\n",
    "noisy_cr, H  = add_cosmic_rays(mu, n_hits=4, spike_amp=0.01)\n",
    "H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc354ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = base['wavelength_index']\n",
    "fig, ax = plt.subplots(2,2, figsize=(12,6), sharex=True)\n",
    "ax = ax.ravel()\n",
    "ax[0].plot(x, mu, lw=1.2, label='base')\n",
    "ax[0].plot(x, noisy_shot, lw=0.8, label='shot')\n",
    "ax[0].set_title('Shot noise') ; ax[0].legend()\n",
    "\n",
    "ax[1].plot(x, mu, lw=1.2, label='base')\n",
    "ax[1].plot(x, noisy_read, lw=0.8, label='read')\n",
    "ax[1].set_title('Read noise') ; ax[1].legend()\n",
    "\n",
    "ax[2].plot(x, mu, lw=1.2, label='base')\n",
    "ax[2].plot(x, noisy_1f, lw=0.8, label='1/f')\n",
    "ax[2].set_title('1/f noise') ; ax[2].legend()\n",
    "\n",
    "ax[3].plot(x, mu, lw=1.2, label='base')\n",
    "ax[3].plot(x, noisy_cr, lw=0.8, label='cosmic rays')\n",
    "if H.size:\n",
    "    ax[3].scatter(x.iloc[H], noisy_cr[H], s=20, zorder=3)\n",
    "ax[3].set_title('Radiation hits (spikes)') ; ax[3].legend()\n",
    "\n",
    "for a in ax: a.set_ylabel('μ (arb)')\n",
    "ax[2].set_xlabel('wavelength index'); ax[3].set_xlabel('wavelength index')\n",
    "fig.tight_layout(); fig.savefig(NB_OUT/'noise_panels.png', dpi=150); plt.close(fig)\n",
    "print('Saved noise_panels.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eea4ba88",
   "metadata": {},
   "source": [
    "## 3) FFT & autocorrelation impact\n",
    "We reuse the simple FFT/AC routines used elsewhere to illustrate how each noise class alters spectral frequency content and lag structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8feff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fft_power_onesided(y):\n",
    "    y = np.asarray(y, float)\n",
    "    y = y - np.nanmean(y)\n",
    "    fy = np.fft.rfft(y)\n",
    "    power = np.abs(fy)**2\n",
    "    freqs = np.fft.rfftfreq(len(y))\n",
    "    return freqs, power\n",
    "\n",
    "def autocorr_norm(y):\n",
    "    y = np.asarray(y, float)\n",
    "    y = y - np.nanmean(y)\n",
    "    r = np.correlate(y, y, mode='full')\n",
    "    r = r[r.size//2:]\n",
    "    if r[0] != 0:\n",
    "        r = r / r[0]\n",
    "    lags = np.arange(r.size)\n",
    "    return lags, r\n",
    "\n",
    "series = {\n",
    "    'base': mu,\n",
    "    'shot': noisy_shot,\n",
    "    'read': noisy_read,\n",
    "    '1f':   noisy_1f,\n",
    "    'cr':   noisy_cr\n",
    "}\n",
    "\n",
    "fig, ax = plt.subplots(2,2, figsize=(12,6))\n",
    "ax = ax.ravel()\n",
    "for i,(name, y) in enumerate(series.items()):\n",
    "    f,p = fft_power_onesided(y)\n",
    "    ax[i//2].semilogy(f[1:], p[1:], lw=1, label=name)\n",
    "ax[0].set_title('FFT power (A)') ; ax[1].set_title('FFT power (B)')\n",
    "ax[0].legend(); ax[1].legend()\n",
    "for a in ax[:2]: a.set_xlabel('freq'); a.set_ylabel('power')\n",
    "\n",
    "for i,(name, y) in enumerate(series.items()):\n",
    "    l,r = autocorr_norm(y)\n",
    "    ax[2 + (i%2)].plot(l, r, lw=1, label=name)\n",
    "ax[2].set_title('Autocorr (A)') ; ax[3].set_title('Autocorr (B)')\n",
    "ax[2].legend(); ax[3].legend()\n",
    "for a in ax[2:]: a.set_xlabel('lag'); a.set_ylabel('norm acorr')\n",
    "fig.tight_layout(); fig.savefig(NB_OUT/'fft_autocorr_noise_compare.png', dpi=150); plt.close(fig)\n",
    "print('Saved fft_autocorr_noise_compare.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60933a1",
   "metadata": {},
   "source": [
    "## 4) Per‑bin variance & symbolic band overlays\n",
    "We compute per‑bin variance across a small ensemble of perturbed spectra and (optionally) overlay symbolic bands (e.g., water) to show if noise masks lines of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427a5a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_variance(mu, K=64, cfg=None):\n",
    "    cfg = cfg or {}\n",
    "    ens = []\n",
    "    for k in range(K):\n",
    "        y = mu.copy()\n",
    "        if cfg.get('shot'):   y = add_shot_noise(y, scale_counts=cfg.get('shot_scale',3e5))\n",
    "        if cfg.get('read'):   y = add_read_noise(y, sigma_read=cfg.get('read_sigma',2e-4))\n",
    "        if cfg.get('one_over_f'): y = add_1f_noise(y, alpha=cfg.get('alpha',1.0), amp=cfg.get('amp',2e-4))\n",
    "        if cfg.get('cosmic_hits'):\n",
    "            y,_ = add_cosmic_rays(y, n_hits=cfg.get('cr_n',3), spike_amp=cfg.get('cr_amp',0.01))\n",
    "        ens.append(y)\n",
    "    ens = np.stack(ens, axis=0)\n",
    "    return ens.var(axis=0)\n",
    "\n",
    "cfg = {'shot':True, 'read':True, 'one_over_f':True, 'cosmic_hits':True}\n",
    "var_bins = ensemble_variance(mu, K=64, cfg=cfg)\n",
    "plt.figure(figsize=(10,3))\n",
    "plt.plot(base['wavelength_index'], var_bins, lw=1)\n",
    "plt.title('Per‑bin variance under composite noise model')\n",
    "plt.xlabel('wavelength index'); plt.ylabel('variance')\n",
    "plt.tight_layout(); plt.savefig(NB_OUT/'perbin_variance.png', dpi=150); plt.close()\n",
    "print('Saved perbin_variance.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137bc309",
   "metadata": {},
   "source": [
    "## 5) Bundle export\n",
    "We export a compact JSON + CSV for dashboards and lessons learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe81c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bundle = {\n",
    "  'source': base_source,\n",
    "  'noise_panels': 'noise_panels.png',\n",
    "  'fft_autocorr_compare': 'fft_autocorr_noise_compare.png',\n",
    "  'perbin_variance': 'perbin_variance.png',\n",
    "  'notes': 'Educational demo; parameters are illustrative and not instrument‑calibrated.'\n",
    "}\n",
    "(NB_OUT/'radiation_noise_bundle.json').write_text(json.dumps(bundle, indent=2))\n",
    "pd.DataFrame({'wavelength_index': base['wavelength_index'], 'mu_base': mu, 'var_noise': var_bins}).to_csv(NB_OUT/'radiation_noise_detail.csv', index=False)\n",
    "print('Wrote bundle & detail CSV to', NB_OUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159447d2",
   "metadata": {},
   "source": [
    "---\n",
    "### Notes & Further Reading\n",
    "- **Shot/read/dark/background** processes and their statistics are standard detector topics; see your instrument handbook for exact models and units.\n",
    "- **Cosmic ray** rates depend on orbit and shielding; spikes must be detected/flagged to avoid biasing spectra and FFT/autocorr diagnostics.\n",
    "- For production pipeline, rely on the **calibration kill chain** and CLI diagnostics rather than these educational injectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3bc3bd3",
   "metadata": {},
   "source": [
    "## 15_gravitational_lensing_demo.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db6716f",
   "metadata": {},
   "source": [
    "# 15 · Gravitational Lensing Demo (SpectraMind V50)\n",
    "\n",
    "Educational, **mission‑grade** demo that shows how a simple microlensing event can bias *observed* transit data products when sampling/weighting effects are present. This notebook is **pipeline‑safe**:\n",
    "\n",
    "- It **does not** implement pipeline logic; it *reads* artifacts produced by the CLI (if present) or synthesizes a clean spectrum.\n",
    "- It simulates a **point‑lens microlensing** light curve during transit and demonstrates how temporal magnification + wavelength‑dependent weights can create apparent spectral biases.\n",
    "- Artifacts are written under `outputs/notebooks/15_gravitational_lensing/` for DVC tracking.\n",
    "\n",
    "**What you'll do**\n",
    "1. Load a base transmission spectrum (from `outputs/` or synthesize).\n",
    "2. Simulate microlensing magnification `A(t)` with typical point‑lens formula.\n",
    "3. Combine `A(t)` with a simple exposure model and wavelength‑dependent weights to build an **effective** spectrum.\n",
    "4. Compare original vs lensed spectra; visualize Einstein‑curve, residuals, and band overlays.\n",
    "5. Export a compact diagnostics bundle (JSON + CSV + PNGs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1718bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, json, platform\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "try:\n",
    "    import seaborn as sns\n",
    "    sns.set_context('notebook'); sns.set_style('whitegrid')\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "ROOT = Path.cwd().resolve()\n",
    "NB_OUT = ROOT / 'outputs' / 'notebooks' / '15_gravitational_lensing'\n",
    "NB_OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "ENV = {'python': platform.python_version(), 'platform': platform.platform(), 'time': __import__('datetime').datetime.utcnow().isoformat()+'Z'}\n",
    "(NB_OUT/'env_snapshot.json').write_text(json.dumps(ENV, indent=2))\n",
    "print('ROOT:', ROOT) ; print('NB_OUT:', NB_OUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c12b276",
   "metadata": {},
   "source": [
    "## 0) Background: point‑lens microlensing\n",
    "For a point lens, the (achromatic) magnification for a projected separation `u = θ/θ_E` is:\n",
    "\n",
    "$$A(u) = \\frac{u^2 + 2}{u\\,\\sqrt{u^2 + 4}}\\,.$$ \n",
    "\n",
    "With constant proper motion, `u(t) = \\sqrt{u_0^2 + ((t - t_0)/t_E)^2}`. In reality lensing is achromatic, but **effective** spectral biases can arise if magnification varies during a transit while the instrument/pipeline applies time‑/wavelength‑dependent weights (exposure timing, throughput, flagging). We illustrate this effect using a simplified toy model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eca645d",
   "metadata": {},
   "source": [
    "## 1) Load base spectrum\n",
    "We attempt to load a predictions table from `outputs/` and select one spectrum; if not found, we synthesize a smooth spectrum with two absorption features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1416100b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_candidates():\n",
    "    roots = [ROOT/'outputs']\n",
    "    pats = ['**/predictions.csv','**/mu.csv','**/spectra.npy','**/mu.npy']\n",
    "    cands = []\n",
    "    for r in roots:\n",
    "        if not r.exists():\n",
    "            continue\n",
    "        for pat in pats:\n",
    "            cands += list(r.glob(pat))\n",
    "    return sorted(set(cands), key=lambda p: p.stat().st_mtime) if cands else []\n",
    "\n",
    "def load_one_spectrum(path: Path) -> pd.DataFrame:\n",
    "    if path.suffix=='.npy':\n",
    "        arr = np.load(path)\n",
    "        if arr.ndim==1: arr = arr[None,:]\n",
    "        mu = arr[0]\n",
    "        return pd.DataFrame({'wavelength_index': np.arange(len(mu)), 'mu': mu})\n",
    "    df = pd.read_csv(path)\n",
    "    cols = {str(c).lower(): c for c in df.columns}\n",
    "    if {'planet_id','wavelength_index','mu'}.issubset(cols):\n",
    "        one = df[df[cols['planet_id']]==df[cols['planet_id']].iloc[0]].copy()\n",
    "        one = one.rename(columns={cols['wavelength_index']:'wavelength_index', cols['mu']:'mu'})\n",
    "        return one[['wavelength_index','mu']].sort_values('wavelength_index').reset_index(drop=True)\n",
    "    mu_cols = [c for c in df.columns if str(c).startswith('mu_')]\n",
    "    if mu_cols:\n",
    "        mu = df[mu_cols].iloc[0].to_numpy(float)\n",
    "        return pd.DataFrame({'wavelength_index': np.arange(len(mu)), 'mu': mu})\n",
    "    raise ValueError(f'Unsupported schema for {path}')\n",
    "\n",
    "CANDS = find_candidates()\n",
    "if not CANDS:\n",
    "    # synthesize a clean spectrum (283 bins) with two features\n",
    "    x = np.linspace(0, 1, 283)\n",
    "    mu_clean = 0.01 + 0.002*np.exp(-0.5*((x-0.30)/0.06)**2) + 0.0015*np.exp(-0.5*((x-0.70)/0.05)**2)\n",
    "    base = pd.DataFrame({'wavelength_index': np.arange(283), 'mu': mu_clean})\n",
    "    base_source = 'synthetic'\n",
    "    print('No outputs found; using synthetic spectrum.')\n",
    "else:\n",
    "    base = load_one_spectrum(CANDS[-1])\n",
    "    base_source = CANDS[-1].relative_to(ROOT).as_posix()\n",
    "    print('Loaded from:', base_source)\n",
    "base.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b5a5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,3))\n",
    "plt.plot(base['wavelength_index'], base['mu'], lw=1.5)\n",
    "plt.title('Base transmission spectrum (μ)')\n",
    "plt.xlabel('wavelength index'); plt.ylabel('μ (arb)')\n",
    "plt.tight_layout(); plt.savefig(NB_OUT/'base_spectrum.png', dpi=150); plt.close()\n",
    "print('Saved base_spectrum.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515845f6",
   "metadata": {},
   "source": [
    "## 2) Microlensing model in time\n",
    "We simulate a simple transit time series with uniform exposure spacing and a point‑lens magnification that varies across the observing window.\n",
    "\n",
    "**Parameters** (tweak to taste):\n",
    "- `u0` : minimum impact parameter in Einstein‑radius units (smaller → stronger peak).\n",
    "- `tE` : Einstein timescale half‑width (controls event duration).\n",
    "- `t0` : time of closest approach (center of peak)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7816a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def A_point_lens(u):\n",
    "    u = np.asarray(u, float)\n",
    "    return (u*u + 2) / (u * np.sqrt(u*u + 4))\n",
    "\n",
    "def u_of_t(t, t0=0.0, u0=0.2, tE=0.3):\n",
    "    return np.sqrt(u0*u0 + ((t - t0)/tE)**2)\n",
    "\n",
    "# Exposure grid (normalized time)\n",
    "N_EXP = 200\n",
    "t = np.linspace(-0.8, 0.8, N_EXP)\n",
    "A = A_point_lens(u_of_t(t, t0=0.0, u0=0.20, tE=0.25))\n",
    "\n",
    "plt.figure(figsize=(8,3))\n",
    "plt.plot(t, A, lw=1.6)\n",
    "plt.title('Microlensing magnification A(t)')\n",
    "plt.xlabel('time [arb]'); plt.ylabel('A')\n",
    "plt.tight_layout(); plt.savefig(NB_OUT/'magnification_curve.png', dpi=150); plt.close()\n",
    "print('Saved magnification_curve.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9877d7c1",
   "metadata": {},
   "source": [
    "## 3) From time to effective spectrum\n",
    "In an ideal ratio measurement, pure lensing would cancel out in the transit depth. In practice, **time‑varying magnification** during the observation coupled with **wavelength‑dependent exposure/weighting** (due to throughput, flagging, or rolling shutter timing) can produce wavelength‑dependent biases.\n",
    "\n",
    "We model this by defining per‑wavelength exposure weights `W(λ)` (normalized) and computing a weighted average across time windows that intersect the lensing peak. This toy model demonstrates a possible **effective** bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b446a9c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "wl = base['wavelength_index'].to_numpy()\n",
    "mu = base['mu'].to_numpy(float)\n",
    "W = wl / wl.max()  # simple monotonic weight vs wavelength (proxy for throughput)\n",
    "W = (W - W.min())/(W.ptp() + 1e-12)\n",
    "W = 0.5 + 0.5*W  # scale to [0.5,1.0]\n",
    "\n",
    "# Effective lensed spectrum: weight time samples by A(t) and wavelength weights W(λ)\n",
    "# Simplified: pretend each λ samples a slightly shifted time slice (e.g., readout order)\n",
    "phase_per_wl = np.linspace(-0.15, 0.15, len(wl))  # mock readout time offset per wavelength\n",
    "A_per_wl = np.interp(phase_per_wl, t, A)\n",
    "\n",
    "mu_lensed = mu * (1.0 + (A_per_wl - 1.0)* (W))  # multiplicative bias modulated by weights\n",
    "residual = mu_lensed - mu\n",
    "\n",
    "fig, ax = plt.subplots(2,1, figsize=(10,6), sharex=True)\n",
    "ax[0].plot(wl, mu, lw=1.4, label='base μ')\n",
    "ax[0].plot(wl, mu_lensed, lw=1.2, label='effective μ (with lensing+weights)')\n",
    "ax[0].set_ylabel('μ (arb)'); ax[0].legend()\n",
    "ax[1].plot(wl, residual, lw=1.2, label='residual (lensed − base)')\n",
    "ax[1].axhline(0, color='k', lw=0.7)\n",
    "ax[1].set_xlabel('wavelength index'); ax[1].set_ylabel('Δμ (arb)'); ax[1].legend()\n",
    "fig.suptitle('Effective spectrum under microlensing + wavelength weights')\n",
    "fig.tight_layout(); fig.savefig(NB_OUT/'effective_spectrum_lensing.png', dpi=150); plt.close(fig)\n",
    "print('Saved effective_spectrum_lensing.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a946821",
   "metadata": {},
   "source": [
    "### Symbolic band overlays (educational)\n",
    "We overlay two nominal water‑band index ranges to show whether apparent residuals cluster in key regions. Replace with instrument‑accurate bands for your grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4046a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYM_BANDS = {\n",
    "    'H2O_1': (120, 150),\n",
    "    'H2O_2': (180, 220)\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(10,3))\n",
    "plt.plot(wl, residual, lw=1.2)\n",
    "for name,(a,b) in SYM_BANDS.items():\n",
    "    a,b = max(0,a), min(len(wl),b)\n",
    "    if b>a:\n",
    "        plt.axvspan(wl[a], wl[b-1], alpha=0.18)\n",
    "        plt.text((wl[a]+wl[b-1])/2, residual.max()*0.9 if residual.max()!=0 else 0.0, name, ha='center', va='top', fontsize=8, alpha=0.8)\n",
    "plt.axhline(0, lw=0.7)\n",
    "plt.title('Residuals with symbolic band overlays')\n",
    "plt.xlabel('wavelength index'); plt.ylabel('Δμ (arb)')\n",
    "plt.tight_layout(); plt.savefig(NB_OUT/'residuals_with_bands.png', dpi=150); plt.close()\n",
    "print('Saved residuals_with_bands.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8769a5d9",
   "metadata": {},
   "source": [
    "## 4) Einstein‑ring radius sketch (quick look)\n",
    "We plot the classic point‑lens image separation scale as context (qualitative sketch; not used directly in spectra)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f9d89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = np.linspace(0, 2*np.pi, 400)\n",
    "x = np.cos(theta)\n",
    "y = np.sin(theta)\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.plot(x, y, lw=1.5)\n",
    "plt.gca().set_aspect('equal', adjustable='box')\n",
    "plt.title('Einstein ring (unit radius sketch)')\n",
    "plt.axis('off')\n",
    "plt.tight_layout(); plt.savefig(NB_OUT/'einstein_ring_sketch.png', dpi=150); plt.close()\n",
    "print('Saved einstein_ring_sketch.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc61abb9",
   "metadata": {},
   "source": [
    "## 5) Export bundle\n",
    "We save a compact JSON bundle and CSV for dashboards/teaching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bf659e",
   "metadata": {},
   "outputs": [],
   "source": [
    "bundle = {\n",
    "  'base_source': base_source,\n",
    "  'n_exp': int(len(np.linspace(-0.8,0.8,200))),\n",
    "  'figures': [\n",
    "      'base_spectrum.png',\n",
    "      'magnification_curve.png',\n",
    "      'effective_spectrum_lensing.png',\n",
    "      'residuals_with_bands.png',\n",
    "      'einstein_ring_sketch.png'\n",
    "  ],\n",
    "  'notes': 'Toy model: achromatic microlensing + wavelength/time weighting may yield effective spectral bias.'\n",
    "}\n",
    "(NB_OUT/'lensing_demo_bundle.json').write_text(json.dumps(bundle, indent=2))\n",
    "import pandas as pd\n",
    "pd.DataFrame({'wavelength_index': wl, 'mu_base': mu, 'mu_lensed': mu_lensed, 'residual': residual}).to_csv(NB_OUT/'lensing_demo_detail.csv', index=False)\n",
    "print('Wrote bundle & detail CSV to', NB_OUT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e8d3fe",
   "metadata": {},
   "source": [
    "## 6) (Optional) DVC add\n",
    "Register outputs for full reproducibility if your project uses DVC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e037fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil, subprocess\n",
    "if shutil.which('dvc'):\n",
    "    try:\n",
    "        subprocess.run(['dvc','add', str(NB_OUT)], check=False)\n",
    "        subprocess.run(['git','add', f'{NB_OUT}.dvc', '.gitignore'], check=False)\n",
    "        subprocess.run(['dvc','status'], check=False)\n",
    "        print('DVC add done (non‑blocking).')\n",
    "    except Exception as e:\n",
    "        print('DVC step failed (non‑blocking):', e)\n",
    "else:\n",
    "    print('DVC not found; skipping.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b8e6a46",
   "metadata": {},
   "source": [
    "---\n",
    "### Notes & caveats\n",
    "- **Achromatic** lensing by itself does *not* change intrinsic spectral features; apparent biases here arise from simplified time/weight coupling used for demonstration.\n",
    "- For realistic use, replace the weight model `W(λ)` and timing offsets with instrument‑specific readout/throughput and sampling.\n",
    "- Keep production spectra in the CLI calibration/prediction pipeline; use this notebook for physics intuition and QA visualization only."
   ]
  }
 ],
 "metadata": {
  "authors": [
   "SpectraMind V50 Team"
  ],
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  },
  "spectramind": {
   "cli_first": true,
   "outputs_dir": "outputs/notebooks/master_walkthrough",
   "reproducibility": {
    "dvc": true,
    "hydra": true,
    "logs": true
   },
   "role": "master/walkthrough"
  },
  "title": "SpectraMind V50 — Master Walkthrough (One Notebook)"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
