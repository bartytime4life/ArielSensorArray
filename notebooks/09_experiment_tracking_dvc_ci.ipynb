{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61b39026",
   "metadata": {},
   "source": [
    "# ðŸ§ª SpectraMind V50 â€” Experiment Tracking, DVC DAG & CI (Notebook 09)\n",
    "\n",
    "**Purpose.** Close the reproducibility loop by wiring **experiment tracking**, **DVC pipeline/DAG**, and **CI artifacts**:\n",
    "- Track runs (config â†’ metrics â†’ artifacts) with **MLflow** (or structured JSON fallback).\n",
    "- Define/validate a **DVC DAG** for calibration â†’ train â†’ predict â†’ diagnostics stages.\n",
    "- Emit CI-friendly artifacts (JSON summaries, HTML reports) and run **self-test** hooks.\n",
    "\n",
    "This continues the sequence after 08 (ensembles/MC Dropout/COREL) and focuses on **engineering rigor**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4310ed67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â–‘â–‘ Pre-flight: env, run IDs, paths, CLI detection â–‘â–‘\n",
    "import os, sys, json, platform, shutil, subprocess, datetime, pathlib\n",
    "\n",
    "RUN_TS = datetime.datetime.utcnow().strftime(\"%Y%m%dT%H%M%SZ\")\n",
    "RUN_ID = f\"tracking_dvc_ci_{RUN_TS}\"\n",
    "ROOT_OUT = \"/mnt/data/tracking_dvc_ci\"\n",
    "ARTIFACTS = os.path.join(ROOT_OUT, RUN_ID)\n",
    "LOGS = os.path.join(ARTIFACTS, \"logs\")\n",
    "CFG_OUT = os.path.join(ARTIFACTS, \"configs\")\n",
    "CI_OUT = os.path.join(ARTIFACTS, \"ci_artifacts\")\n",
    "MLF_OUT = os.path.join(ARTIFACTS, \"mlruns\")  # default local store if mlflow used\n",
    "for p in (ROOT_OUT, ARTIFACTS, LOGS, CFG_OUT, CI_OUT, MLF_OUT):\n",
    "    os.makedirs(p, exist_ok=True)\n",
    "\n",
    "def which(cmd: str) -> bool:\n",
    "    return shutil.which(cmd) is not None\n",
    "\n",
    "CLI_PRESENT = which(\"spectramind\")\n",
    "DRY_RUN = not CLI_PRESENT\n",
    "\n",
    "def git_cmd(args):\n",
    "    try:\n",
    "        out = subprocess.check_output([\"git\", *args], stderr=subprocess.STDOUT, timeout=5).decode().strip()\n",
    "        return out\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "env = {\n",
    "    \"python\": sys.version.replace(\"\\n\", \" \"),\n",
    "    \"platform\": platform.platform(),\n",
    "    \"cli_present\": CLI_PRESENT,\n",
    "    \"dry_run\": DRY_RUN,\n",
    "    \"run_id\": RUN_ID,\n",
    "    \"paths\": {\"artifacts\": ARTIFACTS, \"logs\": LOGS, \"configs\": CFG_OUT, \"ci\": CI_OUT, \"mlruns\": MLF_OUT},\n",
    "    \"git\": {\n",
    "        \"commit\": git_cmd([\"rev-parse\", \"HEAD\"]),\n",
    "        \"branch\": git_cmd([\"rev-parse\", \"--abbrev-ref\", \"HEAD\"]),\n",
    "        \"status\": git_cmd([\"status\", \"--porcelain\"]),\n",
    "    },\n",
    "}\n",
    "with open(os.path.join(ARTIFACTS, \"env.json\"), \"w\") as f:\n",
    "    json.dump(env, f, indent=2)\n",
    "\n",
    "print(\"=== SpectraMind V50 â€” Notebook 09 ===\")\n",
    "print(json.dumps(env, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d805e24",
   "metadata": {},
   "source": [
    "## Experiment Tracking: MLflow (with structured JSON fallback)\n",
    "\n",
    "We attempt to use **MLflow**; if unavailable, we record runs in `events.jsonl` + `summary.json` so CI can ingest metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3013b3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "MLFLOW_OK = False\n",
    "try:\n",
    "    import mlflow  # type: ignore\n",
    "    MLFLOW_OK = True\n",
    "except Exception:\n",
    "    MLFLOW_OK = False\n",
    "\n",
    "print(\"MLflow available:\", MLFLOW_OK)\n",
    "EVENTS = os.path.join(ARTIFACTS, \"events.jsonl\")\n",
    "SUMMARY = os.path.join(ARTIFACTS, \"summary.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4f8256",
   "metadata": {},
   "source": [
    "## Compose Hydra overrides for a small demo run\n",
    "\n",
    "We keep it light here; use your ablation tooling for full sweeps. The goal is to demonstrate **tracked runs**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a5289a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os\n",
    "\n",
    "overrides = {\n",
    "    \"data\": \"ariel_nominal\",\n",
    "    \"model\": \"v50\",\n",
    "    \"training.max_epochs\": \"3\",\n",
    "    \"training.batch_size\": \"16\",\n",
    "    \"training.seed\": \"2025\",\n",
    "    # Diagnostics toggles for predictable outputs\n",
    "    \"diagnostics.export_json\": \"true\",\n",
    "}\n",
    "cfg_file = os.path.join(CFG_OUT, \"tracking_overrides.json\")\n",
    "with open(cfg_file, \"w\") as f:\n",
    "    json.dump(overrides, f, indent=2)\n",
    "print(\"Saved overrides ->\", cfg_file)\n",
    "print(json.dumps(overrides, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e15f54",
   "metadata": {},
   "source": [
    "## Helper: robust CLI runner (DRY-RUN if CLI not present)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2914f252",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shlex, time\n",
    "\n",
    "def run_cli(cmd_list, log_name=\"run\"):\n",
    "    log_path = os.path.join(LOGS, f\"{log_name}.log\")\n",
    "    err_path = os.path.join(LOGS, f\"{log_name}.err\")\n",
    "    start = time.time()\n",
    "    result = {\"cmd\": cmd_list, \"dry_run\": DRY_RUN, \"returncode\": 0, \"stdout\": \"\", \"stderr\": \"\"}\n",
    "    if DRY_RUN:\n",
    "        msg = f\"[DRY-RUN] Would execute: {' '.join(shlex.quote(c) for c in cmd_list)}\\n\"\n",
    "        result[\"stdout\"] = msg\n",
    "        with open(log_path, \"w\") as f: f.write(msg)\n",
    "        with open(err_path, \"w\") as f: f.write(\"\")\n",
    "        with open(os.path.join(ARTIFACTS, \"events.jsonl\"), \"a\") as ev:\n",
    "            ev.write(json.dumps({\"ts\": RUN_TS, \"event\": \"dry_run\", \"cmd\": result[\"cmd\"]}) + \"\\n\")\n",
    "        return result\n",
    "\n",
    "    with open(log_path, \"wb\") as out, open(err_path, \"wb\") as err:\n",
    "        try:\n",
    "            proc = subprocess.Popen(cmd_list, stdout=out, stderr=err, env=os.environ.copy())\n",
    "            proc.wait()\n",
    "            result[\"returncode\"] = proc.returncode\n",
    "        except Exception as e:\n",
    "            result[\"returncode\"] = 99\n",
    "            with open(err_path, \"ab\") as errf:\n",
    "                errf.write(str(e).encode())\n",
    "\n",
    "    try: result[\"stdout\"] = open(log_path, \"r\").read()\n",
    "    except Exception: pass\n",
    "    try: result[\"stderr\"] = open(err_path, \"r\").read()\n",
    "    except Exception: pass\n",
    "    result[\"elapsed_sec\"] = round(time.time() - start, 3)\n",
    "    print(f\"[rc={result['returncode']}] logs: {log_path}\")\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229c60b2",
   "metadata": {},
   "source": [
    "## Tracked demo run: `train â†’ predict â†’ diagnose`\n",
    "\n",
    "- If MLflow is present, we start a run, log params/metrics/artifacts.\n",
    "- Otherwise, we append to `events.jsonl` and assemble `summary.json` for CI.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326dd5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid, time, json, os, glob\n",
    "\n",
    "run_uid = str(uuid.uuid4())\n",
    "\n",
    "def log_fallback(record: dict):\n",
    "    with open(EVENTS, \"a\") as ev:\n",
    "        ev.write(json.dumps(record) + \"\\n\")\n",
    "\n",
    "# Start run\n",
    "if MLFLOW_OK:\n",
    "    mlflow.set_tracking_uri(\"file://\" + os.path.abspath(MLF_OUT))\n",
    "    mlflow.set_experiment(\"spectramind_v50\")\n",
    "    mlflow.start_run(run_name=f\"demo_{run_uid}\")\n",
    "    mlflow.log_params(overrides)\n",
    "else:\n",
    "    log_fallback({\"event\": \"start_run\", \"run_id\": run_uid, \"params\": overrides, \"ts\": time.time()})\n",
    "\n",
    "# train\n",
    "cmd_train = [\"spectramind\", \"train\", \"--config-name\", \"config_v50.yaml\", \"+outputs.root_dir=\" + ARTIFACTS]\n",
    "for k, v in overrides.items():\n",
    "    cmd_train.append(f\"+{k}={v}\")\n",
    "res_tr = run_cli(cmd_train, log_name=\"01_train\")\n",
    "if MLFLOW_OK:\n",
    "    mlflow.log_text(res_tr[\"stdout\"][:1000], \"logs/train_head.txt\")\n",
    "else:\n",
    "    log_fallback({\"event\": \"train_done\", \"rc\": res_tr[\"returncode\"], \"ts\": time.time()})\n",
    "\n",
    "# predict\n",
    "pred_dir = os.path.join(ARTIFACTS, \"pred\")\n",
    "os.makedirs(pred_dir, exist_ok=True)\n",
    "cmd_pred = [\"spectramind\", \"predict\", \"--config-name\", \"config_v50.yaml\", \"+load.from_checkpoint=true\", \"+outputs.root_dir=\" + pred_dir]\n",
    "for k, v in overrides.items():\n",
    "    cmd_pred.append(f\"+{k}={v}\")\n",
    "cmd_pred += [\"+inference.save_mu_sigma=true\"]\n",
    "res_pr = run_cli(cmd_pred, log_name=\"02_predict\")\n",
    "if MLFLOW_OK:\n",
    "    mlflow.log_text(res_pr[\"stdout\"][:1000], \"logs/predict_head.txt\")\n",
    "else:\n",
    "    log_fallback({\"event\": \"predict_done\", \"rc\": res_pr[\"returncode\"], \"ts\": time.time()})\n",
    "\n",
    "# diagnose dashboard\n",
    "diag_dir = os.path.join(ARTIFACTS, \"diagnostics\")\n",
    "os.makedirs(diag_dir, exist_ok=True)\n",
    "dashboard = os.path.join(diag_dir, \"diagnostic_report_v1.html\")\n",
    "cmd_diag = [\"spectramind\", \"diagnose\", \"dashboard\", \"--out\", dashboard]\n",
    "res_dg = run_cli(cmd_diag, log_name=\"03_diagnose\")\n",
    "if MLFLOW_OK:\n",
    "    if os.path.exists(dashboard):\n",
    "        mlflow.log_artifact(dashboard, artifact_path=\"reports\")\n",
    "    mlflow.log_text(res_dg[\"stdout\"][:1000], \"logs/diagnose_head.txt\")\n",
    "else:\n",
    "    log_fallback({\"event\": \"diagnose_done\", \"rc\": res_dg[\"returncode\"], \"dashboard\": dashboard, \"ts\": time.time()})\n",
    "\n",
    "# Finish run\n",
    "if MLFLOW_OK:\n",
    "    # Example metric emit (replace with real metrics from diagnostics JSON if available)\n",
    "    mlflow.log_metric(\"demo_metric_gll\", 0.0)\n",
    "    mlflow.end_run()\n",
    "else:\n",
    "    with open(SUMMARY, \"w\") as f:\n",
    "        json.dump({\"run_id\": run_uid, \"metrics\": {\"demo_metric_gll\": 0.0}}, f, indent=2)\n",
    "print(\"Tracking complete.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd10ae33",
   "metadata": {},
   "source": [
    "## DVC DAG: emit stage templates\n",
    "\n",
    "We write **DVC stage YAML fragments** for `calibrate â†’ train â†’ predict â†’ diagnose` so you can paste or integrate into `dvc.yaml`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6c6312",
   "metadata": {},
   "outputs": [],
   "source": [
    "dvc_frag = os.path.join(ARTIFACTS, \"dvc_stages.yaml\")\n",
    "frag = f\"\"\"\n",
    "stages:\n",
    "  calibrate:\n",
    "    cmd: spectramind calibrate --config-name config_v50.yaml\n",
    "    deps:\n",
    "    - configs\n",
    "    outs:\n",
    "    - data/calibrated\n",
    "  train:\n",
    "    cmd: spectramind train --config-name config_v50.yaml\n",
    "    deps:\n",
    "    - data/calibrated\n",
    "    - configs\n",
    "    outs:\n",
    "    - models/v50_checkpoint\n",
    "  predict:\n",
    "    cmd: spectramind predict --config-name config_v50.yaml +load.from_checkpoint=true\n",
    "    deps:\n",
    "    - models/v50_checkpoint\n",
    "    outs:\n",
    "    - outputs/predictions\n",
    "  diagnose:\n",
    "    cmd: spectramind diagnose dashboard --out outputs/diagnostic_report_v1.html\n",
    "    deps:\n",
    "    - outputs/predictions\n",
    "    outs:\n",
    "    - outputs/diagnostic_report_v1.html\n",
    "\"\"\"\n",
    "open(dvc_frag, \"w\").write(frag)\n",
    "print(\"Wrote DVC stage template ->\", dvc_frag)\n",
    "print(open(dvc_frag).read())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4b769a",
   "metadata": {},
   "source": [
    "## CI artifacts: package JSON summaries & HTML dashboard\n",
    "\n",
    "We copy key files to the `ci_artifacts/` folder so a CI job can always upload them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5be83bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil, os, glob\n",
    "\n",
    "# Collect likely artifacts if present; tolerate absence in DRY-RUN\n",
    "maybe = []\n",
    "for path in [\n",
    "    os.path.join(ARTIFACTS, \"diagnostics\", \"diagnostic_report_v1.html\"),\n",
    "    os.path.join(ARTIFACTS, \"summary.json\"),\n",
    "    os.path.join(ARTIFACTS, \"events.jsonl\"),\n",
    "]:\n",
    "    if os.path.exists(path):\n",
    "        maybe.append(path)\n",
    "\n",
    "for pth in maybe:\n",
    "    shutil.copy2(pth, CI_OUT)\n",
    "\n",
    "print(\"CI bundle contains:\", os.listdir(CI_OUT))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9760b559",
   "metadata": {},
   "source": [
    "## Browse produced artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e200c903",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def tree(path, prefix=\"\"):\n",
    "    items = sorted(os.listdir(path))\n",
    "    lines = []\n",
    "    for i, name in enumerate(items):\n",
    "        full = os.path.join(path, name)\n",
    "        connector = \"â””â”€â”€ \" if i == len(items)-1 else \"â”œâ”€â”€ \"\n",
    "        lines.append(prefix + connector + name)\n",
    "        if os.path.isdir(full):\n",
    "            extension = \"    \" if i == len(items)-1 else \"â”‚   \"\n",
    "            lines.extend(tree(full, prefix + extension))\n",
    "    return lines\n",
    "\n",
    "print(\"ARTIFACTS TREE:\", ARTIFACTS)\n",
    "print(\"\\n\".join(tree(ARTIFACTS)))\n",
    "print(\"CI artifacts:\", os.listdir(os.path.join(ARTIFACTS, \"ci_artifacts\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ea5bea",
   "metadata": {},
   "source": [
    "## Pipeline sketch (Mermaid)\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "  A[Hydra config] --> B[Calibrate]\n",
    "  B --> C[Train (tracked)]\n",
    "  C --> D[Predict]\n",
    "  D --> E[Diagnose â†’ HTML]\n",
    "  C --> F[Metrics JSON/MLflow]\n",
    "  E --> G[CI Upload]\n",
    "  F --> G\n",
    "```\n",
    "\n",
    "## Next steps\n",
    "- Wire **real metrics** from diagnostics JSON into MLflow/summary outputs.\n",
    "- Paste `dvc_stages.yaml` content into `dvc.yaml` and `dvc repro` to validate the DAG on small data.\n",
    "- Hook this notebook (or the CLI) into **CI** to auto-run `selftest` + produce a **CI bundle** on every PR.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
