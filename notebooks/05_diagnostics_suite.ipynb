{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 05 · Diagnostics Suite (SpectraMind V50)\n",
        "\n",
        "Unified, mission‑grade diagnostics for the SpectraMind V50 pipeline:\n",
        "\n",
        "- Auto‑locate the latest run outputs\n",
        "- Generate summary reports via the CLI\n",
        "- Visualize training curves, spectra, residuals, UMAP/t‑SNE, and rule overlays\n",
        "- Compare multiple runs side‑by‑side\n",
        "\n",
        "> Notes:\n",
        "> * This notebook assumes the SpectraMind V50 repository is the working directory and the `spectramind` Typer/Hy\-dra CLI is installed in the active environment.\n",
        "> * All plots and reports are read from (or written to) your `outputs/` tree to keep artifacts versioned and reproducible.\n",
        "\n",
        "_Refs: CLI/Hy\-dra + logging flow; reproducible outputs under `outputs/`; Rich/plots for diagnostics._"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [
          "parameters"
        ]
      },
      "outputs": [],
      "source": [
        "# --- Notebook Parameters (edit as needed) ---\n",
        "PROJECT_ROOT = \".\"  # path to repo root\n",
        "OUTPUTS_ROOT = \"outputs\"  # default outputs dir\n",
        "\n",
        "# If you want to force a (re)run of the diagnostics via CLI on an existing run folder\n",
        "RUN_DIAGNOSTICS_IF_MISSING = True\n",
        "\n",
        "# Optional: explicit run directory (under OUTPUTS_ROOT). If None, auto-pick the most recent.\n",
        "RUN_DIR = None  # e.g., \"2025-08-20/11-52-30_v50_sweep_A\"\n",
        "\n",
        "# Optional: list of run dirs for side-by-side comparisons; leave empty to skip.\n",
        "COMPARE_RUN_DIRS = []  # e.g., [\"2025-08-20/11-52-30_v50_sweep_A\", \"2025-08-21/09-07-12_v50_mamba_gnn\"]\n",
        "\n",
        "# Hydra overrides for on-demand diagnostics generation (when RUN_DIAGNOSTICS_IF_MISSING=True)\n",
        "HYDRA_OVERRIDES = [\n",
        "    # Example overrides; adjust to your config hierarchy\n",
        "    # \"diagnostics.generate_plots=true\",\n",
        "    # \"diagnostics.umap.n_neighbors=30\",\n",
        "    # \"diagnostics.tsne.perplexity=40\"\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 0. Environment checks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "import os, sys, json, re, subprocess, shutil, glob\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from typing import List, Optional, Tuple\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.rcParams.update({\n",
        "    \"figure.figsize\": (10, 4.8),\n",
        "    \"axes.grid\": True,\n",
        "    \"axes.spines.top\": False,\n",
        "    \"axes.spines.right\": False\n",
        "})\n",
        "\n",
        "ROOT = Path(PROJECT_ROOT).resolve()\n",
        "OUT_ROOT = (ROOT / OUTPUTS_ROOT).resolve()\n",
        "print(f\"Repo root:     {ROOT}\")\n",
        "print(f\"Outputs root:  {OUT_ROOT}\")\n",
        "\n",
        "# Quick checks\n",
        "if not OUT_ROOT.exists():\n",
        "    raise FileNotFoundError(f\"Outputs directory not found: {OUT_ROOT}\")\n",
        "\n",
        "# Try to find CLI\n",
        "def _which(cmd: str) -> Optional[str]:\n",
        "    path = shutil.which(cmd)\n",
        "    return path\n",
        "\n",
        "CLI_PATH = _which(\"spectramind\")\n",
        "print(\"spectramind CLI:\", CLI_PATH or \"<not found on PATH>\")\n",
        "if CLI_PATH is None:\n",
        "    print(\"WARNING: spectramind CLI not found on PATH. You can still view existing artifacts.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "def list_run_dirs(outputs_root: Path) -> List[Path]:\n",
        "    \"\"\"Return a list of run directories under outputs/, sorted by mtime descending.\"\"\"\n",
        "    # Accept two common layouts:\n",
        "    #  A) outputs/YYYY-MM-DD/HH-MM-SS_*   (hierarchical date folders)\n",
        "    #  B) outputs/runs/<run_name_or_timestamp>\n",
        "    candidates = []\n",
        "    # Layout A\n",
        "    for date_dir in sorted(outputs_root.glob(\"20*\")):\n",
        "        if date_dir.is_dir():\n",
        "            for run_dir in date_dir.iterdir():\n",
        "                if run_dir.is_dir():\n",
        "                    candidates.append(run_dir)\n",
        "    # Layout B\n",
        "    runs_root = outputs_root / \"runs\"\n",
        "    if runs_root.exists():\n",
        "        candidates.extend([p for p in runs_root.iterdir() if p.is_dir()])\n",
        "\n",
        "    # Dedup and sort by modified time\n",
        "    uniq = sorted(set(candidates), key=lambda p: p.stat().st_mtime, reverse=True)\n",
        "    return uniq\n",
        "\n",
        "def pick_latest_run(outputs_root: Path) -> Optional[Path]:\n",
        "    runs = list_run_dirs(outputs_root)\n",
        "    return runs[0] if runs else None\n",
        "\n",
        "def find_artifacts(run_dir: Path) -> dict:\n",
        "    \"\"\"Discover common artifacts produced by the pipeline/diagnostics.\"\"\"\n",
        "    artifacts = {\n",
        "        \"config\": None,\n",
        "        \"manifest\": None,\n",
        "        \"metrics_csv\": None,\n",
        "        \"loss_curve\": None,\n",
        "        \"spectra_preview\": None,\n",
        "        \"residuals\": None,\n",
        "        \"umap_png\": None,\n",
        "        \"tsne_png\": None,\n",
        "        \"rule_overlay\": None,\n",
        "        \"html_report\": None\n",
        "    }\n",
        "\n",
        "    # Common file names/locations (adjust if your repo uses different names):\n",
        "    candidates = {\n",
        "        \"config\": [run_dir / \"config.yaml\", run_dir / \".hydra\" / \"config.yaml\"],\n",
        "        \"manifest\": [run_dir / \"run_manifest.json\", run_dir / \"manifest.json\"],\n",
        "        \"metrics_csv\": [run_dir / \"metrics.csv\", run_dir / \"training_metrics.csv\"],\n",
        "        \"loss_curve\": [run_dir / \"loss_curve.png\", run_dir / \"plots\" / \"loss_curve.png\"],\n",
        "        \"spectra_preview\": [run_dir / \"spectra_preview.png\", run_dir / \"plots\" / \"spectra.png\"],\n",
        "        \"residuals\": [run_dir / \"residuals.png\", run_dir / \"plots\" / \"residuals.png\"],\n",
        "        \"umap_png\": [run_dir / \"umap.png\", run_dir / \"plots\" / \"umap.png\"],\n",
        "        \"tsne_png\": [run_dir / \"tsne.png\", run_dir / \"plots\" / \"tsne.png\"],\n",
        "        \"rule_overlay\": [run_dir / \"rule_overlay.png\", run_dir / \"plots\" / \"rule_overlay.png\"],\n",
        "        \"html_report\": [run_dir / \"diagnostics.html\", run_dir / \"reports\" / \"diagnostics.html\"]\n",
        "    }\n",
        "    for k, paths in candidates.items():\n",
        "        for p in paths:\n",
        "            if p.exists():\n",
        "                artifacts[k] = p\n",
        "                break\n",
        "    return artifacts\n",
        "\n",
        "def run_cli_diagnostics(run_dir: Path, overrides: Optional[List[str]] = None) -> int:\n",
        "    \"\"\"Call the spectramind CLI to (re)generate diagnostics for a given run directory.\n",
        "    Returns exit code (0 on success).\"\"\"\n",
        "    if CLI_PATH is None:\n",
        "        print(\"spectramind CLI not available; skipping generation.\")\n",
        "        return 0\n",
        "    cmd = [CLI_PATH, \"diagnose\", f\"run_dir={str(run_dir)}\"]\n",
        "    if overrides:\n",
        "        cmd.extend(overrides)\n",
        "    print(\"\\n$\", \" \".join(cmd))\n",
        "    return subprocess.call(cmd)\n",
        "\n",
        "def load_metrics_csv(path: Optional[Path]) -> Optional[pd.DataFrame]:\n",
        "    if path and path.exists():\n",
        "        try:\n",
        "            df = pd.read_csv(path)\n",
        "            return df\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to load metrics CSV: {e}\")\n",
        "    return None\n",
        "\n",
        "def show_image(path: Optional[Path], title: str):\n",
        "    if not path or not path.exists():\n",
        "        print(f\"[missing] {title}: {path}\")\n",
        "        return\n",
        "    img = plt.imread(str(path))\n",
        "    plt.figure(figsize=(9.5, 4.8))\n",
        "    plt.imshow(img)\n",
        "    plt.axis('off')\n",
        "    plt.title(title)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Select run directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "if RUN_DIR is None:\n",
        "    run_dir = pick_latest_run(OUT_ROOT)\n",
        "else:\n",
        "    run_dir = (OUT_ROOT / RUN_DIR).resolve()\n",
        "\n",
        "print(\"Selected run dir:\", run_dir)\n",
        "if run_dir is None or not run_dir.exists():\n",
        "    raise FileNotFoundError(\"No run directory found. Configure RUN_DIR or generate a run first.\")\n",
        "\n",
        "artifacts = find_artifacts(run_dir)\n",
        "artifacts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. (Optional) Generate diagnostics via CLI\n",
        "\n",
        "If key diagnostics artifacts are missing, we can ask the CLI to (re)generate them for the selected run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "missing_core = [\n",
        "    k for k in (\"loss_curve\", \"metrics_csv\", \"html_report\")\n",
        "    if artifacts.get(k) is None\n",
        "]\n",
        "print(\"Missing core artifacts:\", missing_core)\n",
        "if RUN_DIAGNOSTICS_IF_MISSING and missing_core:\n",
        "    rc = run_cli_diagnostics(run_dir, overrides=HYDRA_OVERRIDES)\n",
        "    print(\"CLI return code:\", rc)\n",
        "    # Refresh artifact discovery\n",
        "    artifacts = find_artifacts(run_dir)\n",
        "    print(\"Artifacts after CLI run:\\n\", artifacts)\n",
        "else:\n",
        "    print(\"Skipping CLI diagnostics generation.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Summaries & config snapshot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Print config snapshot if available\n",
        "cfg_path = artifacts.get(\"config\")\n",
        "if cfg_path and cfg_path.exists():\n",
        "    print(\"— Config snapshot:\")\n",
        "    try:\n",
        "        with open(cfg_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            print(f.read()[:4000])  # preview first ~4KB\n",
        "    except Exception as e:\n",
        "        print(\"Failed to print config:\", e)\n",
        "else:\n",
        "    print(\"No config snapshot found.\")\n",
        "\n",
        "# Print manifest (run metadata) if present\n",
        "manifest_path = artifacts.get(\"manifest\")\n",
        "if manifest_path and manifest_path.exists():\n",
        "    try:\n",
        "        manifest = json.loads(manifest_path.read_text())\n",
        "        print(\"\\n— Run manifest (keys):\", list(manifest.keys()))\n",
        "        print(json.dumps({k: manifest[k] for k in list(manifest.keys())[:10]}, indent=2)[:2000])\n",
        "    except Exception as e:\n",
        "        print(\"Failed to parse manifest:\", e)\n",
        "else:\n",
        "    print(\"No manifest found.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Training metrics & curves"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "metrics_df = load_metrics_csv(artifacts.get(\"metrics_csv\"))\n",
        "if metrics_df is not None:\n",
        "    display(metrics_df.head())\n",
        "    # Plot common metrics if present\n",
        "    candidates = [\n",
        "        (\"train_loss\", \"Train Loss\"),\n",
        "        (\"val_loss\", \"Val Loss\"),\n",
        "        (\"gll\", \"Gaussian Log-Likelihood\"),\n",
        "        (\"val_gll\", \"Val Gaussian Log-Likelihood\")\n",
        "    ]\n",
        "    plt.figure(figsize=(10, 4.8))\n",
        "    for col, label in candidates:\n",
        "        if col in metrics_df.columns:\n",
        "            plt.plot(metrics_df[col], label=label)\n",
        "    plt.xlabel(\"Epoch/Step\")\n",
        "    plt.ylabel(\"Metric\")\n",
        "    plt.title(\"Training Metrics\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"metrics.csv not found; showing saved loss curve (if any).\")\n",
        "\n",
        "show_image(artifacts.get(\"loss_curve\"), \"Loss Curve (saved)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Spectra & residuals previews"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "show_image(artifacts.get(\"spectra_preview\"), \"Spectra Preview\")\n",
        "show_image(artifacts.get(\"residuals\"), \"Residuals Preview\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Embeddings (UMAP / t‑SNE) & rule overlays"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "show_image(artifacts.get(\"umap_png\"), \"UMAP (latent / features)\")\n",
        "show_image(artifacts.get(\"tsne_png\"), \"t‑SNE (latent / features)\")\n",
        "show_image(artifacts.get(\"rule_overlay\"), \"Symbolic Rule Overlay / Violations Map\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. HTML diagnostics report (if generated by CLI)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "html_report = artifacts.get(\"html_report\")\n",
        "if html_report and html_report.exists():\n",
        "    from IPython.display import IFrame, display\n",
        "    display(IFrame(src=str(html_report), width=\"100%\", height=600))\n",
        "else:\n",
        "    print(\"No Diagnostics HTML report found.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Compare multiple runs (optional)\n",
        "Give a list of run directory names (relative to `outputs/`) in `COMPARE_RUN_DIRS` to compare curves and metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "def load_metrics_for_run(run_rel: str) -> Tuple[str, Optional[pd.DataFrame], dict]:\n",
        "    p = (OUT_ROOT / run_rel).resolve()\n",
        "    arts = find_artifacts(p)\n",
        "    m = load_metrics_csv(arts.get(\"metrics_csv\"))\n",
        "    return run_rel, m, arts\n",
        "\n",
        "if COMPARE_RUN_DIRS:\n",
        "    runs_loaded = [load_metrics_for_run(r) for r in COMPARE_RUN_DIRS]\n",
        "\n",
        "    # Plot loss for each run\n",
        "    plt.figure(figsize=(10, 4.8))\n",
        "    any_plotted = False\n",
        "    for name, df, _arts in runs_loaded:\n",
        "        if df is not None and \"val_loss\" in df.columns:\n",
        "            plt.plot(df[\"val_loss\"], label=f\"{name} : val_loss\")\n",
        "            any_plotted = True\n",
        "    if any_plotted:\n",
        "        plt.title(\"Val Loss (comparison)\")\n",
        "        plt.xlabel(\"Epoch/Step\")\n",
        "        plt.legend()\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"No comparable 'val_loss' series across selected runs.\")\n",
        "\n",
        "    # If UMAP images exist for each, show in a grid preview\n",
        "    from math import ceil\n",
        "    umap_imgs = [(name, _arts.get(\"umap_png\")) for name, _df, _arts in runs_loaded if _arts.get(\"umap_png\")]\n",
        "    if umap_imgs:\n",
        "        cols = 2\n",
        "        rows = ceil(len(umap_imgs)/cols)\n",
        "        fig, axes = plt.subplots(rows, cols, figsize=(cols*6, rows*4))\n",
        "        axes = np.array(axes).reshape(-1)\n",
        "        for ax, (name, path) in zip(axes, umap_imgs):\n",
        "            img = plt.imread(str(path))\n",
        "            ax.imshow(img)\n",
        "            ax.axis('off')\n",
        "            ax.set_title(f\"UMAP: {name}\")\n",
        "        for ax in axes[len(umap_imgs):]:\n",
        "            ax.axis('off')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "else:\n",
        "    print(\"No COMPARE_RUN_DIRS configured; skipping comparison.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Export compact run summary\n",
        "Save a machine‑readable summary JSON combining key pointers and the last metrics row."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": [
        "summary = {\n",
        "    \"run_dir\": str(run_dir),\n",
        "    \"artifacts\": {k: (str(v) if v else None) for k, v in artifacts.items()},\n",
        "}\n",
        "if metrics_df is not None and len(metrics_df) > 0:\n",
        "    summary[\"last_metrics_row\"] = metrics_df.iloc[-1].to_dict()\n",
        "else:\n",
        "    summary[\"last_metrics_row\"] = None\n",
        "\n",
        "summary_path = run_dir / \"diagnostics_summary.json\"\n",
        "with open(summary_path, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "print(\"Wrote:\", summary_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "**Provenance** (design/flow per SpectraMind V50 plan: Typer/Hy‑dra CLI, reproducible `outputs/`, Rich diagnostics; optional GUI later⁠—kept CLI‑first)."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    },
    "colab": {
      "name": "05_diagnostics_suite.ipynb"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}