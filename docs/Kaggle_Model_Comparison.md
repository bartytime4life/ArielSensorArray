# SpectraMind V50 â€” Kaggle Model Comparison  
*NeurIPS 2025 Ariel Data Challenge*

This document reviews three notable Kaggle models that participated in the NeurIPS 2025 Ariel Data Challenge, focused on retrieving **exoplanet transmission spectra** from Ariel telescope simulations.  

We analyze their **architecture, training pipelines, leaderboard performance, and relevance** to the SpectraMind V50 project, and extract best practices for building a **neuro-symbolic, physics-informed, uncertainty-calibrated system**.

---

## ğŸ“Š Models Reviewed
1. **Thang Do Duc â€” â€œ0.329 LBâ€ Baseline**  
   Fork of V1ctorious3010â€™s reference MLP model.  
2. **V1ctorious3010 â€” â€œ80bl-128hd-impactâ€**  
   Extremely deep residual MLP (~80 blocks Ã— 128 dims).  
3. **Fawad Awan â€” â€œSpectrum Regressorâ€**  
   Multi-output PyTorch regressor with moderate complexity.  

---

## ğŸ” Model Details

### 1. Thang Do Duc â€” â€œ0.329 LBâ€ Baseline
- **Architecture:**  
  Residual fully connected MLP. Dense layers with BatchNorm + ReLU + skip connections (ResNet-style).  
- **Training & Preprocessing:**  
  - Minimal preprocessing (basic normalization).  
  - Trains quickly (~56 seconds on Kaggle T4 GPU).  
- **Performance:**  
  - Public LB ~**0.329**.  
  - Stable across dataset updates (drop to 0.287, later recovered).  
- **Strengths:**  
  - Simple, reproducible, fast.  
  - Strong reference baseline.  
- **Weaknesses:**  
  - No uncertainty (Ïƒ).  
  - No physics-informed constraints (smoothness, asymmetry).  

---

### 2. V1ctorious3010 â€” â€œ80bl-128hd-impactâ€
- **Architecture:**  
  - ~80 residual blocks Ã— 128 hidden units.  
  - Very deep MLP, relies on BatchNorm + dropout for stability.  
- **Training & Preprocessing:**  
  - Strong normalization & detrending.  
  - Requires multi-GPU for efficient training.  
- **Performance:**  
  - Public LB ~**0.322** (slightly above baseline).  
  - Risk of overfitting due to extreme depth.  
- **Strengths:**  
  - Captures fine spectral detail.  
  - Reproducible (Kaggle Model release with weights).  
- **Weaknesses:**  
  - Heavy compute requirements.  
  - No uncertainty modeling.  
  - Risk of fitting noise.  

---

### 3. Fawad Awan â€” â€œSpectrum Regressorâ€
- **Architecture:**  
  - Multi-output regressor, outputs all spectral bins jointly.  
  - Likely hybrid dense layers with optional conv/recurrent elements.  
- **Training & Preprocessing:**  
  - Simpler preprocessing, normalized spectra.  
  - Focused on robustness and reproducibility.  
- **Performance:**  
  - Public LB ~**0.317â€“0.320**.  
  - Competitive but not top performing.  
- **Strengths:**  
  - Outputs full spectrum in one shot.  
  - Clean, lighter to train.  
- **Weaknesses:**  
  - Less expressive than deep residual nets.  
  - Still lacks uncertainty modeling.  

---

## ğŸ“‘ Comparative Summary

| Model                          | Depth/Size     | Score (Public LB) | Strengths                                   | Weaknesses                          |
|--------------------------------|----------------|-------------------|---------------------------------------------|-------------------------------------|
| **Thang Do Duc â€“ Baseline**    | Medium (MLP)   | 0.329             | Simple, reproducible, stable reference       | No uncertainty, limited physics      |
| **V1ctorious â€“ 80bl-128hd**    | Very deep MLP  | 0.322             | Captures fine spectral features              | Risk of overfitting, heavy compute   |
| **Fawad â€“ Spectrum Regressor** | Moderate MLP   | 0.317â€“0.320       | Multi-output design, lighter training        | Slightly weaker performance          |

---

## ğŸš€ Best Practices for SpectraMind V50
- Use **residual fully-connected blocks** (baseline + deep model).  
- Add **domain-specific preprocessing**: detrending, jitter correction, smoothing.  
- Incorporate **uncertainty modeling (Î¼/Ïƒ)** to improve calibration.  
- Enforce **physics-informed constraints**:  
  - Non-negativity of spectra.  
  - Smoothness and asymmetry penalties.  
  - FFT-based symbolic regularizers.  
- Use **ensembles of medium-depth models** for generalization over brute-force depth.  
- Maintain **reproducibility** with Hydra configs, DVC data versioning, and deterministic seeding.  

---

## ğŸ“š Sources
- Kaggle notebooks & models analyzed in:  
  - Thang Do Duc â€” [â€œ0.329 LBâ€ baseline notebook]  
  - V1ctorious3010 â€” [â€œ80bl-128hd-impactâ€ model card]  
  - Fawad Awan â€” [â€œSpectrum Regressorâ€ Kaggle model]  

*(See internal file: `Comparison of Kaggle Models from NeurIPS 2025 Ariel Data Challenge.pdf` for detailed notes.)*

---
