# .github/workflows/benchmark.yml
# ==============================================================================
# SpectraMind V50 — Benchmarks (CPU/GPU): timing, throughput, GLL, artifacts
# ------------------------------------------------------------------------------
# Runs on demand, weekly, and on push to main.
# • Builds env via Poetry, exercises the `spectramind` CLI, exports reports.
# • Optional GPU matrix + optional Docker GPU parity smoke (BuildKit cached).
# • Safe-by-default: GPU parts are opt-in via repo/org variables.
# ==============================================================================

name: benchmark

on:
  workflow_dispatch:
    inputs:
      extra_overrides:
        description: "Hydra overrides (e.g., +training.seed=1337 +training.epochs=1)"
        required: false
        type: string
      device:
        description: "Device override for on-demand run"
        required: false
        default: "auto"
        type: choice
        options: ["auto", "cpu", "gpu"]
  schedule:
    - cron: "0 6 * * 1" # Weekly (Mondays 06:00 UTC)
  push:
    branches: ["main"]

permissions:
  contents: read

concurrency:
  group: benchmark-${{ github.ref }}
  cancel-in-progress: false

jobs:
  bench:
    name: Bench • py${{ matrix.python }} • ${{ matrix.device }}
    runs-on: ubuntu-latest
    timeout-minutes: 90

    # GPU matrix entries are enabled only when VARS.ENABLE_GPU_MATRIX == '1'
    strategy:
      fail-fast: false
      matrix:
        python: ["3.11", "3.12"]
        device: ["cpu", "gpu"]
        include:
          # Ensure at least one CPU variant always runs
          - python: "3.11"
            device: "cpu"

    env:
      POETRY_VIRTUALENVS_IN_PROJECT: "true"
      POETRY_NO_INTERACTION: "1"
      TRANSFORMERS_OFFLINE: "1"
      HF_HUB_OFFLINE: "1"
      HYDRA_FULL_ERROR: "1"
      # Where benchmarks place per-axis outputs
      BENCH_RESULTS_DIR: benchmarks/${{ matrix.python }}_${{ matrix.device }}
      # workflow_dispatch override takes precedence; otherwise matrix device
      REQ_DEVICE: ${{ inputs.device && inputs.device || '' }}
      # Respect GPU selection if actually running GPU axis
      CUDA_VISIBLE_DEVICES: ${{ matrix.device == 'gpu' && '0' || '' }}

    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with: { fetch-depth: 0 }

      - name: Set up Python ${{ matrix.python }}
        id: setup-py
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python }}
          cache: "pip"

      - name: Install Poetry
        run: |
          python -m pip install --upgrade pip
          pip install "poetry==1.8.3"

      - name: Cache Poetry venv & cache
        uses: actions/cache@v4
        with:
          path: |
            .venv
            ~/.cache/pypoetry
          key: poetry-${{ runner.os }}-${{ matrix.python }}-${{ hashFiles('**/poetry.lock') }}
          restore-keys: |
            poetry-${{ runner.os }}-${{ matrix.python }}-

      - name: Install deps (Poetry)
        run: |
          poetry env use "${{ steps.setup-py.outputs.python-path }}"
          poetry install --no-interaction

      - name: Environment info
        run: |
          echo "Device request (dispatch) : '${{ inputs.device || 'auto' }}'"
          echo "Matrix device             : '${{ matrix.device }}'"
          echo "Runner                    : $(uname -a)"
          echo "Python                    : $(python --version)"
          echo "Poetry                    : $(poetry --version)"
          command -v nvidia-smi >/dev/null 2>&1 && nvidia-smi || echo "nvidia-smi not present"

      - name: Fast CLI selftest
        run: |
          poetry run spectramind selftest --fast

      - name: Gate GPU axis if not enabled
        if: ${{ matrix.device == 'gpu' && vars.ENABLE_GPU_MATRIX != '1' }}
        run: |
          echo "GPU axis skipped (set repo/org variable ENABLE_GPU_MATRIX=1 to enable)."
          mkdir -p "$BENCH_RESULTS_DIR"
          echo "skipped" > "$BENCH_RESULTS_DIR/_gpu_matrix_skipped.txt"

      - name: Run benchmark (train + diagnostics)
        if: ${{ !(matrix.device == 'gpu' && vars.ENABLE_GPU_MATRIX != '1') }}
        run: |
          set -e
          mkdir -p "$BENCH_RESULTS_DIR"
          DEV="${REQ_DEVICE:-${{ matrix.device }}}"

          echo ">>> Using device=$DEV"
          # Minimal training epoch to keep runtime bounded; user can override via dispatch.
          poetry run spectramind train \
            +training.epochs=1 +benchmark=true \
            ${{ inputs.extra_overrides && format('{0}', inputs.extra_overrides) || '' }} \
            --device "$DEV" --outdir "$BENCH_RESULTS_DIR"

          # Scientific diagnostics (keep fast by default, allow fallback)
          poetry run spectramind diagnose smoothness --outdir "$BENCH_RESULTS_DIR"
          poetry run spectramind diagnose dashboard \
            --no-umap --no-tsne --outdir "$BENCH_RESULTS_DIR" || \
          poetry run spectramind diagnose dashboard \
            --outdir "$BENCH_RESULTS_DIR" || true

      - name: Summarize timing & metrics
        if: always()
        run: |
          set -e
          {
            echo "# SpectraMind V50 Benchmark"
            echo
            echo "- Axis         : py=${{ matrix.python }}, dev=${{ matrix.device }}"
            echo "- Date (UTC)   : $(date -u +'%Y-%m-%d %H:%M:%S')"
            echo "- Runner       : $(uname -a)"
            echo
            command -v nvidia-smi >/dev/null 2>&1 && { echo "## nvidia-smi"; nvidia-smi; echo; } || true
            echo "## Directory"
            echo "\`$BENCH_RESULTS_DIR\`"
            echo
            if [ -f "$BENCH_RESULTS_DIR/metrics.json" ]; then
              echo "## metrics.json (truncated)"
              jq 'del(.large_arrays) | . ' "$BENCH_RESULTS_DIR/metrics.json" | head -n 200 || true
            fi
            echo
            echo "## Files"
            ls -lh "$BENCH_RESULTS_DIR" || true
          } > "$BENCH_RESULTS_DIR/summary.md"
          cat "$BENCH_RESULTS_DIR/summary.md" >> "$GITHUB_STEP_SUMMARY"

      - name: Upload artifacts
        uses: actions/upload-artifact@v4
        with:
          name: bench-${{ matrix.python }}-${{ matrix.device }}
          path: ${{ env.BENCH_RESULTS_DIR }}
          if-no-files-found: warn
          retention-days: 21

  # Optional container parity: build Docker image and run a GPU smoke selftest inside it.
  docker-gpu:
    name: Dockerized GPU Smoke • CUDA (BuildKit cache)
    if: ${{ vars.DOCKER_GPU == '1' }} # opt-in via repo/org variable
    runs-on: ubuntu-latest
    timeout-minutes: 35
    env:
      BUILDKIT_CACHE_TO: /tmp/.buildx-cache-new
      BUILDKIT_CACHE_FROM: /tmp/.buildx-cache
      IMAGE_TAG: spectramindv50:ci
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with: { fetch-depth: 0 }

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Restore BuildKit cache
        uses: actions/cache@v4
        with:
          path: ${{ env.BUILDKIT_CACHE_FROM }}
          key: buildx-${{ runner.os }}-${{ hashFiles('Dockerfile', '**/pyproject.toml', '**/poetry.lock') }}
          restore-keys: |
            buildx-${{ runner.os }}-

      - name: Install Docker & NVIDIA runtime
        run: |
          sudo apt-get update -y
          sudo apt-get install -y docker.io
          sudo systemctl enable --now docker
          distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
          curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg
          curl -fsSL https://nvidia.github.io/libnvidia-container/$distribution/libnvidia-container.list | \
            sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \
            sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list
          sudo apt-get update -y
          sudo apt-get install -y nvidia-container-toolkit
          sudo nvidia-ctk runtime configure --runtime=docker
          sudo systemctl restart docker

      - name: Build CUDA image (cached)
        uses: docker/build-push-action@v5
        with:
          context: .
          file: ./Dockerfile
          tags: ${{ env.IMAGE_TAG }}
          load: true
          cache-from: type=local,src=${{ env.BUILDKIT_CACHE_FROM }}
          cache-to: type=local,dest=${{ env.BUILDKIT_CACHE_TO }},mode=max

      - name: Save BuildKit cache
        if: always()
        uses: actions/cache@v4
        with:
          path: ${{ env.BUILDKIT_CACHE_TO }}
          key: buildx-${{ runner.os }}-${{ hashFiles('Dockerfile', '**/pyproject.toml', '**/poetry.lock') }}

      - name: Run GPU smoke inside container
        run: |
          docker run --rm --gpus all \
            -v "$PWD":/workspace -w /workspace \
            ${{ env.IMAGE_TAG }} \
            bash -lc 'which spectramind || true; spectramind selftest --fast || exit 1'

  aggregate:
    name: Aggregate Benchmark Reports
    needs: [bench]
    runs-on: ubuntu-latest
    if: always()
    steps:
      - name: Download artifacts
        uses: actions/download-artifact@v4
        with:
          pattern: bench-*
          merge-multiple: true

      - name: Build aggregated report
        run: |
          set -e
          mkdir -p aggregated
          echo "# SpectraMind V50 Benchmark Report" > aggregated/report.md
          echo "" >> aggregated/report.md
          if ls benchmarks/*/summary.md >/dev/null 2>&1; then
            for f in benchmarks/*/summary.md; do
              axis="$(basename "$(dirname "$f")")"
              echo "## $axis" >> aggregated/report.md
              cat "$f" >> aggregated/report.md
              echo "" >> aggregated/report.md
            done
          else
            echo "_No summaries found._" >> aggregated/report.md
          fi

      - name: Upload aggregated report
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-aggregated
          path: aggregated/report.md
          retention-days: 60
```
