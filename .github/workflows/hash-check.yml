upgrade, # ==============================================================================

# SpectraMind V50 — Hash/Integrity Gate (configs + code + locks + DVC + LFS)

# - Deterministic SHA-256 over critical surfaces (configs/, src/, pyproject/locks, dvc)

# - Verifies DVC cache status, Git LFS tracking, and reports drift vs. a baseline

# - Baseline file lives at .github/hash-baseline.json (versioned in git)

# - Provides "generate" mode (manual) to refresh the baseline and upload artifact

# - Fork-safe: comments on PRs and never attempts privileged writes

# - Fast paths: only triggers when integrity surface changes (plus manual/scheduled)

# ==============================================================================

name: hash-check

on:
push:
branches: \["main"]
paths:
\- "pyproject.toml"
\- "poetry.lock"
\- "dvc.yaml"
\- ".dvc/**"
\- "configs/**/*.yaml"
\- "src/\*\*/*.py"
\- ".github/workflows/**/\*.yml"
\- ".gitattributes"
\- ".github/hash-baseline.json"
pull\_request:
branches: \["**"]
paths:
\- "pyproject.toml"
\- "poetry.lock"
\- "dvc.yaml"
\- ".dvc/**"
\- "configs/**/*.yaml"
\- "src/\*\*/*.py"
\- ".github/workflows/\*\*/\*.yml"
\- ".gitattributes"
\- ".github/hash-baseline.json"
workflow\_dispatch:
inputs:
mode:
description: "Run mode: verify (default) or generate (create new baseline artifact)"
required: false
default: "verify"
type: choice
options: \["verify", "generate"]
extra\_include\_globs:
description: "Optional newline- or comma-separated extra glob patterns to include in hashing"
required: false
default: ""
exclude\_globs:
description: "Optional newline- or comma-separated glob patterns to exclude from hashing"
required: false
default: ""
schedule:
\# Weekly integrity sweep: Sundays 12:30 UTC
\- cron: "30 12 \* \* 0"

concurrency:
group: \${{ github.workflow }}-\${{ github.ref }}
cancel-in-progress: true

# Minimal permissions; PR comments require pull-requests\:write on PR events

permissions:
contents: read
pull-requests: write

env:
PYTHONUNBUFFERED: "1"
PIP\_DISABLE\_PIP\_VERSION\_CHECK: "1"
POETRY\_VERSION: "1.8.3"
DEFAULT\_PY: "3.12"

# Integrity surface (edit as needed). Additional globs can be passed at dispatch time.

HASH\_GLOBS: |
pyproject.toml
poetry.lock
dvc.yaml
.dvc/\*\*
configs/**/\*.yaml
src/**/*.py
.github/workflows/\*\*/*.yml
.gitattributes

# Default exclusions (avoid huge vendor caches or CI artifacts)

EXCLUDE\_GLOBS: |
**/**pycache**/**
**/.mypy\_cache/**
**/.pytest\_cache/**
**/.ruff\_cache/**
**/.venv/**
**/.tox/**
**/.cache/**
**/\*.ipynb\_checkpoints/**
**/.dvc/tmp/**
**/.git/**

jobs:
hash:
name: Hash / Integrity Gate
runs-on: ubuntu-latest
timeout-minutes: 20

```
steps:
  - name: Checkout (no credentials)
    uses: actions/checkout@v4
    with:
      fetch-depth: 0

  - name: Setup Python ${{ env.DEFAULT_PY }}
    uses: actions/setup-python@v5
    with:
      python-version: ${{ env.DEFAULT_PY }}
      cache: "pip"

  - name: Install Poetry
    run: |
      python -m pip install --upgrade pip
      pip install "poetry==${{ env.POETRY_VERSION }}"

  - name: Cache Poetry & venv
    uses: actions/cache@v4
    with:
      path: |
        ~/.cache/pip
        ~/.cache/pypoetry
        .venv
      key: poetry-${{ runner.os }}-${{ env.DEFAULT_PY }}-${{ hashFiles('**/poetry.lock') }}
      restore-keys: |
        poetry-${{ runner.os }}-${{ env.DEFAULT_PY }}-

  - name: Install deps (thin)
    run: |
      poetry install --no-root --only main || poetry install --no-root

  - name: Ensure jq present
    run: |
      set -euo pipefail
      if ! command -v jq >/dev/null 2>&1; then
        sudo apt-get update -y
        sudo apt-get install -y jq
      fi
      jq --version

  # ---- LFS & DVC health checks -------------------------------------------------

  - name: Verify Git LFS tracked binaries
    shell: bash
    run: |
      set -euo pipefail
      if ! command -v git >/dev/null; then
        echo "::error::git not available"; exit 1
      fi
      if ! git lfs version >/dev/null 2>&1; then
        echo "::warning::Git LFS not installed on runner; checking attributes only."
      fi

      # Prefer patterns from .gitattributes; fallback to common binary patterns
      attr_file=".gitattributes"
      declare -a patterns
      if [[ -f "$attr_file" ]]; then
        # Extract patterns marked with filter=lfs
        mapfile -t patterns < <(git check-attr filter -- :/ | awk '$3=="lfs"{print $1}' | sort -u)
      fi
      if [[ ${#patterns[@]} -eq 0 ]]; then
        patterns=( '*.png' '*.jpg' '*.jpeg' '*.gif' '*.svg' '*.pdf' '*.pt' '*.pth' '*.bin' '*.ckpt' '*.onnx' '*.h5' '*.npz' )
      fi

      missing=0
      for p in "${patterns[@]}"; do
        if git check-attr -a -- "$p" | grep -q 'filter: lfs'; then
          echo "OK: $p is tracked by LFS"
        else
          echo "::warning::Pattern not tracked by LFS: $p"
          missing=$((missing+1))
        fi
      done

      # Optional hard fail:
      # if [ "$missing" -gt 0 ]; then echo "::error::Some binary patterns are not LFS-tracked"; exit 1; fi

  - name: Verify DVC status (if present)
    shell: bash
    run: |
      set -euo pipefail
      if [ -f "dvc.yaml" ] || [ -d ".dvc" ]; then
        python -m pip install --upgrade "dvc[all]" || python -m pip install --upgrade dvc
        echo "Checking DVC status (no remote data pull)..."
        dvc status -c || { echo "::error::DVC status indicates drift or missing cache"; exit 1; }
      else
        echo "No DVC configuration detected. Skipping DVC checks."
      fi

  # ---- Compute hashes & compare to baseline ------------------------------------

  - name: Compute repository hashes (SHA-256)
    id: hash
    shell: bash
    env:
      DISPATCH_EXTRA: ${{ github.event.inputs.extra_include_globs || '' }}
      DISPATCH_EXCLUDES: ${{ github.event.inputs.exclude_globs || '' }}
    run: |
      set -euo pipefail
      python - << 'PY'
```

import hashlib, json, os, sys, glob, pathlib

def parse\_globs(raw):
if not raw: return \[]
\# Support comma- or newline-separated lists
parts = \[]
for line in raw\.replace(',', '\n').splitlines():
s = line.strip()
if s: parts.append(s)
return parts

hash\_globs = os.environ.get("HASH\_GLOBS","")
exclude\_globs = os.environ.get("EXCLUDE\_GLOBS","")
extra = os.environ.get("DISPATCH\_EXTRA","")
excl2 = os.environ.get("DISPATCH\_EXCLUDES","")

include\_patterns = \[p for p in hash\_globs.splitlines() if p.strip()]
include\_patterns += parse\_globs(extra)
exclude\_patterns = \[p for p in exclude\_globs.splitlines() if p.strip()]
exclude\_patterns += parse\_globs(excl2)

def matches\_any(path, patterns):
for pat in patterns:
if glob.fnmatch.fnmatch(path, pat):
return True
return False

# Collect files

files = set()
for pat in include\_patterns:
\# Use recursive glob; include dotfiles via glob.glob
for p in glob.glob(pat, recursive=True):
if os.path.isfile(p):
files.add(p)

# Apply exclusions

filtered = \[]
for p in sorted(files):
\# Normalize to posix for pattern matching
posix = pathlib.Path(p).as\_posix()
if matches\_any(posix, exclude\_patterns):
continue
filtered.append(p)

if not filtered:
print("::warning::No files matched hashing surface after exclusions.")
payload = {"hashes": {}, "files": \[], "sha256\_all": "", "include": include\_patterns, "exclude": exclude\_patterns}
open("hash-report.json","w").write(json.dumps(payload, indent=2))
sys.exit(0)

digests = {}
h\_all = hashlib.sha256()
for path in filtered:
with open(path, "rb") as f:
data = f.read()
h = hashlib.sha256(data).hexdigest()
digests\[path] = h
h\_all.update(path.encode("utf-8") + b"\x00" + h.encode("utf-8"))

report = {
"hashes": digests,
"files": filtered,
"sha256\_all": h\_all.hexdigest(),
"include": include\_patterns,
"exclude": exclude\_patterns,
}
open("hash-report.json","w").write(json.dumps(report, indent=2))

# Also save a simple sha256sum-like text for quick diffs

with open("hashes.sha256","w") as w:
for k in sorted(digests):
w\.write(f"{digests\[k]}  {k}\n")
print(json.dumps({"file\_count": len(filtered), "sha256\_all": report\["sha256\_all"]}, indent=2))
PY

```
  - name: Load baseline (if present)
    id: baseline
    shell: bash
    run: |
      set -euo pipefail
      BASELINE=".github/hash-baseline.json"
      if [ -f "$BASELINE" ]; then
        echo "found=true" >> $GITHUB_OUTPUT
      else
        echo "found=false" >> $GITHUB_OUTPUT
      fi

  - name: Validate JSONs
    if: always()
    shell: bash
    run: |
      set -euo pipefail
      jq -r '.' hash-report.json >/dev/null || { echo "::error::Hash report JSON invalid"; exit 1; }
      if [[ -f .github/hash-baseline.json ]]; then
        jq -r '.' .github/hash-baseline.json >/dev/null || { echo "::error::Baseline JSON invalid"; exit 1; }
      fi

  - name: Compare against baseline
    if: ${{ steps.baseline.outputs.found == 'true' && (github.event.inputs.mode == '' || github.event.inputs.mode == 'verify') }}
    id: compare
    shell: bash
    run: |
      set -euo pipefail
      BASE_ALL=$(jq -r '.sha256_all' .github/hash-baseline.json)
      CURR_ALL=$(jq -r '.sha256_all' hash-report.json)

      if [ "$BASE_ALL" = "$CURR_ALL" ]; then
        echo "Integrity OK: combined SHA-256 matches baseline ($CURR_ALL)"
        echo "drift=false" >> $GITHUB_OUTPUT
        exit 0
      fi

      echo "::warning::Integrity drift detected; computing file-level diff..."
      python - << 'PY'
```

import json, pathlib
base = json.load(open(".github/hash-baseline.json"))
curr = json.load(open("hash-report.json"))

b = base.get("hashes",{})
c = curr.get("hashes",{})

all\_keys = sorted(set(b.keys()) | set(c.keys()))
changed, added, removed = \[], \[], \[]
for k in all\_keys:
bv = b.get(k); cv = c.get(k)
if bv and cv and bv != cv:
changed.append(k)
elif bv and not cv:
removed.append(k)
elif cv and not bv:
added.append(k)

def section(title, items):
if not items: return ""
s = f"\n### {title} ({len(items)})\n"
for i in items:
s += f"- `{i}`\n"
return s

summary = "# Hash Drift Report\n"
summary += f"\n**Combined SHA baseline**: `{base.get('sha256_all','')}`"
summary += f"\n**Combined SHA current** : `{curr.get('sha256_all','')}`\n"
summary += section("Changed", changed)
summary += section("Added", added)
summary += section("Removed", removed)
open("hash-diff.md","w").write(summary)
print(summary)
PY
echo "drift=true" >> \$GITHUB\_OUTPUT
\# Fail the job to block merges until acknowledged/updated
exit 1

```
  - name: Generate new baseline (artifact only)
    if: ${{ github.event.inputs.mode == 'generate' }}
    shell: bash
    run: |
      set -euo pipefail
      cp hash-report.json hash-baseline.generated.json
      echo "Generated new baseline (not committed). Uploading as artifact..."

  - name: Upload hash artifacts
    if: always()
    uses: actions/upload-artifact@v4
    with:
      name: hash-report
      path: |
        hash-report.json
        hashes.sha256
        hash-diff.md
        hash-baseline.generated.json
      if-no-files-found: ignore
      retention-days: 14

  - name: Comment on PR with drift summary
    if: ${{ always() && github.event_name == 'pull_request' && steps.compare.outputs.drift == 'true' }}
    uses: actions/github-script@v7
    with:
      script: |
        const fs = require('fs');
        let body = '';
        try {
          body = fs.readFileSync('hash-diff.md', 'utf8');
        } catch (e) {
          body = 'Hash drift detected, but summary could not be read.';
        }
        await github.rest.issues.createComment({
          owner: context.repo.owner,
          repo: context.repo.repo,
          issue_number: context.issue.number,
          body
        });

  - name: Summary
    if: always()
    shell: bash
    run: |
      {
        echo "### Hash/Integrity Gate Summary"
        if [[ -f hash-report.json ]]; then
          SHA_ALL=$(jq -r '.sha256_all' hash-report.json 2>/dev/null || echo "")
          FILES=$(jq -r '.files | length' hash-report.json 2>/dev/null || echo "0")
          echo "- Files hashed: ${FILES}"
          echo "- Combined SHA-256: \`${SHA_ALL}\`"
        else
          echo "- No hash-report.json produced."
        fi
        if [[ -f .github/hash-baseline.json ]]; then
          echo "- Baseline present: yes"
        else
          echo "- Baseline present: no"
        fi
        if [[ -f hash-diff.md ]]; then
          COUNT_CHANGED=$(grep -cE '^- `.*`$' hash-diff.md || true)
          echo "- Drift items (changed/added/removed total lines): ${COUNT_CHANGED}"
        fi
      } >> "$GITHUB_STEP_SUMMARY"

  - name: Guidance / Next steps
    if: ${{ failure() && (steps.compare.outputs.drift == 'true' || github.event.inputs.mode == 'generate') }}
    shell: bash
    run: |
      cat <<'NOTE'
```

> Next steps:

* If this drift is expected (e.g., intentional code/config changes), update the baseline:

  1. Run this workflow with "workflow\_dispatch" and mode=generate
  2. Download artifact "hash-baseline.generated.json"
  3. Replace .github/hash-baseline.json with the generated file and commit it in the same PR

* If drift is unexpected:
  • Inspect 'hash-diff.md' to see which files changed
  • Verify DVC and LFS status (the earlier steps printed diagnostics)
  • Re-run local 'poetry lock' only when intentionally changing dependencies
  NOTE
