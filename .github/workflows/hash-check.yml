# ==============================================================================
# SpectraMind V50 — Hash/Integrity Gate (configs + code + locks + DVC + LFS)
# - Deterministic SHA-256 over critical surfaces (configs/, src/, pyproject/locks, dvc)
# - Verifies DVC cache status, Git LFS tracking, and reports drift vs. a baseline
# - Baseline file lives at .github/hash-baseline.json (versioned in git)
# - Provides "generate" mode (manual) to refresh the baseline and upload artifact
# ==============================================================================
name: hash-check

on:
  push:
    branches: ["main"]
  pull_request:
    branches: ["**"]
  workflow_dispatch:
    inputs:
      mode:
        description: "Run mode: verify (default) or generate (create new baseline artifact)"
        required: false
        default: "verify"
        type: choice
        options: ["verify", "generate"]
  schedule:
    # Weekly integrity sweep: Sundays 12:30 UTC
    - cron: "30 12 * * 0"

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

permissions:
  contents: read
  # Needed to post PR comments (only on pull_request events)
  pull-requests: write

env:
  PYTHONUNBUFFERED: "1"
  PIP_DISABLE_PIP_VERSION_CHECK: "1"
  POETRY_VERSION: "1.8.3"
  DEFAULT_PY: "3.12"
  # Paths/Globs that define the integrity surface (edit as needed)
  HASH_GLOBS: |
    pyproject.toml
    poetry.lock
    dvc.yaml
    .dvc/**.json
    configs/**/*.yaml
    src/**/*.py
    .github/workflows/**/*.yml

jobs:
  hash:
    name: Hash / Integrity Gate
    runs-on: ubuntu-latest
    timeout-minutes: 20

    steps:
      - name: Checkout (no credentials)
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Python ${{ env.DEFAULT_PY }}
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.DEFAULT_PY }}
          cache: "pip"

      - name: Install Poetry
        run: |
          python -m pip install --upgrade pip
          pip install "poetry==${{ env.POETRY_VERSION }}"

      - name: Cache Poetry & venv
        uses: actions/cache@v4
        with:
          path: |
            ~/.cache/pip
            ~/.cache/pypoetry
            .venv
          key: poetry-${{ runner.os }}-${{ env.DEFAULT_PY }}-${{ hashFiles('**/poetry.lock') }}
          restore-keys: |
            poetry-${{ runner.os }}-${{ env.DEFAULT_PY }}-

      - name: Install deps (thin)
        run: |
          poetry install --no-root --only main || poetry install --no-root

      # ---- LFS & DVC health checks -------------------------------------------------

      - name: Verify Git LFS tracked binaries
        shell: bash
        run: |
          set -euo pipefail
          if ! command -v git >/dev/null; then
            echo "::error::git not available"; exit 1
          fi
          if ! git lfs version >/dev/null 2>&1; then
            echo "::warning::Git LFS is not installed in runner image; checking attributes only."
          fi

          # List patterns we expect in LFS (edit as needed)
          patterns='*.png *.jpg *.jpeg *.gif *.svg *.pdf *.pt *.pth *.bin *.ckpt *.onnx *.h5 *.npz'
          missing=0
          for p in $patterns; do
            if git check-attr -a -- $p | grep -q 'filter=lfs'; then
              echo "OK: $p is tracked by LFS"
            else
              echo "::warning::Pattern not tracked by LFS: $p"
              missing=$((missing+1))
            fi
          done

          # Non-blocking but flagged; fail only if you want to enforce strictly:
          # if [ "$missing" -gt 0 ]; then echo "::error::Some binary patterns are not LFS-tracked"; exit 1; fi

      - name: Verify DVC status (if present)
        shell: bash
        run: |
          set -euo pipefail
          if [ -f "dvc.yaml" ] || [ -d ".dvc" ]; then
            python -m pip install --upgrade "dvc[all]" || python -m pip install --upgrade dvc
            echo "Checking DVC status (no remote data pull)..."
            # -c: compare workspace to cache; fail if checksums drift or missing outs
            dvc status -c || (echo "::error::DVC status indicates drift or missing cache"; exit 1)
          else
            echo "No DVC configuration detected. Skipping DVC checks."
          fi

      # ---- Compute hashes & compare to baseline ------------------------------------

      - name: Compute repository hashes (SHA-256)
        id: hash
        shell: bash
        run: |
          set -euo pipefail
          python - << 'PY'
import hashlib, json, os, sys, glob

globs = os.environ.get("HASH_GLOBS","").strip().splitlines()
files = []
for g in globs:
    files.extend(sorted([p for p in glob.glob(g, recursive=True) if os.path.isfile(p)]))

if not files:
    print("::warning::No files matched HASH_GLOBS; integrity surface is empty.")
    payload = {"hashes": {}, "files": [], "sha256_all": ""}
    print(json.dumps(payload, indent=2))
    open("hash-report.json","w").write(json.dumps(payload, indent=2))
    sys.exit(0)

digests = {}
h_all = hashlib.sha256()
for path in files:
    with open(path, "rb") as f:
        data = f.read()
    h = hashlib.sha256(data).hexdigest()
    digests[path] = h
    # Combine into an overall digest (path + NUL + hash)
    h_all.update(path.encode("utf-8") + b"\x00" + h.encode("utf-8"))

report = {
    "hashes": digests,
    "files": files,
    "sha256_all": h_all.hexdigest(),
}
open("hash-report.json","w").write(json.dumps(report, indent=2))
print(json.dumps(report, indent=2))
PY

      - name: Load baseline (if present)
        id: baseline
        shell: bash
        run: |
          set -euo pipefail
          BASELINE=".github/hash-baseline.json"
          if [ -f "$BASELINE" ]; then
            echo "found=true" >> $GITHUB_OUTPUT
          else
            echo "found=false" >> $GITHUB_OUTPUT
          fi

      - name: Compare against baseline
        if: ${{ steps.baseline.outputs.found == 'true' && (github.event.inputs.mode == '' || github.event.inputs.mode == 'verify') }}
        id: compare
        shell: bash
        run: |
          set -euo pipefail
          jq -r '.' .github/hash-baseline.json >/dev/null || { echo "::error::Baseline JSON invalid"; exit 1; }
          jq -r '.' hash-report.json >/dev/null || { echo "::error::Hash report JSON invalid"; exit 1; }

          BASE_ALL=$(jq -r '.sha256_all' .github/hash-baseline.json)
          CURR_ALL=$(jq -r '.sha256_all' hash-report.json)

          if [ "$BASE_ALL" = "$CURR_ALL" ]; then
            echo "Integrity OK: combined SHA-256 matches baseline ($CURR_ALL)"
            echo "drift=false" >> $GITHUB_OUTPUT
            exit 0
          fi

          echo "::warning::Integrity drift detected; computing file-level diff..."
          # Produce a human-friendly diff table
          python - << 'PY'
import json, sys, pathlib
base = json.load(open(".github/hash-baseline.json"))
curr = json.load(open("hash-report.json"))

all_keys = sorted(set(base["hashes"].keys()) | set(curr["hashes"].keys()))
rows = []
changed, added, removed = [], [], []
for k in all_keys:
    b = base["hashes"].get(k)
    c = curr["hashes"].get(k)
    if b and c and b != c:
        changed.append(k)
    elif b and not c:
        removed.append(k)
    elif c and not b:
        added.append(k)

def section(title, items):
    if not items: return ""
    s = f"\n### {title} ({len(items)})\n"
    for i in items:
        s += f"- `{i}`\n"
    return s

summary = "# Hash Drift Report\n"
summary += f"\n**Combined SHA baseline**: `{base.get('sha256_all')}`"
summary += f"\n**Combined SHA current** : `{curr.get('sha256_all')}`\n"
summary += section("Changed", changed)
summary += section("Added", added)
summary += section("Removed", removed)
open("hash-diff.md","w").write(summary)
print(summary)
PY
          echo "drift=true" >> $GITHUB_OUTPUT
          # Fail the job to block merges until acknowledged/updated
          exit 1

      - name: Generate new baseline (artifact only)
        if: ${{ github.event.inputs.mode == 'generate' }}
        shell: bash
        run: |
          set -euo pipefail
          cp hash-report.json hash-baseline.generated.json
          echo "Generated new baseline (not committed). Uploading as artifact..."

      - name: Upload hash artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: hash-report
          path: |
            hash-report.json
            hash-diff.md
            hash-baseline.generated.json
          if-no-files-found: ignore
          retention-days: 14

      - name: Comment on PR with drift summary
        if: ${{ always() && github.event_name == 'pull_request' && steps.compare.outputs.drift == 'true' }}
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            let body = '';
            try {
              body = fs.readFileSync('hash-diff.md', 'utf8');
            } catch (e) {
              body = 'Hash drift detected, but summary could not be read.';
            }
            await github.rest.issues.createComment({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
              body
            });

      - name: Guidance / Next steps
        if: ${{ failure() && (steps.compare.outputs.drift == 'true' || github.event.inputs.mode == 'generate') }}
        shell: bash
        run: |
          cat <<'NOTE'
> Next steps:

- If this drift is expected (e.g., intentional code/config changes), update the baseline:
  1) Run this workflow with "workflow_dispatch" and mode=generate
  2) Download artifact "hash-baseline.generated.json"
  3) Replace .github/hash-baseline.json with the generated file and commit it in the same PR

- If drift is unexpected:
  • Inspect 'hash-diff.md' to see which files changed
  • Verify DVC and LFS status (the earlier steps printed diagnostics)
  • Re-run local 'poetry lock' only when intentionally changing dependencies
NOTE
