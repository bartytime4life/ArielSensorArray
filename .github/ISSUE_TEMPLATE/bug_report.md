
‚∏ª

üõ∞Ô∏è SpectraMind V50 ‚Äî Bug Report

Fill out every section. If something doesn‚Äôt apply, write N/A and explain why. This template enforces our NASA‚Äëgrade reproducibility, physics‚Äëinformed modeling, and CLI‚Äëfirst standards.

‚∏ª

0) Pre‚Äësubmission Checklist (must pass before filing)
	‚Ä¢	I searched existing issues and discussions for this bug.
	‚Ä¢	I reproduced on latest main (paste commit hash below).
	‚Ä¢	I ran spectramind selftest --fast and spectramind selftest --deep and captured logs.
	‚Ä¢	I ran pre-commit run -a (ruff/black/isort/yaml checks) and fixed style issues.
	‚Ä¢	I can reproduce with a clean environment (e.g., fresh Poetry env or container).
	‚Ä¢	I attached key artifacts (logs, HTML diagnostics, config snapshot) listed in ¬ß5.

Current commit (git rev-parse HEAD):
<hash>

‚∏ª

1) Summary

Title:
[BUG] <concise problem statement>

Observed Behavior (what happened):
<clear, factual description>

Expected Behavior (what you thought should happen):


Severity: ‚òê Blocker (prevents submission) ‚òê High ‚òê Medium ‚òê Low
Scope: ‚òê CLI ‚òê Hydra Configs ‚òê Data/Calibration ‚òê Model Training ‚òê Inference ‚òê Diagnostics/Dashboard ‚òê DVC ‚òê CI/GitHub Actions ‚òê Docker/Container ‚òê Packaging/Submission ‚òê Other: <specify>

Reproducibility: ‚òê Always ‚òê Often (‚â•70%) ‚òê Intermittent (30‚Äì70%) ‚òê Rare (<30%)
First seen on: <date> (America/Chicago)

‚∏ª

2) Environment

Paste exact outputs where indicated. Do not paraphrase.
	‚Ä¢	Platform: ‚òê Linux (distro + version) ‚òê macOS ‚òê Windows ‚òê Kaggle ‚òê Other
	‚Ä¢	Kernel (Linux): uname -a
	‚Ä¢	Container: ‚òê Docker ‚òê Bare‚Äëmetal
Image/tag: <e.g., spectramindv50:dev>  Dockerfile hash (if custom): <hash>
	‚Ä¢	GPU(s): <e.g., 1√óRTX 5080 16GB>  Driver: nvidia-smi top lines
	‚Ä¢	CUDA/cuDNN: python -c "import torch;print('torch',torch.__version__);import torch,platform;print('cuda?',torch.cuda.is_available(),'cuda_ver',torch.version.cuda)" and python -c "import torch.backends.cudnn as c;c.enabled and print('cudnn',c.version())"
	‚Ä¢	Python/Poetry: python --version, poetry --version
	‚Ä¢	Core libs: hydra-core, dvc, mlflow (versions) ‚Äî pip show hydra-core dvc mlflow | grep -E 'Name|Version'
	‚Ä¢	Project CLI: spectramind --version (include CLI version, config hash, timestamp printed by the app)
	‚Ä¢	Git: git rev-parse HEAD (again, for clarity)
	‚Ä¢	Data location/mode: ‚òê toy ‚òê challenge ‚òê custom; ‚òê local ‚òê DVC remote ‚òê Kaggle dataset

‚∏ª

3) Minimal Reproduction (exact steps & commands)

Provide the smallest scenario that still reproduces the problem. Prefer the toy split first. Include all flags/overrides, exact working directory, and any environment variables used.

Working directory: <absolute or repo‚Äërelative path>

Commands (copy‚Äëpaste exactly as run):

# Example (adjust to your case)
spectramind selftest --deep

# Toy CI/dev loop (fast)
spectramind train --config-name=config_v50 +data.split=toy +training.seed=1337

# Full repro
spectramind calibrate --config-name=config_v50
spectramind train --config-name=config_v50 +training.seed=1337 training.mixed_precision=true
spectramind predict --out-csv outputs/submission.csv
spectramind diagnose dashboard --open-html

Hydra overrides used:
<e.g., +data.split=toy +training.seed=1337 model.decoder.head=gaussian_corel>

DVC context (if relevant):

dvc repro -f
dvc status
dvc doctor

Dataset / Planets involved (IDs or count):
<list or range> (note any that consistently fail)

Seeds:
<list> (confirm if seed changes alter outcome)

‚∏ª

4) Regression Info (if applicable)
	‚Ä¢	Last known good commit: <hash> (commands & metrics attached below)
	‚Ä¢	First bad commit: <hash> (suspected)
	‚Ä¢	Œî Changes (high‚Äëlevel): <e.g., upgraded GATConv; new calibration stage>
	‚Ä¢	Metrics Œî (baseline ‚Üí now):

Metric	Baseline (commit)	Current (commit)	Œî
GLL (val/public)			
MAE(Œº)			
Calibration z‚Äëmean/z‚Äëstd			
Coverage@95%			
Runtime (planet avg)			


‚∏ª

5) Logs & Artifacts (attach or link)
	‚Ä¢	logs/v50_debug_log.md (relevant excerpt + full file if small)
	‚Ä¢	logs/v50_event_log.jsonl (or events.jsonl)
	‚Ä¢	outputs/diagnostics/report_vX.html (dashboard)
	‚Ä¢	outputs/diagnostics/diagnostic_summary.json
	‚Ä¢	constraint_violation_log.json (symbolic)
	‚Ä¢	run_hash_summary_v50.json
	‚Ä¢	Any relevant PNG/CSV (e.g., GLL heatmap, calibration plots)

If artifacts are large, use DVC and paste the .dvc meta + remote path.

‚∏ª

6) Config Snapshot (exact composed Hydra config)

Paste the full OmegaConf.to_yaml(cfg) for the main failing run (redact secrets).

# --- BEGIN HYDRA CONFIG SNAPSHOT ---
<insert exact composed config>
# --- END HYDRA CONFIG SNAPSHOT ---


‚∏ª

7) Symbolic / Physics Integrity

Check all that apply and describe:
	‚Ä¢	New or increased symbolic rule violations (which rules?):
	‚Ä¢	‚òê Smoothness ‚òê Nonnegativity ‚òê Asymmetry ‚òê FFT prior ‚òê Photonic alignment
	‚Ä¢	Molecule region inconsistencies: ‚òê H‚ÇÇO ‚òê CH‚ÇÑ ‚òê CO‚ÇÇ ‚òê CO ‚òê Other: <...>
	‚Ä¢	Gravitational lensing or calibration anomalies suspected
	‚Ä¢	Physical constraints broken in specific bins / wavelengths (list):
	‚Ä¢	<bin indices / wavelength ranges>

Details / screenshots:
<describe and/or attach figures>

‚∏ª

8) Error Trace / Console Output

<paste the exact error/traceback or relevant console snippet>

If a hang/deadlock, provide:
	‚Ä¢	Location (stage/command)
	‚Ä¢	Last visible log line(s)
	‚Ä¢	nvidia-smi top summary during hang (utilization, memory)
	‚Ä¢	Whether --debug or increased logging reveals progress

‚∏ª

9) Workarounds Tried
	‚Ä¢	Disable mixed precision (training.mixed_precision=false)
	‚Ä¢	Reduce batch size (+data.batch_size=<n>)
	‚Ä¢	Different seed(s)
	‚Ä¢	Rebuild environment / re‚Äëinstall deps
	‚Ä¢	Pin Torch/CUDA versions
	‚Ä¢	Skip diagnostics (diagnose --no-umap/--no-tsne)
	‚Ä¢	Other: <what else?>

Effect of each attempt:
<brief results; include timings/metrics if relevant>

‚∏ª

10) Security & Data Compliance
	‚Ä¢	No credentials or secrets leaked in logs/configs
	‚Ä¢	No prohibited data uploaded (competition rules compliant)
	‚Ä¢	Licenses/attributions unchanged

‚∏ª

11) Additional Context

Anything else we should know? Links to PRs, discussions, Slack threads, or external references.

‚∏ª

Maintainer Triage (to be filled by core team)
	‚Ä¢	Category: ‚òê Data/Calibration ‚òê Model/GNN/Decoder ‚òê CLI/Config ‚òê Diagnostics ‚òê Infra/CI ‚òê Packaging/Submission ‚òê Docs
	‚Ä¢	Owner: @assignee
	‚Ä¢	Priority: P0 / P1 / P2
	‚Ä¢	Root Cause (when known): <summary>
	‚Ä¢	Next Actions: <checklist>
	‚Ä¢	Target Release/Tag: <e.g., v0.1.2>
	‚Ä¢	Labels to apply: bug, needs-repro, physics, symbolic, perf, infra, docs, help-wanted

‚∏ª


<details>
<summary><strong>How to Collect Useful Evidence (quick guide)</strong></summary>


# 1) Version & hash
spectramind --version
git rev-parse HEAD

# 2) Self-tests
spectramind selftest --fast
spectramind selftest --deep

# 3) Minimal toy repro
spectramind train --config-name=config_v50 +data.split=toy +training.seed=1337

# 4) Full pipeline (if needed)
spectramind calibrate --config-name=config_v50
spectramind train --config-name=config_v50 +training.seed=1337
spectramind predict --out-csv outputs/submission.csv
spectramind diagnose dashboard

# 5) Collect logs & artifacts
ls -lah logs/ outputs/diagnostics/

If you have DVC:

dvc repro -f
dvc status
dvc doctor

For CUDA/Torch:

nvidia-smi
python - <<'PY'
import torch, json
print(json.dumps({
  "torch": torch.__version__,
  "cuda_available": torch.cuda.is_available(),
  "cuda_version": getattr(torch.version, "cuda", None),
  "device_count": torch.cuda.device_count()
}, indent=2))
PY

</details>
