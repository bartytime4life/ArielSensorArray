Perfect — here’s the minimal Kaggle inference notebook you asked for, ready to upload alongside your datasets. Save it as:

assets/kaggle_infer_template.ipynb

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SpectraMind V50 — Kaggle Inference Template\\n",
    "\\n",
    "This notebook runs the **inference → σ calibration → validate → package** pipeline on Kaggle without internet.\\n",
    "\\n",
    "**Instructions**\\n",
    "1. Attach these datasets in the right sidebar (Add Data):\\n",
    "   - `ADC2025-competition-data` (or your exact competition input dataset)\\n",
    "   - `v50-weights` (trained checkpoints / artifacts)\\n",
    "   - `v50-runtime` (code bundle + configs if needed)\\n",
    "2. Make sure **GPU** is enabled and **Internet** is OFF.\\n",
    "3. Run cells top to bottom.\\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0) Environment sanity & paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, json, platform, subprocess, shlex, hashlib\\n",
    "from pathlib import Path\\n",
    "\\n",
    "ROOT = Path('/kaggle/working').resolve()\\n",
    "IN_COMP = Path('/kaggle/input/adc2025-competition-data')  # <-- change to exact dataset name\\n",
    "IN_WEIGHTS = Path('/kaggle/input/v50-weights')            # <-- change to your weights dataset name\\n",
    "IN_RUNTIME = Path('/kaggle/input/v50-runtime')            # optional: if delivering code as a dataset\\n",
    "\\n",
    "print('Python:', sys.version)\\n",
    "print('Platform:', platform.platform())\\n",
    "print('GPU visible via nvidia-smi:')\\n",
    "subprocess.run(shlex.split('nvidia-smi'))\\n",
    "print('\\nMounted datasets:')\\n",
    "print('  COMP :', IN_COMP.exists(), str(IN_COMP))\\n",
    "print('  WEIGHTS:', IN_WEIGHTS.exists(), str(IN_WEIGHTS))\\n",
    "print('  RUNTIME:', IN_RUNTIME.exists(), str(IN_RUNTIME))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) (Optional) Add runtime code to PYTHONPATH\\n",
    "Use this if you've shipped the repository (or a minimal wheel) as an attached dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IN_RUNTIME.exists():\\n",
    "    add_paths = []\\n",
    "    # If you shipped a wheel, install it (recommended)\\n",
    "    wheel_files = list(IN_RUNTIME.glob('*.whl'))\\n",
    "    if wheel_files:\\n",
    "        print('Installing wheel:', wheel_files[0].name)\\n",
    "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', '--no-index', '--find-links', str(IN_RUNTIME), str(wheel_files[0])])\\n",
    "    else:\\n",
    "        # Or just append repo path to pythonpath if you shipped the source tree\\n",
    "        add_paths = [str(IN_RUNTIME)]\\n",
    "        for p in add_paths:\\n",
    "            if p not in sys.path:\\n",
    "                sys.path.append(p)\\n",
    "        print('Added to sys.path:', add_paths)\\n",
    "else:\\n",
    "    print('IN_RUNTIME not attached — assuming code is pre-installed in this image/notebook.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Self-test (fast) — confirm CLI, config, shapes\\n",
    "This calls the project self-test to catch environment or path issues early."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = \"python -m spectramind test --mode=fast\"\\n",
    "print('RUN:', cmd)\\n",
    "ret = subprocess.run(shlex.split(cmd), cwd=str(ROOT))\\n",
    "if ret.returncode != 0:\\n",
    "    raise SystemExit('Self-test failed — check paths / attached datasets.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Predict μ, raw σ\\n",
    "We set Kaggle-aware paths via Hydra overrides. Adjust dataset names if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = (\\n",
    "    \"python -m spectramind predict \"\\n",
    "    \"+data.split=test \"\\n",
    "    \"+runtime.kaggle=true \"\\n",
    "    f\"+paths.input={IN_COMP} \"\\n",
    "    f\"+paths.out={ROOT}/output \"\\n",
    "    f\"+model.ckpt={IN_WEIGHTS}/model.ckpt \"\\n",
    ")\\n",
    "print('RUN:', cmd)\\n",
    "ret = subprocess.run(shlex.split(cmd))\\n",
    "if ret.returncode != 0:\\n",
    "    raise SystemExit('Predict step failed — see logs above.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Calibrate σ (temperature + COREL)\\n",
    "Applies post-hoc uncertainty calibration for leaderboard-safe σ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd = (\\n",
    "    \"python -m spectramind calibrate-temp \"\\n",
    "    \"+calib.corel=true \"\\n",
    "    f\"+paths.out={ROOT}/output \"\\n",
    ")\\n",
    "print('RUN:', cmd)\\n",
    "ret = subprocess.run(shlex.split(cmd))\\n",
    "if ret.returncode != 0:\\n",
    "    raise SystemExit('Calibration step failed — see logs above.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Validate and package (CSV/ZIP + report.html)\\n",
    "Runs the validator, then bundles a submission zip that complies with the competition rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUBMIT_ZIP = ROOT / 'submission.zip'\\n",
    "cmd = (\\n",
    "    \"python -m spectramind submit \"\\n",
    "    f\"+submit.bundle={SUBMIT_ZIP} \"\\n",
    "    \"+report.html=true \"\\n",
    ")\\n",
    "print('RUN:', cmd)\\n",
    "ret = subprocess.run(shlex.split(cmd))\\n",
    "if ret.returncode != 0:\\n",
    "    raise SystemExit('Submit step failed — see logs above.')\\n",
    "print('Submission bundle:', SUBMIT_ZIP, SUBMIT_ZIP.exists())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Show run metadata (Hydra configs, logs)\\n",
    "Useful for audit; the CLI writes hydra configs and logs in outputs/ and logs/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lah /kaggle/working/output || true\\n",
    "!ls -lah /kaggle/working/output/hydra || true\\n",
    "!ls -lah logs || true"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

What you’ll change:
	•	Replace dataset names in the top cell:
	•	/kaggle/input/adc2025-competition-data → your exact competition dataset slug
	•	/kaggle/input/v50-weights → your weights artifacts dataset
	•	/kaggle/input/v50-runtime → your code bundle dataset (if using the wheel/source approach)
	•	If you shipped a wheel: ensure it’s present in v50-runtime and named accordingly (the installer cell picks the first *.whl).

Tip: Pair this notebook with the assets/KAGGLE_GUIDE.md we just created to keep the workflow self-documented inside your repo.