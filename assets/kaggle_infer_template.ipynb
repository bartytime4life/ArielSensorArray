Here’s a clean, ASCII-only, valid .ipynb JSON with straight quotes, fixed globs, proper f-strings, and no Unicode dashes. You can paste this into a file like spectramind_v50_kaggle_inference.ipynb and open it directly in Kaggle/Jupyter.

{
“cells”: [
{
“cell_type”: “markdown”,
“metadata”: {},
“source”: [
“# SpectraMind V50 — Kaggle Inference Template (Hardened v3)\n”,
“\n”,
“End-to-end inference -> sigma calibration -> validate -> package on Kaggle with Internet OFF.\n”,
“\n”,
“### Attach these datasets (right sidebar -> Add Data)\n”,
“- Competition data: your exact competition input dataset (e.g., neurips-2025-ariel-data-challenge)\n”,
“- Weights/artifacts: e.g., v50-weights\n”,
“- (Optional) Runtime code: e.g., v50-runtime (wheel or source tree)\n”,
“\n”,
“### Runtime knobs\n”,
“- Enable GPU (T4 / A10 / L4) • Internet OFF • Accelerator ON\n”,
“- Run cells top -> bottom. Errors are fail-fast with clear messages.\n”,
“\n”,
“> Tip: Pair with assets/KAGGLE_GUIDE.md in your repo for a self-documented workflow.\n”
]
},
{
“cell_type”: “markdown”,
“metadata”: {},
“source”: [
“## 0) Environment sanity, offline guards, canonical paths\n”,
“- Edit only the dataset slugs in DATASETS below (or set SMV50_* env vars in a custom image).\n”,
“- We set strict offline/env flags, deterministic seeds, and print quick diagnostics.\n”
]
},
{
“cell_type”: “code”,
“execution_count”: null,
“metadata”: {
“trusted”: true
},
“outputs”: [],
“source”: [
“import os, sys, json, time, random, platform, subprocess, shlex, hashlib\n”,
“from pathlib import Path\n”,
“\n”,
“# — Kaggle dataset slugs (EDIT THESE) —\n”,
“# You can also override via env vars SMV50_COMP, SMV50_WEIGHTS, SMV50_RUNTIME (optional)\n”,
“DATASETS = {\n”,
“    "COMP":    os.environ.get("SMV50_COMP",    "/kaggle/input/neurips-2025-ariel-data-challenge"),\n”,
“    "WEIGHTS": os.environ.get("SMV50_WEIGHTS", "/kaggle/input/v50-weights"),\n”,
“    "RUNTIME": os.environ.get("SMV50_RUNTIME", "/kaggle/input/v50-runtime"),  # optional\n”,
“}\n”,
“\n”,
“IN_COMP    = Path(DATASETS["COMP"]).resolve()\n”,
“IN_WEIGHTS = Path(DATASETS["WEIGHTS"]).resolve()\n”,
“IN_RUNTIME = Path(DATASETS["RUNTIME"]).resolve()\n”,
“\n”,
“# — Working/output roots —\n”,
“ROOT    = Path("/kaggle/working").resolve()\n”,
“OUTDIR  = ROOT / "output"\n”,
“LOGDIR  = ROOT / "logs"\n”,
“OUTDIR.mkdir(parents=True, exist_ok=True)\n”,
“LOGDIR.mkdir(parents=True, exist_ok=True)\n”,
“\n”,
“# — Offline / safety / OOM hygiene —\n”,
“os.environ.setdefault("PYTHONHASHSEED", "0")\n”,
“os.environ.setdefault("TOKENIZERS_PARALLELISM", "false")\n”,
“os.environ.setdefault("TRANSFORMERS_OFFLINE", "1")\n”,
“os.environ.setdefault("HF_HOME", str(ROOT / ".hf"))\n”,
“os.environ.setdefault("WANDB_MODE", "disabled")\n”,
“os.environ.setdefault("MPLBACKEND", "Agg")\n”,
“os.environ.setdefault("OMP_NUM_THREADS", "1")\n”,
“os.environ.setdefault("MKL_NUM_THREADS", "1")\n”,
“os.environ.setdefault("PYTORCH_CUDA_ALLOC_CONF", "max_split_size_mb=128")\n”,
“os.environ.setdefault("CUDA_LAUNCH_BLOCKING", "1")  # clearer GPU tracebacks\n”,
“\n”,
“print("Python  :", sys.version.split()[0])\n”,
“print("Platform:", platform.platform())\n”,
“print("ROOT    :", str(ROOT))\n”,
“\n”,
“print("\nMounted datasets:")\n”,
“print("  COMP   :", IN_COMP.exists(), str(IN_COMP))\n”,
“print("  WEIGHTS:", IN_WEIGHTS.exists(), str(IN_WEIGHTS))\n”,
“print("  RUNTIME:", IN_RUNTIME.exists(), str(IN_RUNTIME))\n”,
“\n”,
“# Attempt to show GPU presence\n”,
“try:\n”,
“    print("\nGPU via nvidia-smi:")\n”,
“    subprocess.run(shlex.split("nvidia-smi"), check=False)\n”,
“except Exception as e:\n”,
“    print("nvidia-smi not available:", e)\n”,
“\n”,
“# Basic sanity\n”,
“missing = []\n”,
“if not IN_COMP.exists():\n”,
“    missing.append(f"COMP dataset not found: {IN_COMP}")\n”,
“if not IN_WEIGHTS.exists():\n”,
“    missing.append(f"WEIGHTS dataset not found: {IN_WEIGHTS}")\n”,
“if missing:\n”,
“    print("\n\n".join(missing))\n”,
“    raise SystemExit("Required dataset(s) missing — fix dataset slugs in DATASETS or attach datasets.")\n”,
“\n”,
“# Utility: quick checksum (first file) to fingerprint artifacts\n”,
“def sha256_of(path: Path, bufsize=10241024) -> str:\n”,
“    h = hashlib.sha256()\n”,
“    with open(path, "rb") as f:\n”,
“        while True:\n”,
“            b = f.read(bufsize)\n”,
“            if not b:\n”,
“                break\n”,
“            h.update(b)\n”,
“    return h.hexdigest()\n”,
“\n”,
“# Try to fingerprint the main checkpoint if present\n”,
“ckpt_guess = None\n”,
“for pat in [".ckpt", ".pt", ".pth"]:\n”,
“    cand = sorted(IN_WEIGHTS.glob(pat))\n”,
“    if cand:\n”,
“        ckpt_guess = cand[0]\n”,
“        break\n”,
“if ckpt_guess is not None:\n”,
“    try:\n”,
“        print("\nArtifact fingerprint (sha256, first 12):", sha256_of(ckpt_guess)[:12], "\n  ->", ckpt_guess.name)\n”,
“    except Exception as e:\n”,
“        print("Could not hash checkpoint:", e)\n”,
“else:\n”,
“    print("No obvious checkpoint file found in WEIGHTS dataset; the CLI may refer to a specific path.")\n”
]
},
{
“cell_type”: “markdown”,
“metadata”: {},
“source”: [
“## 1) Optional: install runtime code from wheel or add source to PYTHONPATH\n”,
“- If v50-runtime contains a wheel (.whl), install it offline (–no-index, –find-links).\n”,
“- Otherwise, append the source directory to sys.path.\n”,
“- If neither is attached, assume spectramind is preinstalled in the image/notebook.\n”
]
},
{
“cell_type”: “code”,
“execution_count”: null,
“metadata”: {
“trusted”: true
},
“outputs”: [],
“source”: [
“if IN_RUNTIME.exists():\n”,
“    wheel_files = sorted(IN_RUNTIME.glob(".whl"))\n”,
“    if wheel_files:\n”,
“        whl = wheel_files[0]\n”,
“        print("Installing wheel (offline):", whl.name)\n”,
“        subprocess.check_call([\n”,
“            sys.executable, "-m", "pip", "install", "–no-index", "–find-links", str(IN_RUNTIME), str(whl)\n”,
“        ])\n”,
“    else:\n”,
“        if str(IN_RUNTIME) not in sys.path:\n”,
“            sys.path.append(str(IN_RUNTIME))\n”,
“        print("Added to sys.path:", IN_RUNTIME)\n”,
“else:\n”,
“    print("IN_RUNTIME not attached — assuming spectramind is already available in the image/notebook.")\n”,
“\n”,
“try:\n”,
“    import spectramind  # noqa: F401\n”,
“    print("spectramind module import: OK")\n”,
“except Exception as e:\n”,
“    raise SystemExit("Could not import spectramind. Ensure v50-runtime is attached (wheel/source) or the image includes it.\n" + str(e))\n”
]
},
{
“cell_type”: “markdown”,
“metadata”: {},
“source”: [
“## 2) Command wrapper, self-test (fast): verify CLI/config/shapes\n”,
“A small wrapper logs outputs to logs/ and fails fast if any step fails.\n”
]
},
{
“cell_type”: “code”,
“execution_count”: null,
“metadata”: {
“trusted”: true
},
“outputs”: [],
“source”: [
“def run(cmd: str, log_name: str):\n”,
“    """Run shell command, tee stdout/stderr to logs, fail fast on error."""\n”,
“    from datetime import datetime\n”,
“    LOGDIR.mkdir(parents=True, exist_ok=True)\n”,
“    log_path = LOGDIR / log_name\n”,
“    print("RUN:", cmd)\n”,
“    with open(log_path, "w", encoding="utf-8") as f:\n”,
“        f.write(f"# {datetime.utcnow().isoformat()}Z\n$ {cmd}\n\n")\n”,
“        ret = subprocess.run(shlex.split(cmd), stdout=f, stderr=subprocess.STDOUT, cwd=str(ROOT))\n”,
“    if ret.returncode != 0:\n”,
“        print(f"FAILED -> see {log_path}")\n”,
“        raise SystemExit(f"Step failed: {cmd}")\n”,
“    print("OK:", log_path)\n”,
“\n”,
“# Fast self-test\n”,
“run("python -m spectramind test –fast", "00_selftest.log")\n”,
“print("Self-test: OK")\n”
]
},
{
“cell_type”: “markdown”,
“metadata”: {},
“source”: [
“## 3) Predict mu and raw sigma (Hydra overrides are Kaggle-aware)\n”,
“- We pass canonical Kaggle paths via +paths.* overrides.\n”,
“- If your weights checkpoint file has a specific name/path, set it below.\n”,
“- Outputs are written to output/ with Hydra logs/configs in output/hydra/.\n”
]
},
{
“cell_type”: “code”,
“execution_count”: null,
“metadata”: {
“trusted”: true
},
“outputs”: [],
“source”: [
“# Pick checkpoint path: prefer detected file, else default\n”,
“CKPT_PATH = str(ckpt_guess) if "ckpt_guess" in globals() and ckpt_guess is not None else str(IN_WEIGHTS / "model.ckpt")\n”,
“print("Using checkpoint:", CKPT_PATH)\n”,
“\n”,
“# Build predict command\n”,
“predict_cmd = " ".join([\n”,
“    "python -m spectramind predict",\n”,
“    "+data.split=test",\n”,
“    "+runtime.kaggle=true",\n”,
f”    "+paths.input={IN_COMP}",\n”,
f”    "+paths.out={OUTDIR}",\n”,
f”    "+model.ckpt={CKPT_PATH}",\n”,
“])\n”,
“run(predict_cmd, "10_predict.log")\n”,
“print("Predict: OK -> outputs in", OUTDIR)\n”,
“\n”,
“# Surface Hydra snapshot(s) for audit\n”,
“try:\n”,
“    HYDRA_DIR = OUTDIR / "hydra"\n”,
“    if HYDRA_DIR.exists():\n”,
“        for name in ["config.yaml", "overrides.yaml"]:\n”,
“            p = HYDRA_DIR / name\n”,
“            if p.exists():\n”,
“                dst = ROOT / f"hydra_{name}"\n”,
“                if dst.exists():\n”,
“                    dst.unlink()\n”,
“                import shutil; shutil.copy2(p, dst)\n”,
“                print("Hydra snapshot ->", dst)\n”,
“except Exception as e:\n”,
“    print("Hydra snapshot copy warning:", e)\n”
]
},
{
“cell_type”: “markdown”,
“metadata”: {},
“source”: [
“## 4) Calibrate sigma (temperature scaling + COREL)\n”,
“- Applies post-hoc uncertainty calibration for leaderboard-safe sigma.\n”,
“- Writes calibrated artifacts back into output/ (e.g., submission.csv or calibrated tensors).\n”
]
},
{
“cell_type”: “code”,
“execution_count”: null,
“metadata”: {
“trusted”: true
},
“outputs”: [],
“source”: [
“cal_cmd = " ".join([\n”,
“    "python -m spectramind calibrate-temp",\n”,
“    "+calib.corel=true",\n”,
f”    "+paths.out={OUTDIR}",\n”,
“])\n”,
“run(cal_cmd, "20_calibrate.log")\n”,
“print("Calibration: OK")\n”
]
},
{
“cell_type”: “markdown”,
“metadata”: {},
“source”: [
“## 5) Validate and package (CSV/ZIP + report.html)\n”,
“- Runs validator and bundles a submission zip per competition rules.\n”,
“- If your competition requires submission.csv directly, also surface it to /kaggle/working/.\n”
]
},
{
“cell_type”: “code”,
“execution_count”: null,
“metadata”: {
“trusted”: true
},
“outputs”: [],
“source”: [
“SUBMIT_ZIP = ROOT / "submission.zip"\n”,
“submit_cmd = " ".join([\n”,
“    "python -m spectramind submit",\n”,
f”    "+submit.bundle={SUBMIT_ZIP}",\n”,
“    "+report.html=true",\n”,
“])\n”,
“run(submit_cmd, "30_submit.log")\n”,
“print("Submission bundle:", SUBMIT_ZIP, SUBMIT_ZIP.exists())\n”,
“\n”,
“import shutil\n”,
“cand_csv = None\n”,
“for name in ["submission.csv", "predictions.csv"]:\n”,
“    p = OUTDIR / name\n”,
“    if p.exists():\n”,
“        cand_csv = p\n”,
“        break\n”,
“if cand_csv:\n”,
“    shutil.copy2(cand_csv, ROOT / "submission.csv")\n”,
“    print("Surfaced CSV ->", ROOT / "submission.csv")\n”,
“else:\n”,
“    print("No submission.csv in output/ (your pipeline may embed it only in the zip).")\n”,
“\n”,
“# Surface core audit artifacts if present\n”,
“for audit_name in ["run_hash_summary_v50.json", "v50_debug_log.md"]:\n”,
“    src = OUTDIR / audit_name\n”,
“    if src.exists():\n”,
“        dst = ROOT / audit_name\n”,
“        try:\n”,
“            if dst.exists():\n”,
“                dst.unlink()\n”,
“            shutil.copy2(src, dst)\n”,
“            print("Audit artifact ->", dst)\n”,
“        except Exception as e:\n”,
“            print("Audit artifact copy warning:", audit_name, e)\n”
]
},
{
“cell_type”: “markdown”,
“metadata”: {},
“source”: [
“## 6) Artifact peek (Hydra configs, diagnostics, logs)\n”,
“- Lists artifacts for quick audit; displays head of submission.csv if present.\n”
]
},
{
“cell_type”: “code”,
“execution_count”: null,
“metadata”: {
“trusted”: true
},
“outputs”: [],
“source”: [
“print("\n== output/ ==")\n”,
“subprocess.run(["bash", "-lc", "ls -lah /kaggle/working/output || true"]) \n”,
“print("\n== output/hydra ==")\n”,
“subprocess.run(["bash", "-lc", "ls -lah /kaggle/working/output/hydra || true"]) \n”,
“print("\n== logs ==")\n”,
“subprocess.run(["bash", "-lc", "ls -lah /kaggle/working/logs || true"]) \n”,
“\n”,
“csv_path = ROOT / "submission.csv"\n”,
“if csv_path.exists():\n”,
“    print("\n== submission.csv (head) ==")\n”,
“    try:\n”,
“        with open(csv_path, "r", encoding="utf-8") as f:\n”,
“            for i, line in enumerate(f):\n”,
“                print(line.rstrip())\n”,
“                if i >= 7:\n”,
“                    break\n”,
“    except Exception as e:\n”,
“        print("Could not read submission.csv:", e)\n”,
“else:\n”,
“    print("\nNo submission.csv at working root.")\n”
]
}
],
“metadata”: {
“accelerator”: “GPU”,
“colab”: {
“name”: “SpectraMind V50 — Kaggle Inference Template (Hardened v3)”
},
“kernelspec”: {
“display_name”: “Python 3”,
“language”: “python”,
“name”: “python3”
},
“language_info”: {
“name”: “python”,
“version”: “3.10”
},
“tags”: [
“kaggle”,
“offline”,
“spectramind”,
“ariel-2025”,
“submission”
]
},
“nbformat”: 4,
“nbformat_minor”: 5
}