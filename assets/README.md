
â¸»

assets/

SpectraMind V50 â€” Ariel Data Challenge 2025
Central repository assets: diagrams, dashboards, reports, and reproducibility visuals

All assets are generated and consumed through the Typer CLI + Hydra configs (no hidden notebook state), so every figure is reproducible and traceable to code, data, and config.

â¸»

ğŸ“Œ Purpose

This directory houses the visual artifacts used across SpectraMind V50:
	â€¢	Architecture & pipeline Mermaid diagrams (source .mmd + rendered .svg/.png)
	â€¢	HTML reports and the diagnostics dashboard
	â€¢	Example/static images used in docs and Kaggle handâ€‘offs

Everything here is sourceâ€‘tracked (Mermaid is the canonical source), autoâ€‘exported in CI, and consumed by docs & dashboards.

â¸»

ğŸ“‚ Layout

assets/
â”œâ”€ diagrams/                 # Mermaid sources + renders
â”‚  â”œâ”€ architecture_stack.mmd / .svg / .png
â”‚  â”œâ”€ pipeline_overview.mmd  / .svg / .png
â”‚  â”œâ”€ symbolic_logic_layers.mmd / .svg / .png
â”‚  â”œâ”€ kaggle_ci_pipeline.mmd / .svg / .png
â”‚  â”œâ”€ test_diagrams.py       # render/validate all .mmd
â”‚  â””â”€ README.md
â”œâ”€ diagnostics_dashboard.html
â”œâ”€ report.html
â””â”€ sample_plots/             # optional PNGs (e.g., spectrum, SHAP overlay)

Why this structure: it keeps sourceâ†’render close, participates in CI, and feeds docs & HTML outputs without manual steps.

â¸»

ğŸ§­ What to edit vs. what not to edit
	â€¢	Edit: *.mmd (Mermaid sources).
	â€¢	Do not edit: generated .svg/.png. Regenerate them via the commands below or CI.

â¸»

ğŸ›  Rendering diagrams

Option A â€” Local (Mermaid CLI)

# 1) Install mermaid-cli
npm i -g @mermaid-js/mermaid-cli

# 2) Render one diagram
mmdc -i assets/diagrams/pipeline_overview.mmd -o assets/diagrams/pipeline_overview.svg
mmdc -i assets/diagrams/pipeline_overview.mmd -o assets/diagrams/pipeline_overview.png

# 3) Render all .mmd in the folder
for f in assets/diagrams/*.mmd; do
  base="${f%.mmd}"
  mmdc -i "$f" -o "${base}.svg"
  mmdc -i "$f" -o "${base}.png"
done

Option B â€” Python test harness

# Render & validate all Mermaid sources
python assets/diagrams/test_diagrams.py --render --strict
# Only some files
python assets/diagrams/test_diagrams.py --render --only pipeline_overview.mmd,symbolic_logic_layers.mmd

The harness is designed to fail fast in CI if any source canâ€™t export cleanly.

Option C â€” GitHub Actions (recommended)

On pushes/PRs, the Mermaid export job runs and attaches the artifacts; renders are committed/packaged per repo policy. This keeps reviewers in the loop without asking them to install toolchains.

â¸»

ğŸ§© Diagrams that must exist (and what they show)
	â€¢	pipeline_overview â€” FGS1/AIRS â†’ Calibration â†’ Modeling (Î¼/Ïƒ) â†’ UQ â†’ Diagnostics â†’ Submission â†’ Ops. Aligns with our calibrated, physicsâ€‘aware pipeline.
	â€¢	architecture_stack â€” CLI (Typer) â†’ Hydra configs â†’ DVC/Git â†’ Calibration â†’ Encoders/Decoders â†’ UQ â†’ Diagnostics â†’ Packaging â†’ CI/Runtime. Mirrors the CLIâ€‘first, Hydraâ€‘composed system.
	â€¢	symbolic_logic_layers â€” constraint families (smoothness, FFT coherence, asymmetry, molecular alignment) used as overlays and diagnostics.
	â€¢	kaggle_ci_pipeline â€” GitHub Actions â†’ Selftest â†’ Train â†’ Diagnose â†’ Validate â†’ Package â†’ Kaggle Submit to ensure portable, leaderboardâ€‘ready assets.

All four are embedded in docs and HTML dashboards (SVG preferred) and are part of CI checks.

â¸»

ğŸ“‘ Embedding (docs, dashboards, Kaggle)
	â€¢	Markdown:
![Pipeline Overview](assets/diagrams/pipeline_overview.svg)
	â€¢	HTML:
<img src="assets/diagrams/pipeline_overview.svg" alt="Pipeline Overview" />
	â€¢	Prefer SVG for clarity; keep PNG only for environments that canâ€™t inline SVG (some viewers/Kaggle).

â¸»

ğŸ” Reproducibility hooks (how assets stay auditable)
	â€¢	CLIâ€‘first: all generation is done through Typer subcommands + Hydra configs â€” no hidden notebook state.
	â€¢	Hydra configs captured with each run; config + data hash land in logs so any figure can be traced to code+config+data.
	â€¢	DVC ties large artifacts (data/models) to Git commits to reproduce figures exactly from historical states.
	â€¢	CI renders diagrams and runs a pipeline sanity pass on sample data to ensure nothing regresses before merge.

â¸»

ğŸ§ª Quickâ€‘start commands (developer workstation)

Render diagrams + run diagnostics endâ€‘toâ€‘end:

# Render all diagrams locally
python assets/diagrams/test_diagrams.py --render --strict

# End-to-end smoke on a tiny slice (example; actual CLI shown here)
spectramind calibrate --sample 5
spectramind train --epochs 1 --fast_dev_run
spectramind diagnose

The CLI uses Hydra overrides (e.g., trainer=ddp or training.epochs=20) to keep experiments codeâ€‘free and repeatable.

â¸»

ğŸ§ª Diagram test (assets/diagrams/test_diagrams.py)
	â€¢	--render: build .svg/.png for each .mmd.
	â€¢	--strict: fail the run on any Mermaid warnings/errors.
	â€¢	--only: commaâ€‘separated subset.
Use this both locally and in CI to catch broken sources early.

â¸»

ğŸ“Š Kaggle model insights (why diagrams look the way they do)

Our diagram flow reflects lessons from public Kaggle baselines:
	â€¢	Residual MLP baselines (~0.329 LB) offer reproducible starting points.
	â€¢	Very deep residual MLPs (80 blocks) squeeze extra signal but risk overfitting without strong detrending.
	â€¢	Multiâ€‘output regressors provide stable fullâ€‘spectrum predictions and are easy to operationalize.

These observations are â€œbakedâ€ into the architecture_stack and pipeline_overview diagrams to document why our pipeline emphasizes calibration, physicsâ€‘informed features, Î¼/Ïƒ prediction, and downstream validation.

â¸»

ğŸ§ª Style guide (Mermaid)
	â€¢	Graph direction: topâ€‘down (TD) unless inherently leftâ€‘toâ€‘right.
	â€¢	Subgraphs: group stages (Calibration, Modeling, Diagnostics).
	â€¢	Node text: concise; longer prose belongs in docs.
	â€¢	No external links inside nodes.
	â€¢	Stable IDs so SVG diffs are readable.
	â€¢	Use defaults for theme/colors to keep CI consistent.

Keep arrows readable; prefer subgraphs and labels over spaghetti connectors.

â¸»

ğŸ§· Maintenance checklist (per PR)
	â€¢	Only .mmd edited; no manual edits to .svg/.png.
	â€¢	python assets/diagrams/test_diagrams.py --render --strict passes locally.
	â€¢	Renders are readable at 100% and wired into docs/HTML.
	â€¢	Large diffs? Consider splitting a diagram.

â¸»

ğŸ” Source of truth & traceability
	â€¢	Configs: configs/**.yaml (Hydra), committed.
	â€¢	Data/Models: tracked by DVC remotes and tied to Git commits.
	â€¢	Logs: console + structured logs with config & dataset hashes.
	â€¢	CI: GitHub Actions runs unit/selfâ€‘tests and diagram export before merge.

â¸»

ğŸ“¦ Packaging & handoff
	â€¢	Reports (report.html, diagnostics_dashboard.html) bundle diagrams (SVG preferred) and JSON metrics; these are exported as artifacts in CI and included in Kaggle submissions when required.
	â€¢	The kaggle_ci_pipeline diagram documents the path from GitHub Actions to a reproducible Kaggle artifact.

â¸»

ğŸ” References
	â€¢	CLI + Hydra + DVC + CI reproducibility loop.
	â€¢	Diagram policy and rendering harness.
	â€¢	Kaggle platform practices & constraints.

â¸»

End of assets/README.md