# assets/

**SpectraMind V50 â€” Ariel Data Challenge 2025**  
*Central repository assets: diagrams, dashboards, reports, and reproducibility visuals*

---

## ğŸ“Œ Purpose

This directory consolidates all **visual artifacts** used across SpectraMind V50.  
It ensures the system is **self-documented, reproducible, and leaderboard-ready**:

- **Source-tracked** (Mermaid `.mmd` is canonical)  
- **Auto-exported** (`.svg`, `.png`, `.pdf` via CI or `make diagrams`)  
- **CI-validated** (`test_diagrams.py`, mermaid-export workflow)  
- **Dashboard-ready** (`report.html`, `diagnostics_dashboard.html`)  

---

## ğŸ“‚ Contents

- **`diagrams/`** â€” Mermaid diagrams (`.mmd` sources) + rendered `.svg`/`.png`.  
  Used in `ARCHITECTURE.md`, reports, and dashboards.  
- **`report.html`** â€” Compact reproducibility log (pipeline + configs).  
- **`diagnostics_dashboard.html`** â€” Interactive dashboard (UMAP/t-SNE, SHAP overlays, symbolic rule tables).  
- **`sample_plots/`** *(optional)* â€” Example PNGs for testing (`sample_spectrum.png`, `umap_clusters.png`, `shap_overlay.png`).  

---

## ğŸ“Š Kaggle Model Insights

SpectraMind V50 integrates lessons from Kaggle baselines in the NeurIPS 2025 Ariel Data Challenge:

- **Thang Do Duc â€œ0.329 LBâ€**  
  â€¢ Residual MLP, simple preprocessing.  
  â€¢ No Ïƒ (uncertainty) estimation.  
  â€¢ Robust and reproducible reference design.  

- **V1ctorious3010 â€œ80bl-128hd-impactâ€**  
  â€¢ Extremely deep (80 residual blocks, 128 hidden).  
  â€¢ Captures subtle features but prone to overfitting.  
  â€¢ Strong LB score (0.322) but less interpretable.  

- **Fawad Awan â€œSpectrum Regressorâ€**  
  â€¢ Multi-output regressor predicts entire spectrum.  
  â€¢ Stable, interpretable, consistent across bins.  
  â€¢ Slightly weaker LB (0.318) but leaderboard-stable.  

**Takeaways embedded in V50:**  
- Residual-style encoders (Mamba SSM for FGS1, GNN for AIRS).  
- Physics-informed detrending & jitter correction.  
- Explicit uncertainty (Ïƒ) estimation with Temperature Scaling + COREL GNN.  
- Ensemble shallow + deep networks, fused with symbolic overlays.  

---

## ğŸ“ Diagrams

Maintained in `assets/diagrams/` (CI-tested via `test_diagrams.py`):

- `pipeline_overview.mmd` â†’ End-to-end flow (FGS1/AIRS â†’ Calibration â†’ Modeling â†’ Diagnostics â†’ Submission)  
- `architecture_stack.mmd` â†’ Layered view (CLI, configs, DVC, encoders, symbolic logic, CI)  
- `symbolic_logic_layers.mmd` â†’ Constraint families (smoothness, FFT, asymmetry, molecular alignment)  
- `gui_integration.mmd` â†’ Optional UI layer (HTML/Jupyter/React)  
- `kaggle_ci_pipeline.mmd` â†’ GitHub Actions workflow: Train â†’ Diagnose â†’ Package â†’ Kaggle Submit  

Rendered `.svg` and `.png` files are committed for portability.  

---

## ğŸ“‘ Reports

- **`report.html`** â€” Compact reproducibility report.  
- **`diagnostics_dashboard.html`** â€” Rich interactive diagnostics (symbolic overlays, SHAP, latent projections, calibration checks).  

---

## ğŸ›  Reproducibility & CI

- **Hydra Configs:** versioned YAMLs (`configs/`).  
- **Data:** DVC-tracked datasets and models.  
- **CI/CD:** GitHub Actions workflows (selftest, diagnostics, mermaid-export).  
- **Logging:** `logs/v50_debug_log.md` (append-only), JSONL event streams.  
- **Validation:** `test_diagrams.py` checks that `.mmd` sources render cleanly and are embedded in docs.  

Every artifact here is **versioned, CI-tested, and leaderboard-safe**.  

---
